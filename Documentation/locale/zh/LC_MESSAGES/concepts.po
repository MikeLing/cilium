# SOME DESCRIPTIVE TITLE.
# Copyright (C) Cilium Authors
# This file is distributed under the same license as the Cilium package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Cilium \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-05-25 00:09+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../concepts/clustermesh/index.rst:3 ../../concepts/ebpf/index.rst:3
#: ../../concepts/ebpf/intro.rst:3 ../../concepts/ebpf/iptables.rst:3
#: ../../concepts/ebpf/lifeofapacket.rst:3 ../../concepts/ebpf/maps.rst:3
#: ../../concepts/index.rst:3 ../../concepts/kubernetes/ciliumendpoint.rst:3
#: ../../concepts/kubernetes/ciliumendpointslice.rst:3
#: ../../concepts/kubernetes/compatibility.rst:3
#: ../../concepts/kubernetes/concepts.rst:3
#: ../../concepts/kubernetes/configuration.rst:3
#: ../../concepts/kubernetes/index.rst:3 ../../concepts/kubernetes/intro.rst:3
#: ../../concepts/kubernetes/policy.rst:3
#: ../../concepts/kubernetes/requirements.rst:3
#: ../../concepts/kubernetes/troubleshooting.rst:3
#: ../../concepts/networking/fragmentation.rst:3
#: ../../concepts/networking/index.rst:3
#: ../../concepts/networking/ipam/azure.rst:3
#: ../../concepts/networking/ipam/crd.rst:3
#: ../../concepts/networking/ipam/deep_dive.rst:3
#: ../../concepts/networking/ipam/eni.rst:3
#: ../../concepts/networking/ipam/gke.rst:3
#: ../../concepts/networking/ipam/index.rst:3
#: ../../concepts/networking/ipam/kubernetes.rst:3
#: ../../concepts/networking/masquerading.rst:3
#: ../../concepts/networking/routing.rst:3
#: ../../concepts/observability/hubble-configuration.rst:3
#: ../../concepts/observability/index.rst:3
#: ../../concepts/observability/intro.rst:3 ../../concepts/overview.rst:3
#: ../../concepts/security/identity.rst:3 ../../concepts/security/index.rst:3
#: ../../concepts/security/intro.rst:3
#: ../../concepts/security/policyenforcement.rst:3
#: ../../concepts/security/proxy/envoy.rst:3
#: ../../concepts/security/proxy/index.rst:3 ../../concepts/terminology.rst:3
#: ../../gettingstarted/k8s-install-download-release.rst:3
msgid ""
"WARNING: You are looking at unreleased Cilium documentation. Please use "
"the official rendered version released here: https://docs.cilium.io"
msgstr ""

#: ../../concepts/clustermesh/index.rst:11
msgid "Multi-Cluster (Cluster Mesh)"
msgstr ""

#: ../../concepts/clustermesh/index.rst:13
msgid ""
"Cluster mesh extends the networking datapath across multiple clusters. It"
" allows endpoints in all connected clusters to communicate while "
"providing full policy enforcement. Load-balancing is available via "
"Kubernetes annotations."
msgstr ""

#: ../../concepts/clustermesh/index.rst:17
msgid "See :ref:`gs_clustermesh` for instructions on how to set up cluster mesh."
msgstr ""

#: ../../concepts/ebpf/index.rst:11
msgid "eBPF Datapath"
msgstr ""

#: ../../concepts/ebpf/intro.rst:9 ../../concepts/kubernetes/intro.rst:11
#: ../../concepts/observability/intro.rst:11
#: ../../concepts/security/intro.rst:9
msgid "Introduction"
msgstr ""

#: ../../concepts/ebpf/intro.rst:11
msgid ""
"The Linux kernel supports a set of BPF hooks in the networking stack that"
" can be used to run BPF programs. The Cilium datapath uses these hooks to"
" load BPF programs that when used together create higher level networking"
" constructs."
msgstr ""

#: ../../concepts/ebpf/intro.rst:16
msgid ""
"The following is a list of the hooks used by Cilium and a brief "
"description. For a more thorough documentation on specifics of each hook "
"see :ref:`bpf_guide`."
msgstr ""

#: ../../concepts/ebpf/intro.rst:20
msgid ""
"**XDP:** The XDP BPF hook is at the earliest point possible in the "
"networking driver and triggers a run of the BPF program upon packet "
"reception. This achieves the best possible packet processing performance "
"since the program runs directly on the packet data before any other "
"processing can happen. This hook is ideal for running filtering programs "
"that drop malicious or unexpected traffic, and other common DDOS "
"protection mechanisms."
msgstr ""

#: ../../concepts/ebpf/intro.rst:28
msgid ""
"**Traffic Control Ingress/Egress:** BPF programs attached to the traffic "
"control (tc) ingress hook are attached to a networking interface, same as"
" XDP, but will run after the networking stack has done initial processing"
" of the packet. The hook is run before the L3 layer of the stack but has "
"access to most of the metadata associated with a packet. This is ideal "
"for doing local node processing, such as applying L3/L4 endpoint policy "
"and redirecting traffic to endpoints. For networking facing devices the "
"tc ingress hook can be coupled with above XDP hook. When this is done it "
"is reasonable to assume that the majority of the traffic at this point is"
" legitimate and destined for the host."
msgstr ""

#: ../../concepts/ebpf/intro.rst:39
msgid ""
"Containers typically use a virtual device called a veth pair which acts "
"as a virtual wire connecting the container to the host. By attaching to "
"the TC ingress hook of the host side of this veth pair Cilium can monitor"
" and enforce policy on all traffic exiting a container. By attaching a "
"BPF program to the veth pair associated with each container and routing "
"all network traffic to the host side virtual devices with another BPF "
"program attached to the tc ingress hook as well Cilium can monitor and "
"enforce policy on all traffic entering or exiting the node."
msgstr ""

#: ../../concepts/ebpf/intro.rst:48
msgid ""
"**Socket operations:** The socket operations hook is attached to a "
"specific cgroup and runs on TCP events. Cilium attaches a BPF socket "
"operations program to the root cgroup and uses this to monitor for TCP "
"state transitions, specifically for ESTABLISHED state transitions. When a"
" socket transitions into ESTABLISHED state if the TCP socket has a node "
"local peer (possibly a local proxy) a socket send/recv program is "
"attached."
msgstr ""

#: ../../concepts/ebpf/intro.rst:55
msgid ""
"**Socket send/recv:** The socket send/recv hook runs on every send "
"operation performed by a TCP socket. At this point the hook can inspect "
"the message and either drop the message, send the message to the TCP "
"layer, or redirect the message to another socket. Cilium uses this to "
"accelerate the datapath redirects as described below."
msgstr ""

#: ../../concepts/ebpf/intro.rst:61
msgid ""
"Combining the above hooks with virtual interfaces (cilium_host, "
"cilium_net), an optional overlay interface (cilium_vxlan), Linux kernel "
"crypto support and a userspace proxy (Envoy) Cilium creates the following"
" networking objects."
msgstr ""

#: ../../concepts/ebpf/intro.rst:65
msgid ""
"**Prefilter:** The prefilter object runs an XDP program and provides a "
"set of prefilter rules used to filter traffic from the network for best "
"performance. Specifically, a set of CIDR maps supplied by the Cilium "
"agent are used to do a lookup and the packet is either dropped, for "
"example when the destination is not a valid endpoint, or allowed to be "
"processed by the stack. This can be easily extended as needed to build in"
" new prefilter criteria/capabilities."
msgstr ""

#: ../../concepts/ebpf/intro.rst:71
msgid ""
"**Endpoint Policy:** The endpoint policy object implements the Cilium "
"endpoint enforcement. Using a map to lookup a packets associated identity"
" and policy this layer scales well to lots of endpoints. Depending on the"
" policy this layer may drop the packet, forward to a local endpoint, "
"forward to the service object or forward to the L7 Policy object for "
"further L7 rules. This is the primary object in the Cilium datapath "
"responsible for mapping packets to identities and enforcing L3 and L4 "
"policies."
msgstr ""

#: ../../concepts/ebpf/intro.rst:78
msgid ""
"**Service:** The Service object performs a map lookup on the destination "
"IP and optionally destination port for every packet received by the "
"object. If a matching entry is found, the packet will be forwarded to one"
" of the configured L3/L4 endpoints. The Service block can be used to "
"implement a standalone load balancer on any interface using the TC "
"ingress hook or may be integrated in the endpoint policy object."
msgstr ""

#: ../../concepts/ebpf/intro.rst:85
msgid ""
"**L3 Encryption:** On ingress the L3 Encryption object marks packets for "
"decryption, passes the packets to the Linux xfrm (transform) layer for "
"decryption, and after the packet is decrypted the object receives the "
"packet then passes it up the stack for further processing by other "
"objects. Depending on the mode, direct routing or overlay, this may be a "
"BPF tail call or the Linux routing stack that passes the packet to the "
"next object. The key required for decryption is encoded in the IPsec "
"header so on ingress we do not need to do a map lookup to find the "
"decryption key."
msgstr ""

#: ../../concepts/ebpf/intro.rst:94
msgid ""
"On egress a map lookup is first performed using the destination IP to "
"determine if a packet should be encrypted and if so what keys are "
"available on the destination node. The most recent key available on both "
"nodes is chosen and the packet is marked for encryption. The packet is "
"then passed to the Linux xfrm layer where it is encrypted. Upon receiving"
" the now encrypted packet it is passed to the next layer either by "
"sending it to the Linux stack for routing or doing a direct tail call if "
"an overlay is in use."
msgstr ""

#: ../../concepts/ebpf/intro.rst:102
msgid ""
"**Socket Layer Enforcement:** Socket layer enforcement use two hooks the "
"socket operations hook and the socket send/recv hook to monitor and "
"attach to all TCP sockets associated with Cilium managed endpoints, "
"including any L7 proxies. The socket operations hook will identify "
"candidate sockets for accelerating. These include all local node "
"connections (endpoint to endpoint) and any connection to a Cilium proxy. "
"These identified connections will then have all messages handled by the "
"socket send/recv hook and will be accelerated using sockmap fast "
"redirects. The fast redirect ensures all policies implemented in Cilium "
"are valid for the associated socket/endpoint mapping and assuming they "
"are sends the message directly to the peer socket. This is allowed "
"because the sockmap send/recv hooks ensures the message will not need to "
"be processed by any of the objects above."
msgstr ""

#: ../../concepts/ebpf/intro.rst:115
msgid ""
"**L7 Policy:** The L7 Policy object redirect proxy traffic to a Cilium "
"userspace proxy instance. Cilium uses an Envoy instance as its userspace "
"proxy. Envoy will then either forward the traffic or generate appropriate"
" reject messages based on the configured L7 policy."
msgstr ""

#: ../../concepts/ebpf/intro.rst:119
msgid ""
"These components are connected to create the flexible and efficient "
"datapath used by Cilium. Below we show the following possible flows "
"connecting endpoints on a single node, ingress to an endpoint, and "
"endpoint to egress networking device. In each case there is an additional"
" diagram showing the TCP accelerated path available when socket layer "
"enforcement is enabled."
msgstr ""

#: ../../concepts/ebpf/iptables.rst:9
msgid "Iptables Usage"
msgstr ""

#: ../../concepts/ebpf/iptables.rst:11
msgid ""
"Depending on the Linux kernel version being used, the eBPF datapath can "
"implement a varying feature set fully in eBPF. If certain required "
"capabilities are not available, the functionality is provided using a "
"legacy iptables implementation. See :ref:`features_kernel_matrix` for "
"more details."
msgstr ""

#: ../../concepts/ebpf/iptables.rst:18
msgid "kube-proxy Interoperability"
msgstr ""

#: ../../concepts/ebpf/iptables.rst:20
msgid ""
"The following diagram shows the integration of iptables rules as "
"installed by kube-proxy and the iptables rules as installed by Cilium."
msgstr ""

#: ../../concepts/ebpf/lifeofapacket.rst:9
msgid "Life of a Packet"
msgstr ""

#: ../../concepts/ebpf/lifeofapacket.rst:12
msgid "Endpoint to Endpoint"
msgstr ""

#: ../../concepts/ebpf/lifeofapacket.rst:14
msgid ""
"First we show the local endpoint to endpoint flow with optional L7 Policy"
" on egress and ingress. Followed by the same endpoint to endpoint flow "
"with socket layer enforcement enabled. With socket layer enforcement "
"enabled for TCP traffic the handshake initiating the connection will "
"traverse the endpoint policy object until TCP state is ESTABLISHED. Then "
"after the connection is ESTABLISHED only the L7 Policy object is still "
"required."
msgstr ""

#: ../../concepts/ebpf/lifeofapacket.rst:25
msgid "Egress from Endpoint"
msgstr ""

#: ../../concepts/ebpf/lifeofapacket.rst:27
msgid ""
"Next we show local endpoint to egress with optional overlay network. In "
"the optional overlay network traffic is forwarded out the Linux network "
"interface corresponding to the overlay. In the default case the overlay "
"interface is named cilium_vxlan. Similar to above, when socket layer "
"enforcement is enabled and a L7 proxy is in use we can avoid running the "
"endpoint policy block between the endpoint and the L7 Policy for TCP "
"traffic. An optional L3 encryption block will encrypt the packet if "
"enabled."
msgstr ""

#: ../../concepts/ebpf/lifeofapacket.rst:38
msgid "Ingress to Endpoint"
msgstr ""

#: ../../concepts/ebpf/lifeofapacket.rst:40
msgid ""
"Finally we show ingress to local endpoint also with optional overlay "
"network. Similar to above socket layer enforcement can be used to avoid a"
" set of policy traversals between the proxy and the endpoint socket. If "
"the packet is encrypted upon receive it is first decrypted and then "
"handled through the normal flow."
msgstr ""

#: ../../concepts/ebpf/lifeofapacket.rst:48
msgid ""
"This completes the datapath overview. More BPF specifics can be found in "
"the :ref:`bpf_guide`. Additional details on how to extend the L7 Policy "
"exist in the :ref:`envoy` section."
msgstr ""

#: ../../concepts/ebpf/maps.rst:10
msgid "eBPF Maps"
msgstr ""

#: ../../concepts/ebpf/maps.rst:12
msgid ""
"All BPF maps are created with upper capacity limits. Insertion beyond the"
" limit will fail and thus limits the scalability of the datapath. The "
"following table shows the default values of the maps. Each limit can be "
"bumped in the source code. Configuration options will be added on request"
" if demand arises."
msgstr ""

#: ../../concepts/ebpf/maps.rst:18
msgid "Map Name"
msgstr ""

#: ../../concepts/ebpf/maps.rst:18
msgid "Scope"
msgstr ""

#: ../../concepts/ebpf/maps.rst:18
msgid "Default Limit"
msgstr ""

#: ../../concepts/ebpf/maps.rst:18
msgid "Scale Implications"
msgstr ""

#: ../../concepts/ebpf/maps.rst:20
msgid "Connection Tracking"
msgstr ""

#: ../../concepts/ebpf/maps.rst:20
msgid "node or endpoint"
msgstr ""

#: ../../concepts/ebpf/maps.rst:20
msgid "1M TCP/256k UDP"
msgstr ""

#: ../../concepts/ebpf/maps.rst:20
msgid "Max 1M concurrent TCP connections, max 256k expected UDP answers"
msgstr ""

#: ../../concepts/ebpf/maps.rst:21
msgid "NAT"
msgstr ""

#: ../../concepts/ebpf/maps.rst:21 ../../concepts/ebpf/maps.rst:22
#: ../../concepts/ebpf/maps.rst:23 ../../concepts/ebpf/maps.rst:24
#: ../../concepts/ebpf/maps.rst:25 ../../concepts/ebpf/maps.rst:27
#: ../../concepts/ebpf/maps.rst:28 ../../concepts/ebpf/maps.rst:29
#: ../../concepts/ebpf/maps.rst:30 ../../concepts/ebpf/maps.rst:31
#: ../../concepts/ebpf/maps.rst:32
msgid "node"
msgstr ""

#: ../../concepts/ebpf/maps.rst:21 ../../concepts/ebpf/maps.rst:22
#: ../../concepts/ebpf/maps.rst:24 ../../concepts/ebpf/maps.rst:27
msgid "512k"
msgstr ""

#: ../../concepts/ebpf/maps.rst:21
msgid "Max 512k NAT entries"
msgstr ""

#: ../../concepts/ebpf/maps.rst:22
msgid "Neighbor Table"
msgstr ""

#: ../../concepts/ebpf/maps.rst:22
msgid "Max 512k neighbor entries"
msgstr ""

#: ../../concepts/ebpf/maps.rst:23
msgid "Endpoints"
msgstr ""

#: ../../concepts/ebpf/maps.rst:23 ../../concepts/ebpf/maps.rst:25
#: ../../concepts/ebpf/maps.rst:28 ../../concepts/ebpf/maps.rst:30
#: ../../concepts/ebpf/maps.rst:32
msgid "64k"
msgstr ""

#: ../../concepts/ebpf/maps.rst:23
msgid "Max 64k local endpoints + host IPs per node"
msgstr ""

#: ../../concepts/ebpf/maps.rst:24
msgid "IP cache"
msgstr ""

#: ../../concepts/ebpf/maps.rst:24
msgid ""
"Max 256k endpoints (IPv4+IPv6), max 512k endpoints (IPv4 or IPv6) across "
"all clusters"
msgstr ""

#: ../../concepts/ebpf/maps.rst:25
msgid "Load Balancer"
msgstr ""

#: ../../concepts/ebpf/maps.rst:25
msgid "Max 64k cumulative backends across all services across all clusters"
msgstr ""

#: ../../concepts/ebpf/maps.rst:26
msgid "Policy"
msgstr ""

#: ../../concepts/ebpf/maps.rst:26 ../../concepts/ebpf/maps.rst:33
msgid "endpoint"
msgstr ""

#: ../../concepts/ebpf/maps.rst:26 ../../concepts/ebpf/maps.rst:31
#: ../../concepts/ebpf/maps.rst:33
msgid "16k"
msgstr ""

#: ../../concepts/ebpf/maps.rst:26
msgid "Max 16k allowed identity + port + protocol pairs for specific endpoint"
msgstr ""

#: ../../concepts/ebpf/maps.rst:27
msgid "Proxy Map"
msgstr ""

#: ../../concepts/ebpf/maps.rst:27
msgid "Max 512k concurrent redirected TCP connections to proxy"
msgstr ""

#: ../../concepts/ebpf/maps.rst:28
msgid "Tunnel"
msgstr ""

#: ../../concepts/ebpf/maps.rst:28
msgid "Max 32k nodes (IPv4+IPv6) or 64k nodes (IPv4 or IPv6) across all clusters"
msgstr ""

#: ../../concepts/ebpf/maps.rst:29
msgid "IPv4 Fragmentation"
msgstr ""

#: ../../concepts/ebpf/maps.rst:29
msgid "8k"
msgstr ""

#: ../../concepts/ebpf/maps.rst:29
msgid "Max 8k fragmented datagrams in flight simultaneously on the node"
msgstr ""

#: ../../concepts/ebpf/maps.rst:30
msgid "Session Affinity"
msgstr ""

#: ../../concepts/ebpf/maps.rst:30
msgid "Max 64k affinities from different clients"
msgstr ""

#: ../../concepts/ebpf/maps.rst:31
msgid "IP Masq"
msgstr ""

#: ../../concepts/ebpf/maps.rst:31
msgid "Max 16k IPv4 cidrs used by BPF-based ip-masq-agent"
msgstr ""

#: ../../concepts/ebpf/maps.rst:32
msgid "Service Source Ranges"
msgstr ""

#: ../../concepts/ebpf/maps.rst:32
msgid "Max 64k cumulative LB source ranges across all services"
msgstr ""

#: ../../concepts/ebpf/maps.rst:33
msgid "Egress Policy"
msgstr ""

#: ../../concepts/ebpf/maps.rst:33
msgid "Max 16k endpoints across all destination CIDRs across all clusters"
msgstr ""

#: ../../concepts/ebpf/maps.rst:36
msgid ""
"For some BPF maps, the upper capacity limit can be overridden using "
"command line options for ``cilium-agent``. A given capacity can be set "
"using ``--bpf-ct-global-tcp-max``, ``--bpf-ct-global-any-max``, ``--bpf-"
"nat-global-max``, ``--bpf-neigh-global-max``, ``--bpf-policy-map-max``, "
"``--bpf-fragments-map-max`` and ``--bpf-lb-map-max``."
msgstr ""

#: ../../concepts/ebpf/maps.rst:44
msgid ""
"In case the ``--bpf-ct-global-tcp-max`` and/or ``--bpf-ct-global-any-"
"max`` are specified, the NAT table size (``--bpf-nat-global-max``) must "
"not exceed 2/3 of the combined CT table size (TCP + UDP). This will "
"automatically be set if either ``--bpf-nat-global-max`` is not explicitly"
" set or if dynamic BPF map sizing is used (see below)."
msgstr ""

#: ../../concepts/ebpf/maps.rst:50
#, python-format
msgid ""
"Using the ``--bpf-map-dynamic-size-ratio`` flag, the upper capacity "
"limits of several large BPF maps are determined at agent startup based on"
" the given ratio of the total system memory. For example, a given ratio "
"of 0.0025 leads to 0.25% of the total system memory to be used for these "
"maps."
msgstr ""

#: ../../concepts/ebpf/maps.rst:55
msgid ""
"This flag affects the following BPF maps that consume most memory in the "
"system: ``cilium_ct_{4,6}_global``, ``cilium_ct_{4,6}_any``, "
"``cilium_nodeport_neigh{4,6}``, ``cilium_snat_v{4,6}_external`` and "
"``cilium_lb{4,6}_reverse_sk``."
msgstr ""

#: ../../concepts/ebpf/maps.rst:60
msgid ""
"``kube-proxy`` sets as the maximum number entries in the linux's "
"connection tracking table based on the number of cores the machine has. "
"``kube-proxy`` has a default of ``32768`` maximum entries per core with a"
" minimum of ``131072`` entries regardless of the number of cores the "
"machine has."
msgstr ""

#: ../../concepts/ebpf/maps.rst:65
msgid ""
"Cilium has its own connection tracking tables as BPF Maps and the number "
"of entries of such maps is calculated based on the amount of total memory"
" in the node with a minimum of ``131072`` entries regardless the amount "
"of memory the machine has."
msgstr ""

#: ../../concepts/ebpf/maps.rst:70
msgid ""
"The following table presents the value that ``kube-proxy`` and Cilium "
"sets for their own connection tracking tables when Cilium is configured "
"with ``--bpf-map-dynamic-size-ratio: 0.0025``."
msgstr ""

#: ../../concepts/ebpf/maps.rst:75
msgid "vCPU"
msgstr ""

#: ../../concepts/ebpf/maps.rst:75
msgid "Memory (GiB)"
msgstr ""

#: ../../concepts/ebpf/maps.rst:75
msgid "Kube-proxy CT entries"
msgstr ""

#: ../../concepts/ebpf/maps.rst:75
msgid "Cilium CT entries"
msgstr ""

#: ../../concepts/ebpf/maps.rst:77 ../../concepts/terminology.rst:207
msgid "1"
msgstr ""

#: ../../concepts/ebpf/maps.rst:77
msgid "3.75"
msgstr ""

#: ../../concepts/ebpf/maps.rst:77 ../../concepts/ebpf/maps.rst:79
#: ../../concepts/ebpf/maps.rst:81
msgid "131072"
msgstr ""

#: ../../concepts/ebpf/maps.rst:79 ../../concepts/terminology.rst:210
msgid "2"
msgstr ""

#: ../../concepts/ebpf/maps.rst:79
msgid "7.5"
msgstr ""

#: ../../concepts/ebpf/maps.rst:81 ../../concepts/terminology.rst:216
msgid "4"
msgstr ""

#: ../../concepts/ebpf/maps.rst:81
msgid "15"
msgstr ""

#: ../../concepts/ebpf/maps.rst:83
msgid "8"
msgstr ""

#: ../../concepts/ebpf/maps.rst:83
msgid "30"
msgstr ""

#: ../../concepts/ebpf/maps.rst:83
msgid "262144"
msgstr ""

#: ../../concepts/ebpf/maps.rst:83
msgid "284560"
msgstr ""

#: ../../concepts/ebpf/maps.rst:85
msgid "16"
msgstr ""

#: ../../concepts/ebpf/maps.rst:85
msgid "60"
msgstr ""

#: ../../concepts/ebpf/maps.rst:85
msgid "524288"
msgstr ""

#: ../../concepts/ebpf/maps.rst:85
msgid "569120"
msgstr ""

#: ../../concepts/ebpf/maps.rst:87
msgid "32"
msgstr ""

#: ../../concepts/ebpf/maps.rst:87
msgid "120"
msgstr ""

#: ../../concepts/ebpf/maps.rst:87
msgid "1048576"
msgstr ""

#: ../../concepts/ebpf/maps.rst:87
msgid "1138240"
msgstr ""

#: ../../concepts/ebpf/maps.rst:89
msgid "64"
msgstr ""

#: ../../concepts/ebpf/maps.rst:89
msgid "240"
msgstr ""

#: ../../concepts/ebpf/maps.rst:89
msgid "2097152"
msgstr ""

#: ../../concepts/ebpf/maps.rst:89
msgid "2276480"
msgstr ""

#: ../../concepts/ebpf/maps.rst:91
msgid "96"
msgstr ""

#: ../../concepts/ebpf/maps.rst:91
msgid "360"
msgstr ""

#: ../../concepts/ebpf/maps.rst:91
msgid "3145728"
msgstr ""

#: ../../concepts/ebpf/maps.rst:91
msgid "4552960"
msgstr ""

#: ../../concepts/index.rst:11 ../../concepts/kubernetes/concepts.rst:9
msgid "Concepts"
msgstr ""

#: ../../concepts/index.rst:13
msgid ""
"The concepts chapter provides a deeper overview and deep dives over all "
"aspects of Cilium and Hubble. If you are looking for a high-level "
"introduction of Cilium and Hubble, see the section :ref:`intro`."
msgstr ""

#: ../../concepts/index.rst:17
msgid "Choose one of the following topics to start reading:"
msgstr ""

#: ../../concepts/kubernetes/ciliumendpoint.rst:11
msgid "Endpoint CRD"
msgstr ""

#: ../../concepts/kubernetes/ciliumendpoint.rst:13
msgid ""
"When managing pods in Kubernetes, Cilium will create a Custom Resource "
"Definition (CRD) of Kind ``CiliumEndpoint``. One ``CiliumEndpoint`` is "
"created for each pod managed by Cilium, with the same name and in the "
"same namespace. The ``CiliumEndpoint`` objects contain the same "
"information as the json output of ``cilium endpoint get`` under the "
"``.status`` field, but can be fetched for all pods in the cluster.  "
"Adding the ``-o json`` will export more information about each endpoint. "
"This includes the endpoint's labels, security identity and the policy in "
"effect on it."
msgstr ""

#: ../../concepts/kubernetes/ciliumendpoint.rst:22
#: ../../concepts/kubernetes/ciliumendpointslice.rst:23
msgid "For example:"
msgstr ""

#: ../../concepts/kubernetes/ciliumendpoint.rst:35
msgid ""
"Each cilium-agent pod will create a CiliumEndpoint to represent its own "
"inter-agent health-check endpoint. These are not pods in Kubernetes and "
"are in the ``kube-system`` namespace. They are named as ``cilium-health"
"-<node-name>``"
msgstr ""

#: ../../concepts/kubernetes/ciliumendpointslice.rst:11
msgid "EndpointSlice CRD"
msgstr ""

#: ../../concepts/kubernetes/ciliumendpointslice.rst:13
msgid ""
"When managing pods in Kubernetes, Cilium will create a Custom Resource "
"Definition (CRD) of Kind :ref:`CiliumEndpoint<CiliumEndpoint>` (CEP) for "
"each pod managed by Cilium. If ``enable-cilium-endpoint-slice`` is "
"enabled, then Cilium will also create a CRD of Kind "
"``CiliumEndpointSlice`` (CES) that groups a set of slim CEP objects with "
"the same :ref:`security identity<arch_id_security>` together into a "
"single CES object and broadcast CES objects to communicate identities to "
"other agents instead of doing so via broadcasting CEP. In most cases, "
"this reduces load on the control plane and can sustain larger-scaled "
"cluster using the same master resource."
msgstr ""

#: ../../concepts/kubernetes/compatibility.rst:10
msgid "Kubernetes Compatibility"
msgstr ""

#: ../../concepts/kubernetes/compatibility.rst:12
msgid ""
"Cilium is compatible with multiple Kubernetes API Groups. Some are "
"deprecated or beta, and may only be available in specific versions of "
"Kubernetes."
msgstr ""

#: ../../concepts/kubernetes/compatibility.rst:15
msgid ""
"All Kubernetes versions listed are e2e tested and guaranteed to be "
"compatible with Cilium. Older Kubernetes versions not listed in this "
"table do not have Cilium support. Newer Kubernetes versions, while not "
"listed, will depend on the backward compatibility offered by Kubernetes."
msgstr ""

#: ../../concepts/kubernetes/compatibility.rst:21
msgid "k8s Version"
msgstr ""

#: ../../concepts/kubernetes/compatibility.rst:21
msgid "k8s NetworkPolicy API"
msgstr ""

#: ../../concepts/kubernetes/compatibility.rst:21
#: ../../concepts/kubernetes/policy.rst:60
msgid "CiliumNetworkPolicy"
msgstr ""

#: ../../concepts/kubernetes/compatibility.rst:24
msgid "1.16, 1.17, 1.18, 1.19, 1.20, 1.21, 1.22, 1.23, 1.24"
msgstr ""

#: ../../concepts/kubernetes/compatibility.rst:24
msgid "`networking.k8s.io/v1`_"
msgstr ""

#: ../../concepts/kubernetes/compatibility.rst:23
msgid "``cilium.io/v2`` has a :term:`CustomResourceDefinition`"
msgstr ""

#: ../../concepts/kubernetes/compatibility.rst:28
msgid "Cilium CRD schema validation"
msgstr ""

#: ../../concepts/kubernetes/compatibility.rst:30
msgid ""
"Cilium uses a CRD for its Network Policies in Kubernetes. This CRD might "
"have changes in its schema validation, which allows it to verify the "
"correctness of a Cilium Clusterwide Network Policy (CCNP) or a Cilium "
"Network Policy (CNP)."
msgstr ""

#: ../../concepts/kubernetes/compatibility.rst:34
msgid ""
"The CRD itself has an annotation, ``io.cilium.k8s.crd.schema.version``, "
"with the schema definition version. By default, Cilium automatically "
"updates the CRD, and its validation, with a newer one."
msgstr ""

#: ../../concepts/kubernetes/compatibility.rst:38
msgid ""
"The following table lists all Cilium versions and their expected schema "
"validation version:"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:2
msgid "Cilium Version"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:2
msgid "CNP and CCNP Schema Version"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:5
msgid "v1.9.0-rc0"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:5
msgid "1.22.1"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:7
msgid "v1.9.0-rc1"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:7
#: ../../concepts/kubernetes/compatibility-table.rst:9
msgid "1.22.2"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:9
msgid "v1.9.0-rc2"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:11
msgid "v1.9.0-rc3"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:11
#: ../../concepts/kubernetes/compatibility-table.rst:13
#: ../../concepts/kubernetes/compatibility-table.rst:15
#: ../../concepts/kubernetes/compatibility-table.rst:17
#: ../../concepts/kubernetes/compatibility-table.rst:19
#: ../../concepts/kubernetes/compatibility-table.rst:21
#: ../../concepts/kubernetes/compatibility-table.rst:23
msgid "1.22.3"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:13
msgid "v1.9.0"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:15
msgid "v1.9.1"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:17
msgid "v1.9.2"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:19
msgid "v1.9.3"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:21
msgid "v1.9.4"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:23
msgid "v1.9.5"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:25
msgid "v1.9.6"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:25
msgid "1.22.4"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:27
msgid "v1.9.7"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:27
#: ../../concepts/kubernetes/compatibility-table.rst:29
msgid "1.22.5"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:29
msgid "v1.9.8"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:31
msgid "v1.9.9"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:31
#: ../../concepts/kubernetes/compatibility-table.rst:33
#: ../../concepts/kubernetes/compatibility-table.rst:35
#: ../../concepts/kubernetes/compatibility-table.rst:37
#: ../../concepts/kubernetes/compatibility-table.rst:39
#: ../../concepts/kubernetes/compatibility-table.rst:41
msgid "1.22.6"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:33
msgid "v1.9.10"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:35
msgid "v1.9.11"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:37
msgid "v1.9.12"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:39
msgid "v1.9.13"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:41
msgid "v1.9"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:43
msgid "v1.10.0-rc0"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:43
msgid "1.23.1"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:45
msgid "v1.10.0-rc1"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:45
#: ../../concepts/kubernetes/compatibility-table.rst:47
#: ../../concepts/kubernetes/compatibility-table.rst:49
#: ../../concepts/kubernetes/compatibility-table.rst:51
msgid "1.23.2"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:47
msgid "v1.10.0-rc2"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:49
msgid "v1.10.0"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:51
msgid "v1.10.1"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:53
msgid "v1.10.2"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:53
#: ../../concepts/kubernetes/compatibility-table.rst:55
#: ../../concepts/kubernetes/compatibility-table.rst:57
#: ../../concepts/kubernetes/compatibility-table.rst:59
msgid "1.23.3"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:55
msgid "v1.10.3"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:57
msgid "v1.10.4"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:59
msgid "v1.10.5"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:61
msgid "v1.10.6"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:61
#: ../../concepts/kubernetes/compatibility-table.rst:63
#: ../../concepts/kubernetes/compatibility-table.rst:65
#: ../../concepts/kubernetes/compatibility-table.rst:67
#: ../../concepts/kubernetes/compatibility-table.rst:69
msgid "1.23.4"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:63
msgid "v1.10.7"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:65
msgid "v1.10.8"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:67
msgid "v1.10.9"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:69
msgid "v1.10"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:71
msgid "v1.11.0-rc0"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:71
msgid "1.24.1"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:73
msgid "v1.11.0-rc1"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:73
#: ../../concepts/kubernetes/compatibility-table.rst:75
#: ../../concepts/kubernetes/compatibility-table.rst:77
#: ../../concepts/kubernetes/compatibility-table.rst:79
msgid "1.24.2"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:75
msgid "v1.11.0-rc2"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:77
msgid "v1.11.0-rc3"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:79
msgid "v1.11.0"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:81
msgid "v1.11.1"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:81
#: ../../concepts/kubernetes/compatibility-table.rst:83
#: ../../concepts/kubernetes/compatibility-table.rst:85
msgid "1.24.3"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:83
msgid "v1.11.2"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:85
msgid "v1.11"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:87
msgid "v1.12.0-rc0"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:87
#: ../../concepts/kubernetes/compatibility-table.rst:89
msgid "1.25.1"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:89
msgid "v1.12.0-rc1"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:91
msgid "v1.12.0-rc2"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:91
#: ../../concepts/kubernetes/compatibility-table.rst:93
msgid "1.25.2"
msgstr ""

#: ../../concepts/kubernetes/compatibility-table.rst:93
msgid "latest / master"
msgstr ""

#: ../../concepts/kubernetes/concepts.rst:14 ../../concepts/terminology.rst:254
msgid "Deployment"
msgstr ""

#: ../../concepts/kubernetes/concepts.rst:16
msgid ""
"The configuration of a standard Cilium Kubernetes deployment consists of "
"several Kubernetes resources:"
msgstr ""

#: ../../concepts/kubernetes/concepts.rst:19
msgid ""
"A ``DaemonSet`` resource:  describes the Cilium pod that is deployed to "
"each Kubernetes node.  This pod runs the cilium-agent and associated "
"daemons. The configuration of this DaemonSet includes the image tag "
"indicating the exact version of the Cilium docker container (e.g., "
"v1.0.0) and command-line options passed to the cilium-agent."
msgstr ""

#: ../../concepts/kubernetes/concepts.rst:25
msgid ""
"A ``ConfigMap`` resource:  describes common configuration values that are"
" passed to the cilium-agent, such as the kvstore endpoint and "
"credentials, enabling/disabling debug mode, etc."
msgstr ""

#: ../../concepts/kubernetes/concepts.rst:29
msgid ""
"``ServiceAccount``, ``ClusterRole``, and ``ClusterRoleBindings`` "
"resources: the identity and permissions used by cilium-agent to access "
"the Kubernetes API server when Kubernetes RBAC is enabled."
msgstr ""

#: ../../concepts/kubernetes/concepts.rst:33
msgid ""
"A ``Secret`` resource: describes the credentials use access the etcd "
"kvstore, if required."
msgstr ""

#: ../../concepts/kubernetes/concepts.rst:37
msgid "Networking For Existing Pods"
msgstr ""

#: ../../concepts/kubernetes/concepts.rst:39
msgid ""
"In case pods were already running before the Cilium :term:`DaemonSet` was"
" deployed, these pods will still be connected using the previous "
"networking plugin according to the CNI configuration. A typical example "
"for this is the ``kube-dns`` service which runs in the ``kube-system`` "
"namespace by default."
msgstr ""

#: ../../concepts/kubernetes/concepts.rst:44
msgid ""
"A simple way to change networking for such existing pods is to rely on "
"the fact that Kubernetes automatically restarts pods in a Deployment if "
"they are deleted, so we can simply delete the original kube-dns pod and "
"the replacement pod started immediately after will have networking "
"managed by Cilium.  In a production deployment, this step could be "
"performed as a rolling update of kube-dns pods to avoid downtime of the "
"DNS service."
msgstr ""

#: ../../concepts/kubernetes/concepts.rst:56
msgid ""
"Running ``kubectl get pods`` will show you that Kubernetes started a new "
"set of ``kube-dns`` pods while at the same time terminating the old pods:"
msgstr ""

#: ../../concepts/kubernetes/concepts.rst:70
msgid "Default Ingress Allow from Local Host"
msgstr ""

#: ../../concepts/kubernetes/concepts.rst:72
msgid ""
"Kubernetes has functionality to indicate to users the current health of "
"their applications via `Liveness Probes and Readiness Probes "
"<https://kubernetes.io/docs/tasks/configure-pod-container/configure-"
"liveness-readiness-startup-probes/>`_. In order for ``kubelet`` to run "
"these health checks for each pod, by default, Cilium will always allow "
"all ingress traffic from the local host to each pod."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:11
#: ../../concepts/networking/ipam/azure.rst:48
#: ../../concepts/networking/ipam/cluster-pool.rst:43
#: ../../concepts/networking/ipam/cluster-pool.rst:104
#: ../../concepts/networking/ipam/crd.rst:45
#: ../../concepts/networking/ipam/eni.rst:56
#: ../../concepts/networking/ipam/gke.rst:36
#: ../../concepts/networking/ipam/kubernetes.rst:59
#: ../../concepts/networking/masquerading.rst:28
#: ../../concepts/networking/routing.rst:128
#: ../../concepts/networking/routing.rst:220
#: ../../concepts/networking/routing.rst:287
msgid "Configuration"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:14
msgid "ConfigMap Options"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:16
msgid ""
"In the :term:`ConfigMap` there are several options that can be configured"
" according to your preferences:"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:19
msgid ""
"``debug`` - Sets to run Cilium in full debug mode, which enables verbose "
"logging and configures eBPF programs to emit more visibility events into "
"the output of ``cilium monitor``."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:23
msgid "``enable-ipv4`` - Enable IPv4 addressing support"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:25
msgid "``enable-ipv6`` - Enable IPv6 addressing support"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:27
msgid ""
"``clean-cilium-bpf-state`` - Removes all eBPF state from the filesystem "
"on startup. Endpoints will be restored with the same IP addresses, but "
"ongoing connections may be briefly disrupted and loadbalancing decisions "
"will be lost, so active connections via the loadbalancer will break. All "
"eBPF state will be reconstructed from their original sources (for "
"example, from kubernetes or the kvstore). This may be used to mitigate "
"serious issues regarding eBPF maps. This option should be turned off "
"again after restarting the daemon."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:36
msgid ""
"``clean-cilium-state`` - Removes **all** Cilium state, including "
"unrecoverable information such as all endpoint state, as well as "
"recoverable state such as eBPF state pinned to the filesystem, CNI "
"configuration files, library code, links, routes, and other information. "
"**This operation is irreversible**. Existing endpoints currently managed "
"by Cilium may continue to operate as before, but Cilium will no longer "
"manage them and they may stop working without warning. After using this "
"operation, endpoints must be deleted and reconnected to allow the new "
"instance of Cilium to manage them."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:45
msgid ""
"``monitor-aggregation`` - This option enables coalescing of tracing "
"events in ``cilium monitor`` to only include periodic updates from active"
" flows, or any packets that involve an L4 connection state change. Valid "
"options are ``none``, ``low``, ``medium``, ``maximum``."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:50
msgid "``none`` - Generate a tracing event on every receive and send packet."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:51
msgid "``low`` - Generate a tracing event on every send packet."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:52
msgid ""
"``medium`` - Generate a tracing event on every new connection, any time a"
" packet contains TCP flags that have not been previously seen for the "
"packet direction, and on average once per ``monitor-aggregation-"
"interval`` (assuming that a packet is seen during the interval). Each "
"direction tracks TCP flags and report interval separately. If Cilium "
"drops a packet, it will emit one event per packet dropped."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:58
msgid ""
"``maximum`` - An alias for the most aggressive aggregation level. "
"Currently this is equivalent to setting ``monitor-aggregation`` to "
"``medium``."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:61
msgid ""
"``monitor-aggregation-interval`` - Defines the interval to report tracing"
" events. Only applicable for ``monitor-aggregation`` levels ``medium`` or"
" higher. Assuming new packets are sent at least once per interval, this "
"ensures that on average one event is sent during the interval."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:66
msgid ""
"``preallocate-bpf-maps`` - Pre-allocation of map entries allows per-"
"packet latency to be reduced, at the expense of up-front memory "
"allocation for the entries in the maps. Set to ``true`` to optimize for "
"latency. If this value is modified, then during the next Cilium startup "
"connectivity may be temporarily disrupted for endpoints with active "
"connections."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:72
msgid ""
"Any changes that you perform in the Cilium :term:`ConfigMap` and in "
"``cilium-etcd-secrets`` ``Secret`` will require you to restart any "
"existing Cilium pods in order for them to pick the latest configuration."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:78
msgid ""
"When updating keys or values in the ConfigMap, the changes might take up "
"to 2 minutes to be propagated to all nodes running in the cluster. For "
"more information see the official Kubernetes docs: `Mounted ConfigMaps "
"are updated automatically <https://kubernetes.io/docs/tasks/configure-"
"pod-container/configure-pod-configmap/#mounted-configmaps-are-updated-"
"automatically>`__"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:83
msgid ""
"The following :term:`ConfigMap` is an example where the etcd cluster is "
"running in 2 nodes, ``node-1`` and ``node-2`` with TLS, and client to "
"server authentication enabled."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:126
msgid "CNI"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:128
msgid ""
":term:`CNI` - Container Network Interface is the plugin layer used by "
"Kubernetes to delegate networking configuration. You can find additional "
"information on the :term:`CNI` project website."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:132
msgid ""
"Kubernetes `` >= 1.3.5`` requires the ``loopback`` :term:`CNI` plugin to "
"be installed on all worker nodes. The binary is typically provided by "
"most Kubernetes distributions. See section :ref:`install_cni` for "
"instructions on how to install :term:`CNI` in case the ``loopback`` "
"binary is not already installed on your worker nodes."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:138
msgid ""
"CNI configuration is automatically being taken care of when deploying "
"Cilium via the provided :term:`DaemonSet`. The script ``cni-install.sh`` "
"is automatically run via the ``postStart`` mechanism when the ``cilium`` "
"pod is started."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:142
msgid ""
"In order for the ``cni-install.sh`` script to work properly, the "
"``kubelet`` task must either be running on the host filesystem of the "
"worker node, or the ``/etc/cni/net.d`` and ``/opt/cni/bin`` directories "
"must be mounted into the container where ``kubelet`` is running. This can"
" be achieved with :term:`Volumes` mounts."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:148
msgid "The CNI auto installation is performed as follows:"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:150
msgid ""
"The ``/etc/cni/net.d`` and ``/opt/cni/bin`` directories are mounted from "
"the host filesystem into the pod where Cilium is running."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:153
msgid ""
"The file ``/etc/cni/net.d/05-cilium.conf`` is written in case it does not"
" exist yet."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:156
msgid ""
"The binary ``cilium-cni`` is installed to ``/opt/cni/bin``. Any existing "
"binary with the name ``cilium-cni`` is overwritten."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:162
msgid "Manually installing CNI"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:164
msgid ""
"This step is typically already included in all Kubernetes distributions "
"or Kubernetes installers but can be performed manually:"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:176
msgid "Adjusting CNI configuration"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:178
msgid ""
"The CNI configuration file is automatically written and maintained by the"
" scripts ``cni-install.sh`` and ``cni-uninstall.sh`` which are running as"
" ``postStart`` and ``preStop`` hooks of the Cilium pod."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:182
msgid ""
"If you want to provide your own custom CNI configuration file, set the "
"``CILIUM_CUSTOM_CNI_CONF`` environment variable to avoid overwriting the "
"configuration file by adding the following to the ``env:`` section of the"
" ``cilium`` DaemonSet:"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:192
msgid ""
"The CNI installation can be configured with environment variables. These "
"environment variables can be specified in the :term:`DaemonSet` file like"
" this:"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:201
msgid "The following variables are supported:"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:204
msgid "Option"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:204
#: ../../concepts/networking/ipam/cluster-pool.rst:36
#: ../../concepts/networking/ipam/cluster-pool.rst:79
#: ../../concepts/networking/ipam/kubernetes.rst:28
#: ../../concepts/networking/ipam/kubernetes.rst:41
#: ../../concepts/terminology.rst:120 ../../concepts/terminology.rst:203
msgid "Description"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:204
msgid "Default"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:206
msgid "HOST_PREFIX"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:206
msgid "Path prefix of all host mounts"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:206
msgid "/host"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:208
msgid "CNI_DIR"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:208
msgid "Path to mounted CNI directory"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:208
msgid "${HOST_PREFIX}/opt/cni"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:210
msgid "CNI_CONF_NAME"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:210
msgid "Name of configuration file"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:210
msgid "05-cilium.conf"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:213
msgid ""
"If you want to further adjust the CNI configuration you may do so by "
"creating the CNI configuration ``/etc/cni/net.d/05-cilium.conf`` "
"manually:"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:225
msgid ""
"Cilium will use any existing ``/etc/cni/net.d/05-cilium.conf`` file if it"
" already exists on a worker node and only creates it if it does not exist"
" yet."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:229
msgid "CRD Validation"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:231
msgid ""
"Custom Resource Validation was introduced in Kubernetes since version "
"``1.8.0``. This is still considered an alpha feature in Kubernetes "
"``1.8.0`` and beta in Kubernetes ``1.9.0``."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:235
msgid ""
"Since Cilium ``v1.0.0-rc3``, Cilium will create, or update in case it "
"exists, the Cilium Network Policy (CNP) Resource Definition with the "
"embedded validation schema. This allows the validation of "
"CiliumNetworkPolicy to be done on the kube-apiserver when the policy is "
"imported with an ability to provide direct feedback when importing the "
"resource."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:241
msgid ""
"To enable this feature, the flag ``--feature-"
"gates=CustomResourceValidation=true`` must be set when starting kube-"
"apiserver. Cilium itself will automatically make use of this feature and "
"no additional flag is required."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:245
msgid ""
"In case there is an invalid CNP before updating to Cilium ``v1.0.0-rc3``,"
" which contains the validator, the kube-apiserver validator will prevent "
"Cilium from updating that invalid CNP with Cilium node status. By "
"checking Cilium logs for ``unable to update CNP, retrying...``, it is "
"possible to determine which Cilium Network Policies are considered "
"invalid after updating to Cilium ``v1.0.0-rc3``."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:253
msgid ""
"To verify that the CNP resource definition contains the validation "
"schema, run the following command:"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:256
msgid "``kubectl get crd ciliumnetworkpolicies.cilium.io -o json``"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:275
msgid ""
"In case the user writes a policy that does not conform to the schema, "
"Kubernetes will return an error, e.g.:"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:313
msgid "In this case, the policy has a port out of the 0-65535 range."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:318
msgid "Mounting BPFFS with systemd"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:320
msgid ""
"Due to how systemd `mounts "
"<https://unix.stackexchange.com/questions/283442/systemd-mount-fails-"
"where-setting-doesnt-match-unit-name>`__ filesystems, the mount point "
"path must be reflected in the unit filename."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:345
msgid "Container Runtimes"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:350
msgid "CRIO"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:352
msgid "If you want to use CRIO, generate the YAML using:"
msgstr ""

#: ../../gettingstarted/k8s-install-download-release.rst:9
msgid ""
"Make sure you have Helm 3 `installed "
"<https://helm.sh/docs/intro/install/>`_. Helm 2 is `no longer supported "
"<https://helm.sh/blog/helm-v2-deprecation-timeline/>`_."
msgstr ""

#: ../../gettingstarted/k8s-install-download-release.rst:14
msgid "Setup Helm repository:"
msgstr ""

#: ../../gettingstarted/k8s-install-download-release.rst:22
msgid ""
"Download the Cilium release tarball and change to the kubernetes install "
"directory:"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:358
msgid ""
"The helm ``--set containerRuntime.integration=crio`` might not be "
"required for your setup. For more info see :ref:`crio-known-issues`."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:367
msgid ""
"Since CRI-O does not automatically detect that a new CNI plugin has been "
"installed, you will need to restart the CRI-O daemon for it to pick up "
"the Cilium CNI configuration."
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:371
msgid "First make sure cilium is running:"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:379
msgid "After that you can restart CRI-O:"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:388
msgid "Common CRIO issues"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:390
msgid ""
"Some CRI-O environments automatically mount the bpf filesystem in the "
"pods, which is something that Cilium avoids doing when ``--set "
"containerRuntime.integration=crio`` is set. However, some CRI-O "
"environments do not mount the bpf filesystem automatically which causes "
"Cilium to print the following message::"
msgstr ""

#: ../../concepts/kubernetes/configuration.rst:404
msgid ""
"If you see this warning in the Cilium pod logs with your CRI-O "
"environment, please remove the flag ``--set "
"containerRuntime.integration=crio`` from your helm setup and redeploy "
"Cilium."
msgstr ""

#: ../../concepts/kubernetes/index.rst:11
msgid "Kubernetes Integration"
msgstr ""

#: ../../concepts/kubernetes/index.rst:13
msgid "The following sections describe the Kubernetes integration in detail:"
msgstr ""

#: ../../concepts/kubernetes/intro.rst:14
msgid "What does Cilium provide in your Kubernetes Cluster?"
msgstr ""

#: ../../concepts/kubernetes/intro.rst:16
msgid ""
"The following functionality is provided as your run Cilium in your "
"Kubernetes cluster:"
msgstr ""

#: ../../concepts/kubernetes/intro.rst:19
msgid ""
":term:`CNI` plugin support to provide pod_connectivity_ with `multi host "
"networking`."
msgstr ""

#: ../../concepts/kubernetes/intro.rst:21
msgid ""
"Identity based implementation of the `NetworkPolicy` resource to isolate "
":term:`pod<Pod>` to pod connectivity on Layer 3 and 4."
msgstr ""

#: ../../concepts/kubernetes/intro.rst:23
msgid ""
"An extension to NetworkPolicy in the form of a "
":term:`CustomResourceDefinition` which extends policy control to add:"
msgstr ""

#: ../../concepts/kubernetes/intro.rst:26
msgid ""
"Layer 7 policy enforcement on ingress and egress for the following "
"application protocols:"
msgstr ""

#: ../../concepts/kubernetes/intro.rst:29
msgid "HTTP"
msgstr ""

#: ../../concepts/kubernetes/intro.rst:30
msgid "Kafka"
msgstr ""

#: ../../concepts/kubernetes/intro.rst:31
msgid "Egress support for CIDRs to secure access to external services"
msgstr ""

#: ../../concepts/kubernetes/intro.rst:32
msgid ""
"Enforcement to external headless services to automatically restrict to "
"the set of Kubernetes endpoints configured for a service."
msgstr ""

#: ../../concepts/kubernetes/intro.rst:34
msgid ""
"ClusterIP implementation to provide distributed load-balancing for pod to"
" pod traffic."
msgstr ""

#: ../../concepts/kubernetes/intro.rst:36
msgid "Fully compatible with existing kube-proxy model"
msgstr ""

#: ../../concepts/kubernetes/intro.rst:41
msgid "Pod-to-Pod Connectivity"
msgstr ""

#: ../../concepts/kubernetes/intro.rst:43
msgid ""
"In Kubernetes, containers are deployed within units referred to as "
":term:`Pods<Pod>`, which include one or more containers reachable via a "
"single IP address.  With Cilium, each Pod gets an IP address from the "
"node prefix of the Linux node running the Pod. See "
":ref:`address_management` for additional details. In the absence of any "
"network security policies, all Pods can reach each other."
msgstr ""

#: ../../concepts/kubernetes/intro.rst:49
msgid ""
"Pod IP addresses are typically local to the Kubernetes cluster. If pods "
"need to reach services outside the cluster as a client, the network "
"traffic is automatically masqueraded as it leaves the node."
msgstr ""

#: ../../concepts/kubernetes/intro.rst:54
msgid "Service Load-balancing"
msgstr ""

#: ../../concepts/kubernetes/intro.rst:56
msgid ""
"Kubernetes has developed the Services abstraction which provides the user"
" the ability to load balance network traffic to different pods. This "
"abstraction allows the pods reaching out to other pods by a single IP "
"address, a virtual IP address, without knowing all the pods that are "
"running that particular service."
msgstr ""

#: ../../concepts/kubernetes/intro.rst:61
msgid ""
"Without Cilium, kube-proxy is installed on every node, watches for "
"endpoints and services addition and removal on the kube-master which "
"allows it to apply the necessary enforcement on iptables. Thus, the "
"received and sent traffic from and to the pods are properly routed to the"
" node and port serving for that service. For more information you can "
"check out the kubernetes user guide for `Services "
"<https://kubernetes.io/docs/concepts/services-networking/service/>`_."
msgstr ""

#: ../../concepts/kubernetes/intro.rst:68
msgid ""
"When implementing ClusterIP, Cilium acts on the same principles as kube-"
"proxy, it watches for services addition or removal, but instead of doing "
"the enforcement on the iptables, it updates eBPF map entries on each "
"node. For more information, see the `Pull Request "
"<https://github.com/cilium/cilium/pull/109>`__."
msgstr ""

#: ../../concepts/kubernetes/intro.rst:75
msgid "Further Reading"
msgstr ""

#: ../../concepts/kubernetes/intro.rst:77
msgid ""
"The Kubernetes documentation contains more background on the `Kubernetes "
"Networking Model <https://kubernetes.io/docs/concepts/cluster-"
"administration/networking/>`_ and `Kubernetes Network Plugins "
"<https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-"
"net/network-plugins/>`_ ."
msgstr ""

#: ../../concepts/kubernetes/policy.rst:11
msgid "Network Policy"
msgstr ""

#: ../../concepts/kubernetes/policy.rst:13
msgid ""
"If you are running Cilium on Kubernetes, you can benefit from Kubernetes "
"distributing policies for you. In this mode, Kubernetes is responsible "
"for distributing the policies across all nodes and Cilium will "
"automatically apply the policies. Three formats are available to "
"configure network policies natively with Kubernetes:"
msgstr ""

#: ../../concepts/kubernetes/policy.rst:19
msgid ""
"The standard `NetworkPolicy` resource which at the time of this writing, "
"supports to specify L3/L4 ingress policies with limited egress support "
"marked as beta."
msgstr ""

#: ../../concepts/kubernetes/policy.rst:23
msgid ""
"The extended `CiliumNetworkPolicy` format which is available as a "
":term:`CustomResourceDefinition` which supports specification of policies"
" at Layers 3-7 for both ingress and egress."
msgstr ""

#: ../../concepts/kubernetes/policy.rst:27
msgid ""
"The `CiliumClusterwideNetworkPolicy` format which is a cluster-scoped "
":term:`CustomResourceDefinition` for specifying cluster-wide policies to "
"be enforced by Cilium. The specification is same as that of "
"`CiliumNetworkPolicy` with no specified namespace."
msgstr ""

#: ../../concepts/kubernetes/policy.rst:32
msgid ""
"Cilium supports running multiple of these policy types at the same time. "
"However caution should be applied when using multiple policy types at the"
" same time, as it can be confusing to understand the complete set of "
"allowed traffic across multiple policy types.  If close attention is not "
"applied this may lead to unintended policy allow behavior."
msgstr ""

#: ../../concepts/kubernetes/policy.rst:42
msgid "NetworkPolicy"
msgstr ""

#: ../../concepts/kubernetes/policy.rst:44
msgid ""
"For more information, see the official `NetworkPolicy documentation "
"<https://kubernetes.io/docs/concepts/services-networking/network-"
"policies/>`_."
msgstr ""

#: ../../concepts/kubernetes/policy.rst:47
msgid "Known missing features for Kubernetes Network Policy:"
msgstr ""

#: ../../concepts/kubernetes/policy.rst:50
msgid "Feature"
msgstr ""

#: ../../concepts/kubernetes/policy.rst:50
msgid "Tracking Issue"
msgstr ""

#: ../../concepts/kubernetes/policy.rst:52
msgid "``ipBlock`` set with a pod IP"
msgstr ""

#: ../../concepts/kubernetes/policy.rst:52
msgid ":gh-issue:`9209`"
msgstr ""

#: ../../concepts/kubernetes/policy.rst:54
msgid "SCTP"
msgstr ""

#: ../../concepts/kubernetes/policy.rst:54
msgid ":gh-issue:`5719`"
msgstr ""

#: ../../concepts/kubernetes/policy.rst:62
msgid ""
"The `CiliumNetworkPolicy` is very similar to the standard "
"`NetworkPolicy`. The purpose is provide the functionality which is not "
"yet supported in `NetworkPolicy`. Ideally all of the functionality will "
"be merged into the standard resource format and this CRD will no longer "
"be required."
msgstr ""

#: ../../concepts/kubernetes/policy.rst:67
msgid "The raw specification of the resource in Go looks like this:"
msgstr ""

#: ../../concepts/kubernetes/policy.rst:95
msgid "Metadata"
msgstr ""

#: ../../concepts/kubernetes/policy.rst:91
msgid "Describes the policy. This includes:"
msgstr ""

#: ../../concepts/kubernetes/policy.rst:93
msgid "Name of the policy, unique within a namespace"
msgstr ""

#: ../../concepts/kubernetes/policy.rst:94
msgid "Namespace of where the policy has been injected into"
msgstr ""

#: ../../concepts/kubernetes/policy.rst:95
msgid "Set of labels to identify resource in Kubernetes"
msgstr ""

#: ../../concepts/kubernetes/policy.rst:97
msgid "Spec"
msgstr ""

#: ../../concepts/kubernetes/policy.rst:98
msgid "Field which contains a :ref:`policy_rule`"
msgstr ""

#: ../../concepts/kubernetes/policy.rst:101
msgid "Specs"
msgstr ""

#: ../../concepts/kubernetes/policy.rst:100
msgid ""
"Field which contains a list of :ref:`policy_rule`. This field is useful "
"if multiple rules must be removed or added automatically."
msgstr ""

#: ../../concepts/kubernetes/policy.rst:104
msgid "Status"
msgstr ""

#: ../../concepts/kubernetes/policy.rst:104
msgid "Provides visibility into whether the policy has been successfully applied"
msgstr ""

#: ../../concepts/kubernetes/policy.rst:107
msgid "Examples"
msgstr ""

#: ../../concepts/kubernetes/policy.rst:109
msgid ""
"See :ref:`policy_examples`, :ref:`l4_policy` and :ref:`l7_policy` for "
"detailed lists of example policies."
msgstr ""

#: ../../concepts/kubernetes/policy.rst:116
msgid "CiliumClusterwideNetworkPolicy"
msgstr ""

#: ../../concepts/kubernetes/policy.rst:118
msgid ""
"`CiliumClusterwideNetworkPolicy` is similar to `CiliumNetworkPolicy`, "
"except (1) policies defined by `CiliumClusterwideNetworkPolicy` are non-"
"namespaced and cluster-scoped, and (2) it enables the use of "
":ref:`NodeSelector`. Internally the policy is identical to "
"`CiliumNetworkPolicy` and thus the effects of this policy specification "
"are also same."
msgstr ""

#: ../../concepts/kubernetes/policy.rst:124
msgid "The raw specification of the resource in go looks like this:"
msgstr ""

#: ../../concepts/kubernetes/requirements.rst:11
msgid "Requirements"
msgstr ""

#: ../../concepts/kubernetes/requirements.rst:14
msgid "Kubernetes Version"
msgstr ""

#: ../../concepts/kubernetes/requirements.rst:16
msgid ""
"All Kubernetes versions listed are e2e tested and guaranteed to be "
"compatible with this Cilium version. Older Kubernetes versions not listed"
" here do not have Cilium support. Newer Kubernetes versions, while not "
"listed, will depend on the backward compatibility offered by Kubernetes."
msgstr ""

#: ../../concepts/kubernetes/requirements.rst:21
msgid "1.16"
msgstr ""

#: ../../concepts/kubernetes/requirements.rst:22
msgid "1.17"
msgstr ""

#: ../../concepts/kubernetes/requirements.rst:23
msgid "1.18"
msgstr ""

#: ../../concepts/kubernetes/requirements.rst:24
msgid "1.19"
msgstr ""

#: ../../concepts/kubernetes/requirements.rst:25
msgid "1.20"
msgstr ""

#: ../../concepts/kubernetes/requirements.rst:26
msgid "1.21"
msgstr ""

#: ../../concepts/kubernetes/requirements.rst:27
msgid "1.22"
msgstr ""

#: ../../concepts/kubernetes/requirements.rst:28
msgid "1.23"
msgstr ""

#: ../../concepts/kubernetes/requirements.rst:29
msgid "1.24"
msgstr ""

#: ../../concepts/kubernetes/requirements.rst:32
msgid "System Requirements"
msgstr ""

#: ../../concepts/kubernetes/requirements.rst:34
msgid ""
"Cilium requires a Linux kernel >= 4.9. See :ref:`admin_system_reqs` for "
"the full details on all systems requirements."
msgstr ""

#: ../../concepts/kubernetes/requirements.rst:38
msgid "Enable CNI in Kubernetes"
msgstr ""

#: ../../concepts/kubernetes/requirements.rst:40
msgid ""
":term:`CNI` - Container Network Interface is the plugin layer used by "
"Kubernetes to delegate networking configuration. CNI must be enabled in "
"your Kubernetes cluster in order to install Cilium. This is done by "
"passing ``--network-plugin=cni`` to kubelet on all nodes. For more "
"information, see the `Kubernets CNI network-plugins documentation "
"<https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-"
"net/network-plugins/>`_."
msgstr ""

#: ../../concepts/kubernetes/requirements.rst:47
msgid "Enable automatic node CIDR allocation (Recommended)"
msgstr ""

#: ../../concepts/kubernetes/requirements.rst:49
msgid ""
"Kubernetes has the capability to automatically allocate and assign a per "
"node IP allocation CIDR. Cilium automatically uses this feature if "
"enabled. This is the easiest method to handle IP allocation in a "
"Kubernetes cluster. To enable this feature, simply add the following flag"
" when starting ``kube-controller-manager``:"
msgstr ""

#: ../../concepts/kubernetes/requirements.rst:59
msgid "This option is not required but highly recommended."
msgstr ""

#: ../../concepts/kubernetes/troubleshooting.rst:11
#: ../../concepts/networking/ipam/cluster-pool.rst:50
#: ../../concepts/networking/ipam/gke.rst:44
msgid "Troubleshooting"
msgstr ""

#: ../../concepts/kubernetes/troubleshooting.rst:14
msgid "Verifying the installation"
msgstr ""

#: ../../concepts/kubernetes/troubleshooting.rst:16
msgid ""
"Check the status of the :term:`DaemonSet` and verify that all desired "
"instances are in \"ready\" state:"
msgstr ""

#: ../../concepts/kubernetes/troubleshooting.rst:25
msgid ""
"In this example, we see a desired state of 1 with 0 being ready. This "
"indicates a problem. The next step is to list all cilium pods by matching"
" on the label ``k8s-app=cilium`` and also sort the list by the restart "
"count of each pod to easily identify the failing pods:"
msgstr ""

#: ../../concepts/kubernetes/troubleshooting.rst:37
msgid ""
"Pod ``cilium-813gf`` is failing and has already been restarted 2 times. "
"Let's print the logfile of that pod to investigate the cause:"
msgstr ""

#: ../../concepts/kubernetes/troubleshooting.rst:50
msgid ""
"In this example, the cause for the failure is a Linux kernel running on "
"the worker node which is not meeting :ref:`admin_system_reqs`."
msgstr ""

#: ../../concepts/kubernetes/troubleshooting.rst:53
msgid ""
"If the cause for the problem is not apparent based on these simple steps,"
" please come and seek help on our :term:`Slack channel`."
msgstr ""

#: ../../concepts/kubernetes/troubleshooting.rst:57
msgid "Apiserver outside of cluster"
msgstr ""

#: ../../concepts/kubernetes/troubleshooting.rst:59
msgid ""
"If you are running Kubernetes Apiserver outside of your cluster for some "
"reason (like keeping master nodes behind a firewall), make sure that you "
"run Cilium on master nodes too. Otherwise Kubernetes pod proxies created "
"by Apiserver will not be able to route to pod IPs and you may encounter "
"errors when trying to proxy traffic to pods."
msgstr ""

#: ../../concepts/kubernetes/troubleshooting.rst:62
msgid ""
"You may run Cilium as a `static pod <https://kubernetes.io/docs/tasks"
"/configure-pod-container/static-pod/>`_ or set `tolerations "
"<https://kubernetes.io/docs/concepts/configuration/taint-and-"
"toleration/>`_ for Cilium DaemonSet to ensure that Cilium pods will be "
"scheduled on your master nodes. The exact way to do it depends on your "
"setup."
msgstr ""

#: ../../concepts/networking/fragmentation.rst:10
msgid "IPv4 fragment handling"
msgstr ""

#: ../../concepts/networking/fragmentation.rst:12
msgid ""
"By default, Cilium configures the eBPF datapath to perform IP fragment "
"tracking to allow protocols that do not support segmentation (such as "
"UDP) to transparently transmit large messages over the network. IP "
"fragment tracking is implemented in eBPF using an LRU (*Least Recently "
"Used*) map which requires Linux 4.10 or later. This feature may be "
"configured using the following options:"
msgstr ""

#: ../../concepts/networking/fragmentation.rst:19
msgid ""
"``--enable-ipv4-fragment-tracking``: Enable or disable IPv4 fragment "
"tracking. Enabled by default."
msgstr ""

#: ../../concepts/networking/fragmentation.rst:21
msgid ""
"``--bpf-fragments-map-max``: Control the maximum number of active "
"concurrent connections using IP fragmentation. For the defaults, see "
"`bpf_map_limitations`."
msgstr ""

#: ../../concepts/networking/fragmentation.rst:26
msgid ""
"When running Cilium with kube-proxy, fragmented NodePort traffic may "
"break due to a kernel bug where route MTU is not respected for forwarded "
"packets. Cilium fragments tracking requires the first logical fragment to"
" arrive first. Due to the kernel bug, additional fragmentation on the "
"outer encapsulation layer may happen that causes packet reordering and "
"results in a failure in tracking the fragments."
msgstr ""

#: ../../concepts/networking/fragmentation.rst:32
msgid ""
"The kernel bug has been `fixed "
"<https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=02a1b175b0e92d9e0fa5df3957ade8d733ceb6a0>`_"
" and backported to all maintained kernel versions. If you observe "
"connectivity problems, ensure that the kernel package on your nodes has "
"been upgraded recently before reporting an issue."
msgstr ""

#: ../../beta.rst:3
msgid ""
"This is a beta feature. Please provide feedback and file a GitHub issue "
"if you experience any problems."
msgstr ""

#: ../../concepts/networking/index.rst:12
msgid "Networking"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:11
msgid "Azure IPAM"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:13
msgid ""
"The Azure IPAM allocator is specific to Cilium deployments running in the"
" Azure cloud and performs IP allocation based on `Azure Private IP "
"addresses <https://docs.microsoft.com/en-us/azure/virtual-network"
"/private-ip-addresses>`__."
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:17
msgid ""
"The architecture ensures that only a single operator communicates with "
"the Azure API to avoid rate-limiting issues in large clusters. A pre-"
"allocation watermark allows to maintain a number of IP addresses to be "
"available for use on nodes at all time without requiring to contact the "
"Azure API when a new pod is scheduled in the cluster."
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:25
#: ../../concepts/networking/ipam/cluster-pool.rst:23
#: ../../concepts/networking/ipam/crd.rst:20
#: ../../concepts/networking/ipam/eni.rst:26
#: ../../concepts/networking/ipam/gke.rst:18
#: ../../concepts/networking/routing.rst:171
msgid "Architecture"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:30
msgid ""
"The Azure IPAM allocator builds on top of the CRD-backed allocator. Each "
"node creates a ``ciliumnodes.cilium.io`` custom resource matching the "
"node name when Cilium starts up for the first time on that node. The "
"Cilium agent running on each node will retrieve the Kubernetes "
"``v1.Node`` resource and extract the ``.Spec.ProviderID`` field in order "
"to derive the `Azure instance ID <https://docs.microsoft.com/en-us/azure"
"/virtual-machine-scale-sets/virtual-machine-scale-sets-instance-ids>`__. "
"Azure allocation parameters are provided as agent configuration option "
"and are passed into the custom resource as well."
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:38
msgid ""
"The Cilium operator listens for new ``ciliumnodes.cilium.io`` custom "
"resources and starts managing the IPAM aspect automatically. It scans the"
" Azure instances for existing interfaces with associated IPs and makes "
"them available via the ``spec.ipam.available`` field. It will then "
"constantly monitor the used IP addresses in the ``status.ipam.used`` "
"field and allocate more IPs as needed to meet the IP pre-allocation "
"watermark. This ensures that there are always IPs available"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:50
msgid ""
"The Cilium agent and operator must be run with the option "
"``--ipam=azure`` or the option ``ipam: azure``  must be set in the "
"ConfigMap. This will enable Azure IPAM allocation in both the node agent "
"and operator."
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:54
#: ../../concepts/networking/ipam/eni.rst:62
msgid ""
"In most scenarios, it makes sense to automatically create the "
"``ciliumnodes.cilium.io`` custom resource when the agent starts up on a "
"node for the first time. To enable this, specify the option ``--auto-"
"create-cilium-node-resource`` or  set ``auto-create-cilium-node-resource:"
" \"true\"`` in the ConfigMap."
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:60
#: ../../concepts/networking/ipam/eni.rst:72
msgid ""
"It is generally a good idea to enable metrics in the Operator as well "
"with the option ``--enable-metrics``. See the section "
":ref:`install_metrics` for additional information how to install and run "
"Prometheus including the Grafana dashboard."
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:66
msgid "Azure Allocation Parameters"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:68
msgid "The following parameters are available to control the IP allocation:"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:76
#: ../../concepts/networking/ipam/eni.rst:165
msgid "``spec.ipam.min-allocate``"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:71
#: ../../concepts/networking/ipam/eni.rst:160
msgid ""
"The minimum number of IPs that must be allocated when the node is first "
"bootstrapped. It defines the minimum base socket of addresses that must "
"be available. After reaching this watermark, the PreAllocate and "
"MaxAboveWatermark logic takes over to continue allocating IPs."
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:76
#: ../../concepts/networking/ipam/eni.rst:165
msgid "If unspecified, no minimum number of IPs is required."
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:83
#: ../../concepts/networking/ipam/eni.rst:180
msgid "``spec.ipam.pre-allocate``"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:79
#: ../../concepts/networking/ipam/eni.rst:176
msgid ""
"The number of IP addresses that must be available for allocation at all "
"times.  It defines the buffer of addresses available immediately without "
"requiring for the operator to get involved."
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:83
#: ../../concepts/networking/ipam/eni.rst:180
msgid "If unspecified, this value defaults to 8."
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:90
#: ../../concepts/networking/ipam/eni.rst:189
msgid "``spec.ipam.max-above-watermark``"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:86
msgid ""
"The maximum number of addresses to allocate beyond the addresses needed "
"to reach the PreAllocate watermark.  Going above the watermark can help "
"reduce the number of API calls to allocate IPs."
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:90
#: ../../concepts/networking/ipam/eni.rst:189
msgid "If let unspecified, the value defaults to 0."
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:94
#: ../../concepts/networking/ipam/eni.rst:244
msgid "Operational Details"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:97
msgid "Cache of Interfaces, Subnets, and VirtualNetworks"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:99
msgid ""
"The operator maintains a list of all Azure ScaleSets, Instances, "
"Interfaces, VirtualNetworks, and Subnets associated with the Azure "
"subscription in a cache."
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:102
msgid ""
"The cache is updated once per minute or after an IP allocation has been "
"performed. When triggered based on an allocation, the operation is "
"performed at most once per second."
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:107
msgid "Publication of available IPs"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:109
#: ../../concepts/networking/ipam/eni.rst:264
msgid ""
"Following the update of the cache, all CiliumNode custom resources "
"representing nodes are updated to publish eventual new IPs that have "
"become available."
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:112
msgid ""
"In this process, all interfaces are scanned for all available IPs.  All "
"IPs found are added to ``spec.ipam.available``. Each interface is also "
"added to ``status.azure.interfaces``."
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:116
msgid ""
"If this update caused the custom resource to change, the custom resource "
"is updated using the Kubernetes API methods ``Update()`` and/or "
"``UpdateStatus()`` if available."
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:121
msgid "Determination of IP deficits or excess"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:123
msgid ""
"The operator constantly monitors all nodes and detects deficits in "
"available IP addresses. The check to recognize a deficit is performed on "
"two occasions:"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:126
#: ../../concepts/networking/ipam/eni.rst:283
msgid "When a ``CiliumNode`` custom resource is updated"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:127
#: ../../concepts/networking/ipam/eni.rst:284
msgid "All nodes are scanned in a regular interval (once per minute)"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:129
#: ../../concepts/networking/ipam/eni.rst:289
msgid ""
"When determining whether a node has a deficit in IP addresses, the "
"following calculation is performed:"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:136
#: ../../concepts/networking/ipam/eni.rst:296
msgid "For excess IP calculation:"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:142
#: ../../concepts/networking/ipam/eni.rst:302
msgid ""
"Upon detection of a deficit, the node is added to the list of nodes which"
" require IP address allocation. When a deficit is detected using the "
"interval based scan, the allocation order of nodes is determined based on"
" the severity of the deficit, i.e. the node with the biggest deficit will"
" be at the front of the allocation queue. Nodes that need to release IPs "
"are behind nodes that need allocation."
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:149
#: ../../concepts/networking/ipam/eni.rst:309
msgid "The allocation queue is handled on demand but at most once per second."
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:152
#: ../../concepts/networking/ipam/eni.rst:312
msgid "IP Allocation"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:154
msgid ""
"When performing IP allocation for a node with an address deficit, the "
"operator first looks at the interfaces already attached to the instance "
"represented by the CiliumNode resource."
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:158
msgid ""
"The operator will then pick the first interface which meets the following"
" criteria:"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:161
msgid ""
"The interface has addresses associated which are not yet used or the "
"number of addresses associated with the interface is lesser than `maximum"
" number of addresses <https://docs.microsoft.com/en-us/azure/azure-"
"resource-manager/management/azure-subscription-service-limits#networking-"
"limits>`__ that can be associated to an interface."
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:167
msgid "The subnet associated with the interface has IPs available for allocation"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:169
msgid ""
"The following formula is used to determine how many IPs are allocated on "
"the interface:"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:176
#: ../../concepts/networking/ipam/eni.rst:347
msgid ""
"This means that the number of IPs allocated in a single allocation cycle "
"can be less than what is required to fulfill ``spec.ipam.pre-allocate``."
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:180
#: ../../concepts/networking/ipam/eni.rst:355
msgid "IP Release"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:182
msgid ""
"When performing IP release for a node with IP excess, the operator scans "
"the interface attached to the node. The following formula is used to "
"determine how many IPs are available for release on the interface:"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:191
#: ../../concepts/networking/ipam/eni.rst:429
msgid "Node Termination"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:193
#: ../../concepts/networking/ipam/eni.rst:431
msgid ""
"When a node or instance terminates, the Kubernetes apiserver will send a "
"node deletion event. This event will be picked up by the operator and the"
" operator will delete the corresponding ``ciliumnodes.cilium.io`` custom "
"resource."
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:201
#: ../../concepts/networking/ipam/eni.rst:439
msgid "Required Privileges"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:203
msgid ""
"The following Azure API calls are being performed by the Cilium operator."
" The Service Principal provided must have privileges to perform these "
"within the scope of the AKS cluster node resource group:"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:207
msgid ""
"`Network Interfaces - Create Or Update <https://docs.microsoft.com/en-"
"us/rest/api/virtualnetwork/networkinterfaces/createorupdate>`__"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:208
#, python-format
msgid ""
"`NetworkInterface In VMSS - List Virtual Machine Scale Set Network "
"Interfaces <https://docs.microsoft.com/en-"
"us/rest/api/virtualnetwork/networkinterface%20in%20vmss/listvirtualmachinescalesetnetworkinterfaces>`__"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:209
msgid ""
"`Virtual Networks - List <https://docs.microsoft.com/en-"
"us/rest/api/virtualnetwork/virtualnetworks/list>`__"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:210
msgid ""
"`Virtual Machine Scale Sets - List All <https://docs.microsoft.com/en-"
"us/rest/api/compute/virtualmachinescalesets/listall>`__"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:214
msgid ""
"The node resource group is *not* the resource group of the AKS cluster. A"
" single resource group may hold multiple AKS clusters, but each AKS "
"cluster regroups all resources in an automatically managed secondary "
"resource group. See `Why are two resource groups created with AKS? "
"<https://docs.microsoft.com/en-us/azure/aks/faq#why-are-two-resource-"
"groups-created-with-aks>`__ for more details."
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:222
#: ../../concepts/networking/ipam/eni.rst:476
msgid "Metrics"
msgstr ""

#: ../../concepts/networking/ipam/azure.rst:224
msgid "The metrics are documented in the section :ref:`ipam_metrics`."
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:3
msgid ""
"WARNING: You are looking at unreleased Cilium documentation. Please use "
"the official rendered version released here: http://docs.cilium.io"
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:11
msgid "Cluster Scope (Default)"
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:13
msgid ""
"The cluster-scope IPAM mode assigns per-node PodCIDRs to each node and "
"allocates IPs using a host-scope allocator on each node. It is thus "
"similar to the :ref:`k8s_hostscope` mode. The difference is that instead "
"of Kubernetes assigning the per-node PodCIDRs via the Kubernetes "
"``v1.Node`` resource, the Cilium operator will manage the per-node "
"PodCIDRs via the ``v2.CiliumNode`` resource. The advantage of this mode "
"is that it does not depend on Kubernetes being configured to hand out "
"per-node PodCIDRs."
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:28
msgid ""
"This is useful if Kubernetes cannot be configured to hand out PodCIDRs or"
" if more control is needed."
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:31
msgid ""
"In this mode, the Cilium agent will wait on startup until the "
"``PodCIDRs`` range are made available via the Cilium Node "
"``v2.CiliumNode`` object for all enabled address families via the "
"resource field set in the ``v2.CiliumNode``:"
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:36
#: ../../concepts/networking/ipam/cluster-pool.rst:79
#: ../../concepts/networking/ipam/kubernetes.rst:28
msgid "Field"
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:38
#: ../../concepts/networking/ipam/cluster-pool.rst:81
msgid "``Spec.IPAM.PodCIDRs``"
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:38
#: ../../concepts/networking/ipam/kubernetes.rst:30
msgid "IPv4 and/or IPv6 PodCIDR range"
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:45
msgid ""
"For a practical tutorial on how to enable this mode in Cilium, see "
":ref:`gsg_ipam_crd_cluster_pool`."
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:53
msgid "Look for allocation errors"
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:55
msgid "Check the ``Error`` field in the ``Status.Operator`` field:"
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:63
msgid "Cluster Pool v2 (Beta)"
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:65
msgid ""
"Cluster Pool v2 (Beta) extends the above mechanism to allow additional "
"PodCIDRs to be dynamically allocated to each node based on usage. With "
"v2, each Cilium agent instance reports the utilization of its PodCIDRs "
"via the ``CiliumNode`` resource."
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:70
msgid ""
"If a node is running low on available pod IPs, the operator will assign "
"an additional PodCIDR to that node. Likewise, if a node has unused "
"PodCIDRs, it will eventually release it, allowing the operator to re-"
"assign the released PodCIDR to a different node if needed."
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:75
msgid ""
"When running v2, the ``CiliumNode`` resource is extended with an "
"additional PodCIDR status section:"
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:81
msgid "List of assigned IPv4 and/or IPv6 PodCIDRs"
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:83
msgid "``Status.IPAM.PodCIDRs``"
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:83
msgid "PodCIDR utilization (one of: ``in-use``, ``depleted``, or ``released``)"
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:87
msgid ""
"The operator assigns a new PodCIDR to a node if all of its PodCIDRs are "
"either depleted or released."
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:91
msgid "Limitations"
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:93
msgid ""
"Cluster Pool v2 is a preview feature. The following limitations currently"
" apply to Cilium running in ``cluster-pool-v2beta`` IPAM mode:"
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:97
msgid ""
"Tunnel mode is not supported. Cluster Pool v2 may only be used in direct "
"routing mode."
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:99
msgid "Transparent encryption with IPSec is not supported."
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:101
msgid "The current status of these limitations is tracked in :gh-issue:`18987`."
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:106
msgid ""
"To enable Cluster Pool v2, pass ``--set ipam.mode=cluster-pool-v2beta`` "
"to your Helm options. The CIDR pool used in Cluster Pool v2 mode are "
"configured the same way as regular cluster pool (see "
":ref:`gsg_ipam_crd_cluster_pool`)."
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:110
msgid ""
"In addition, the thresholds for when a PodCIDR should be allocated or "
"released can be configured per node via the following "
"``CiliumNode.Spec.IPAM`` fields:"
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:125
msgid "``Spec.IPAM.PodCIDRAllocationThreshold``"
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:114
msgid ""
"Defines the minimum number of free IPs which must be available to this "
"node via its PodCIDR pool."
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:117
msgid ""
"If the total number of IP addresses in the PodCIDR pool is less than this"
" value, the PodCIDRs currently in-use by this node will be marked as "
"depleted and Cilium operator will allocate a new PodCIDR to this node."
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:121
msgid ""
"This value effectively defines the buffer of IP addresses available "
"immediately without requiring  Cilium operator to get involved."
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:124
msgid "If unspecified, defaults to 8."
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:135
msgid "``Spec.IPAM.PodCIDRReleaseThreshold``"
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:128
msgid ""
"Defines the maximum number of free IPs which may be available to this "
"node via its PodCIDR pool."
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:131
msgid ""
"While the total number of free IP addresses in the PodCIDR pool is larger"
" than this value, Cilium agent will attempt to release currently unused "
"PodCIDR."
msgstr ""

#: ../../concepts/networking/ipam/cluster-pool.rst:134
msgid "If unspecified, defaults to 16."
msgstr ""

#: ../../concepts/networking/ipam/crd.rst:11
msgid "CRD-Backed"
msgstr ""

#: ../../concepts/networking/ipam/crd.rst:13
msgid ""
"The CRD-backed IPAM mode provides an extendable interface to control the "
"IP address management via a Kubernetes Custom Resource Definition (CRD). "
"This allows to delegate IPAM to external operators or make it user "
"configurable per node."
msgstr ""

#: ../../concepts/networking/ipam/crd.rst:25
msgid ""
"When this mode is enabled, each Cilium agent will start watching for a "
"Kubernetes custom resource ``ciliumnodes.cilium.io`` with a name matching"
" the Kubernetes node on which the agent is running."
msgstr ""

#: ../../concepts/networking/ipam/crd.rst:29
msgid ""
"Whenever the custom resource is updated, the per node allocation pool is "
"updated with all addresses listed in the ``spec.ipam.available`` field. "
"When an IP is removed that is currently allocated, the IP will continue "
"to be used but will not be available for re-allocation after release."
msgstr ""

#: ../../concepts/networking/ipam/crd.rst:34
msgid ""
"Upon allocation of an IP in the allocation pool, the IP is added to the "
"``status.ipam.inuse`` field."
msgstr ""

#: ../../concepts/networking/ipam/crd.rst:39
msgid ""
"The node status update is limited to run at most once every 15 seconds. "
"Therefore, if several pods are scheduled at the same time, the update of "
"the status section can lag behind."
msgstr ""

#: ../../concepts/networking/ipam/crd.rst:47
msgid ""
"The CRD-backed IPAM mode is enabled by setting ``ipam: crd`` in the "
"``cilium-config`` ConfigMap or by specifying the option ``--ipam=crd``. "
"When enabled, the agent will wait for a ``CiliumNode`` custom resource "
"matching the Kubernetes node name to become available with at least one "
"IP address listed as available. When connectivity health-checking is "
"enabled, at least two IP addresses must be available."
msgstr ""

#: ../../concepts/networking/ipam/crd.rst:54
msgid "While waiting, the agent will print the following log message:"
msgstr ""

#: ../../concepts/networking/ipam/crd.rst:61
msgid ""
"For a practical tutorial on how to enable CRD IPAM mode with Cilium, see "
"the section :ref:`gsg_ipam_crd`."
msgstr ""

#: ../../concepts/networking/ipam/crd.rst:65
msgid "Privileges"
msgstr ""

#: ../../concepts/networking/ipam/crd.rst:67
msgid ""
"In order for the custom resource to be functional, the following "
"additional privileges are required. These privileges are automatically "
"granted when using the standard Cilium deployment artifacts:"
msgstr ""

#: ../../concepts/networking/ipam/crd.rst:88
msgid "CRD Definition"
msgstr ""

#: ../../concepts/networking/ipam/crd.rst:90
msgid ""
"The CilumNode custom resource is modeled after a standard Kubernetes "
"resource and is split into a ``spec`` and ``status`` section:"
msgstr ""

#: ../../concepts/networking/ipam/crd.rst:106
msgid "IPAM Specification"
msgstr ""

#: ../../concepts/networking/ipam/crd.rst:108
msgid ""
"The ``spec`` section embeds an IPAM specific field which allows to define"
" the list of all IPs which are available to the node for allocation:"
msgstr ""

#: ../../concepts/networking/ipam/crd.rst:160
msgid "IPAM Status"
msgstr ""

#: ../../concepts/networking/ipam/crd.rst:162
msgid ""
"The ``status`` section contains an IPAM specific field. The IPAM status "
"reports all used addresses on that node:"
msgstr ""

#: ../../concepts/networking/ipam/deep_dive.rst:9
msgid "Technical Deep Dive"
msgstr ""

#: ../../concepts/networking/ipam/deep_dive.rst:12
msgid "Cilium Container Networking Control Flow"
msgstr ""

#: ../../concepts/networking/ipam/deep_dive.rst:14
msgid ""
"The control flow diagram below gives an overview on how endpoints obtain "
"their IP address from the IPAM for each different mode of Address "
"Management that Cilium Supports."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:11
#: ../../concepts/networking/routing.rst:141
msgid "AWS ENI"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:13
msgid ""
"The AWS ENI allocator is specific to Cilium deployments running in the "
"AWS cloud and performs IP allocation based on IPs of `AWS Elastic Network"
" Interfaces (ENI) <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide"
"/using-eni.html>`__ by communicating with the AWS EC2 API."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:18
msgid ""
"The architecture ensures that only a single operator communicates with "
"the EC2 service API to avoid rate-limiting issues in large clusters. A "
"pre-allocation watermark is used to maintain a number of IP addresses to "
"be available for use on nodes at all time without needing to contact the "
"EC2 API when a new pod is scheduled in the cluster."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:31
msgid ""
"The AWS ENI allocator builds on top of the CRD-backed allocator. Each "
"node creates a ``ciliumnodes.cilium.io`` custom resource matching the "
"node name when Cilium starts up for the first time on that node. It "
"contacts the EC2 metadata API to retrieve the instance ID, instance type,"
" and VPC information, then it populates the custom resource with this "
"information. ENI allocation parameters are provided as agent "
"configuration option and are passed into the custom resource as well."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:39
msgid ""
"The Cilium operator listens for new ``ciliumnodes.cilium.io`` custom "
"resources and starts managing the IPAM aspect automatically. It scans the"
" EC2 instances for existing ENIs with associated IPs and makes them "
"available via the ``spec.ipam.available`` field. It will then constantly "
"monitor the used IP addresses in the ``status.ipam.used`` field and "
"automatically create ENIs and allocate more IPs as needed to meet the IP "
"pre-allocation watermark. This ensures that there are always IPs "
"available."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:47
msgid ""
"The selection of subnets to use for allocation as well as attachment of "
"security groups to new ENIs can be controlled separately for each node. "
"This makes it possible to hand out pod IPs with differing security groups"
" on individual nodes."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:52
msgid ""
"The corresponding datapath is described in section "
":ref:`aws_eni_datapath`."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:58
msgid ""
"The Cilium agent and operator must be run with the option ``--ipam=eni`` "
"or the option ``ipam: eni``  must be set in the ConfigMap. This will "
"enable ENI allocation in both the node agent and operator."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:68
msgid ""
"If IPs are limited, run the Operator with option ``--aws-release-excess-"
"ips=true``. When enabled, operator checks the number of IPs regularly and"
" attempts to release excess free IPs from ENI."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:78
msgid "Custom ENI Configuration"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:80
msgid ""
"Custom ENI configuration can be defined with a custom CNI configuration "
"``ConfigMap``:"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:84
msgid "Create a CNI configuration"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:86
msgid ""
"Create a ``cni-config.yaml`` file based on the template below. Fill in "
"the ``subnet-tags`` field, assuming that the subnets in AWS have the tags"
" applied to them:"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:115
msgid ""
"Additional parameters may be configured in the ``eni`` or ``ipam`` "
"section of the CNI configuration file. See the list of ENI allocation "
"parameters below for a reference of the supported options."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:119
msgid "Deploy the ``ConfigMap``:"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:126
msgid "Configure Cilium with subnet-tags-filter"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:128
msgid ""
"Using the instructions above to deploy Cilium and CNI config, specify the"
" following additional arguments to Helm:"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:137
msgid "ENI Allocation Parameters"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:139
msgid ""
"The following parameters are available to control the ENI creation and IP"
" allocation:"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:145
msgid "``InstanceType``"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:143
msgid "The AWS EC2 instance type"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:145
#: ../../concepts/networking/ipam/eni.rst:151
#: ../../concepts/networking/ipam/eni.rst:157
msgid ""
"*This field is automatically populated when using ``--auto-create-cilium-"
"node-resource``*"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:151
msgid "``spec.eni.vpc-id``"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:148
msgid ""
"The VPC identifier used to create ENIs and select AWS subnets for IP "
"allocation."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:157
msgid "``spec.eni.availability-zone``"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:154
msgid ""
"The availability zone used to create ENIs and select AWS subnets for IP "
"allocation."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:173
msgid "``spec.ipam.max-allocate``"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:168
msgid ""
"The maximum number of IPs that can be allocated to the node. When the "
"current amount of allocated IPs will approach this value, the considered "
"value for PreAllocate will decrease down to 0 in order to not attempt to "
"allocate more addresses than defined."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:173
msgid "If unspecified, no maximum number of IPs will be enforced."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:183
msgid ""
"The maximum number of addresses to allocate beyond the addresses needed "
"to reach the PreAllocate watermark.  Going above the watermark can help "
"reduce the number of API calls to allocate IPs, e.g. when a new ENI is "
"allocated, as many secondary IPs as possible are allocated. Limiting the "
"amount can help reduce waste of IPs."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:198
msgid "``spec.eni.first-interface-index``"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:192
msgid ""
"The index of the first ENI to use for IP allocation, e.g. if the node has"
" ``eth0``, ``eth1``, ``eth2`` and FirstInterfaceIndex is set to 1, then "
"only ``eth1`` and ``eth2`` will be used for IP allocation, ``eth0`` will "
"be ignored for PodIP allocation."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:197
msgid ""
"If unspecified, this value defaults to 0 which means that ``eth0`` will "
"be used for pod IPs."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:205
msgid "``spec.eni.security-group-tags``"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:201
msgid ""
"The list tags which will be used to filter the security groups to attach "
"to any ENI that is created and attached to the instance."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:204
msgid ""
"If unspecified, the security group ids passed in ``spec.eni.security-"
"groups`` field will be used."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:211
msgid "``spec.eni.security-groups``"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:208
msgid ""
"The list of security group ids to attach to any ENI that is created and "
"attached to the instance."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:211
msgid "If unspecified, the security group ids of ``eth0`` will be used."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:220
msgid "``spec.eni.subnet-ids``"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:214
msgid ""
"The subnet IDs used to select the AWS subnets for IP allocation. This is "
"an additional requirement on top of requiring to match the availability "
"zone and VPC of the instance. This parameter is mutually exclusive and "
"has priority over ``spec.eni.subnet-tags``."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:219
msgid ""
"If unspecified, it will let the operator pick any available subnet in the"
" AZ with the most IP addresses available."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:227
msgid "``spec.eni.subnet-tags``"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:223
msgid ""
"The tags used to select the AWS subnets for IP allocation. This is an "
"additional requirement on top of requiring to match the availability zone"
" and VPC of the instance."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:227
msgid "If unspecified, no tags are required."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:235
msgid "``spec.eni.exclude-interface-tags``"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:230
msgid ""
"The tags used to exclude interfaces from IP allocation. Any ENI attached "
"to a node which matches this set of tags will be ignored by Cilium and "
"may be used for other purposes. This parameter can be used in combination"
" with ``subnet-tags`` or ``first-interface-index`` to exclude additional "
"interfaces."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:235
msgid "If unspecified, no tags are used to exclude interfaces."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:240
msgid "``spec.eni.delete-on-termination``"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:238
msgid "Remove the ENI when the instance is terminated"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:240
msgid "If unspecified, this option is enabled."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:247
msgid "Cache of ENIs, Subnets, and VPCs"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:249
msgid ""
"The operator maintains a list of all EC2 ENIs, VPCs and subnets "
"associated with the AWS account in a cache. For this purpose, the "
"operator performs the following three EC2 API operations:"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:253
#: ../../concepts/networking/ipam/eni.rst:444
msgid "``DescribeNetworkInterfaces``"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:254
#: ../../concepts/networking/ipam/eni.rst:445
msgid "``DescribeSubnets``"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:255
#: ../../concepts/networking/ipam/eni.rst:446
msgid "``DescribeVpcs``"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:257
msgid ""
"The cache is updated once per minute or after an IP allocation or ENI "
"creation has been performed. When triggered based on an allocation or "
"creation, the operation is performed at most once per second."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:262
msgid "Publication of available ENI IPs"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:267
msgid ""
"In this process, all ENIs with an interface index greater than ``spec.eni"
".first-interface-index`` are scanned for all available IPs.  All IPs "
"found are added to ``spec.ipam.available``. Each ENI meeting this "
"criteria is also added to ``status.eni.enis``."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:272
msgid ""
"If this updated caused the custom resource to change, the custom resource"
" is updated using the Kubernetes API methods ``Update()`` and/or "
"``UpdateStatus()`` if available."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:277
msgid "Determination of ENI IP deficits or excess"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:279
msgid ""
"The operator constantly monitors all nodes and detects deficits in "
"available ENI IP addresses. The check to recognize a deficit is performed"
" on two occasions:"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:286
msgid ""
"If ``--aws-release-excess-ips`` is enabled, the check to recognize IP "
"excess is performed at the interval based scan."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:314
msgid ""
"When performing IP allocation for a node with an address deficit, the "
"operator first looks at the ENIs which are already attached to the "
"instance represented by the CiliumNode resource. All ENIs with an "
"interface index greater than ``spec.eni.first-interface-index`` are "
"considered for use."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:321
msgid ""
"In order to not use ``eth0`` for IP allocation, set ``spec.eni.first-"
"interface-index`` to ``1`` to skip the first interface in line."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:325
msgid ""
"The operator will then pick the first already allocated ENI which meets "
"the following criteria:"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:328
msgid ""
"The ENI has addresses associated which are not yet used or the number of "
"addresses associated with the ENI is lesser than the instance type "
"specific limit."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:332
msgid "The subnet associated with the ENI has IPs available for allocation"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:334
msgid ""
"The following formula is used to determine how many IPs are allocated on "
"the ENI:"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:343
msgid ""
"In scenarios where the pre-allocated number is lower than the number of "
"pending pods on the node, the operator will pro-actively allocate more "
"than the pre-allocated number of IPs to avoid having to wait for the next"
" allocation cycles."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:350
msgid ""
"In order to allocate the IPs, the method ``AssignPrivateIpAddresses`` of "
"the EC2 service API is called. When no more ENIs are available meeting "
"the above criteria, a new ENI is created."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:357
msgid ""
"When performing IP release for a node with IP excess, the operator scans "
"ENIs attached to the node with an interface index greater than ``spec.eni"
".first-interface-index`` and selects an ENI with the most free IPs "
"available for release. The following formula is used to determine how "
"many IPs are available for release on the ENI:"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:367
msgid ""
"Operator releases IPs from the selected ENI, if there is still excess "
"free IP not released, operator will attempt to release in next release "
"cycle."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:370
msgid ""
"In order to release the IPs, the method ``UnassignPrivateIpAddresses`` of"
" the EC2 service API is called. There is no limit on ENIs per subnet so "
"ENIs are remained on the node."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:376
msgid "ENI Creation"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:378
msgid ""
"As long as an instance type is capable allocating additional ENIs, ENIs "
"are allocated automatically based on demand."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:381
msgid ""
"When allocating an ENI, the first operation performed is to identify the "
"best subnet. This is done by searching through all subnets and finding a "
"subnet that matches the following criteria:"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:385
msgid "The VPC ID of the subnet matches ``spec.eni.vpc-id``"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:386
msgid "The Availability Zone of the subnet matches ``spec.eni.availability-zone``"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:388
msgid "The subnet contains all tags as specified by ``spec.eni.subnet-tags``"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:391
msgid ""
"If multiple subnets match, the subnet with the most available addresses "
"is selected."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:393
msgid ""
"After selecting the ENI, the interface index is determined. For this "
"purpose, all existing ENIs are scanned and the first unused index greater"
" than ``spec.eni.first-interface-index`` is selected."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:397
msgid ""
"After determining the subnet and interface index, the ENI is created and "
"attached to the EC2 instance using the methods ``CreateNetworkInterface``"
" and ``AttachNetworkInterface`` of the EC2 API."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:401
msgid ""
"The security group ids attached to the ENI are computed in the following "
"order:"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:403
msgid ""
"The field ``spec.eni.security-groups`` is consulted first. If this is set"
" then these will be the security group ids attached to the newly created "
"ENI."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:405
msgid ""
"The filed ``spec.eni.security-group-tags`` is consulted. If this is set "
"then the operator will list all security groups in the account and will "
"attach to the ENI the ones that match the list of tags passed."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:408
msgid ""
"Finally if none of the above fields are set then the newly created ENI "
"will inherit the security group ids of ``eth0`` of the machine."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:411
msgid "The description will be in the following format:"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:417
msgid ""
"If the ENI tagging feature is enabled then the ENI will be tagged with "
"the provided information."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:420
msgid "ENI Deletion Policy"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:422
msgid ""
"ENIs can be marked for deletion when the EC2 instance to which the ENI is"
" attached to is terminated. In order to enable this, the option "
"``spec.eni.delete-on-termination`` can be enabled. If enabled, the ENI is"
" modified after creation using ``ModifyNetworkInterfaceAttribute`` to "
"specify this deletion policy."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:441
msgid ""
"The following EC2 privileges are required by the Cilium operator in order"
" to perform ENI creation and IP allocation:"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:447
msgid "``DescribeSecurityGroups``"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:448
msgid "``CreateNetworkInterface``"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:449
msgid "``AttachNetworkInterface``"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:450
msgid "``ModifyNetworkInterfaceAttribute``"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:451
msgid "``AssignPrivateIpAddresses``"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:452
msgid "``CreateTags``"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:454
msgid "If release excess IP enabled:"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:456
msgid "``UnassignPrivateIpAddresses``"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:460
msgid "EC2 instance types ENI limits"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:462
msgid ""
"Currently the EC2 Instance ENI limits (adapters per instance + IPv4/IPv6 "
"IPs per adapter) are hardcoded in the Cilium codebase for easy out-of-the"
" box deployment and usage."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:465
msgid ""
"The limits can be modified via the ``--aws-instance-limit-mapping`` CLI "
"flag on the cilium-operator. This allows the user to supply a custom "
"limit."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:468
msgid ""
"Additionally the limits can be updated via the EC2 API by passing the "
"``--update-ec2-adapter-limit-via-api`` CLI flag. This will require an "
"additional EC2 IAM permission:"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:472
msgid "``DescribeInstanceTypes``"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:478
msgid "The IPAM metrics are documented in the section :ref:`ipam_metrics`."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:482
msgid "Node Configuration"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:484
msgid ""
"The IP address and routes on ENIs attached to the instance will be "
"managed by the Cilium agent. Therefore, any system service trying to "
"manage newly attached network interfaces will interfere with Cilium's "
"configuration. Common scenarios are ``NetworkManager`` or ``systemd-"
"networkd`` automatically performing DHCP on these interfaces or removing "
"Cilium's IP address when the carrier is temporarily lost. Be sure to "
"disable these services or configure your Linux distribution to not manage"
" the newly attached ENI devices. The following examples configure all "
"Linux network devices named ``eth*`` except ``eth0`` as unmanaged."
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:496
msgid "Network Manager"
msgstr ""

#: ../../concepts/networking/ipam/eni.rst:506
msgid "systemd-networkd"
msgstr ""

#: ../../concepts/networking/ipam/gke.rst:11
msgid "Google Kubernetes Engine"
msgstr ""

#: ../../concepts/networking/ipam/gke.rst:13
msgid ""
"When running Cilium on Google GKE, the native networking layer of Google "
"Cloud will be utilized for address management and IP forwarding."
msgstr ""

#: ../../concepts/networking/ipam/gke.rst:23
msgid ""
"Cilium running in a GKE configuration mode utilizes the Kubernetes "
"hostscope IPAM mode. It will configure the Cilium agent to wait until the"
" Kubernetes node resource is populated with a ``spec.podCIDR`` or "
"``spec.podCIDRs`` as required by the enabled address families "
"(IPv4/IPv6). See :ref:`k8s_hostscope` for additional details of this IPAM"
" mode."
msgstr ""

#: ../../concepts/networking/ipam/gke.rst:29
msgid "The corresponding datapath is described in section :ref:`gke_datapath`."
msgstr ""

#: ../../concepts/networking/ipam/gke.rst:31
msgid ""
"See the getting started guide :ref:`k8s_install_quick` to install Cilium "
"Google Kubernetes Engine (GKE)."
msgstr ""

#: ../../concepts/networking/ipam/gke.rst:38
msgid ""
"The GKE IPAM mode can be enabled by setting the Helm option "
"``ipam.mode=kubernetes`` or by setting the ConfigMap option ``ipam: "
"kubernetes``."
msgstr ""

#: ../../concepts/networking/ipam/gke.rst:47
msgid "Validate the exposed PodCIDR field"
msgstr ""

#: ../../concepts/networking/ipam/gke.rst:49
msgid "Check if the Kubernetes nodes contain a value in the ``podCIDR`` field:"
msgstr ""

#: ../../concepts/networking/ipam/gke.rst:58
msgid "Check the Cilium status"
msgstr ""

#: ../../concepts/networking/ipam/gke.rst:60
msgid ""
"Run ``cilium status`` on the node in question and validate that the CIDR "
"used for IPAM matches the PodCIDR announced in the Kubernetes node:"
msgstr ""

#: ../../concepts/networking/ipam/index.rst:11
msgid "IP Address Management (IPAM)"
msgstr ""

#: ../../concepts/networking/ipam/index.rst:13
msgid ""
"IP Address Management (IPAM) is responsible for the allocation and "
"management of IP addresses used by network endpoints (container and "
"others) managed by Cilium. Various IPAM modes are supported to meet the "
"needs of different users:"
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:11
msgid "Kubernetes Host Scope"
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:13
msgid ""
"The Kubernetes host-scope IPAM mode is enabled with ``ipam: kubernetes`` "
"and delegates the address allocation to each individual node in the "
"cluster. IPs are allocated out of the ``PodCIDR`` range associated to "
"each node by Kubernetes."
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:21
msgid ""
"In this mode, the Cilium agent will wait on startup until the ``PodCIDR``"
" range is made available via the Kubernetes ``v1.Node`` object for all "
"enabled address families via one of the following methods:"
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:25
msgid "**via v1.Node resource field**"
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:30
msgid "``spec.podCIDRs``"
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:31
msgid "``spec.podCIDR``"
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:31
msgid "IPv4 or IPv6 PodCIDR range"
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:34
msgid ""
"It is important to run the ``kube-controller-manager`` with the flag "
"``--allocate-node-cidrs`` flag to indicate to Kubernetes that PodCIDR "
"ranges should be allocated."
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:38
msgid "**via v1.Node annotation**"
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:41
msgid "Annotation"
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:43
msgid "``io.cilium.network.ipv4-pod-cidr``"
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:43
msgid "IPv4 PodCIDR range"
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:44
msgid "``io.cilium.network.ipv6-pod-cidr``"
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:44
msgid "IPv6 PodCIDR range"
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:45
msgid "``io.cilium.network.ipv4-cilium-host``"
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:45
msgid "IPv4 address of the cilium host interface"
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:46
msgid "``io.cilium.network.ipv6-cilium-host``"
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:46
msgid "IPv6 address of the cilium host interface"
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:47
msgid "``io.cilium.network.ipv4-health-ip``"
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:47
msgid "IPv4 address of the cilium-health endpoint"
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:48
msgid "``io.cilium.network.ipv6-health-ip``"
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:48
msgid "IPv6 address of the cilium-health endpoint"
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:51
msgid ""
"The annotation-based mechanism is primarily useful in combination with "
"older Kubernetes versions which do not support ``spec.podCIDRs`` yet but "
"support for both IPv4 and IPv6 is enabled."
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:61
msgid "The following ConfigMap options exist to configure Kubernetes hostscope:"
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:63
msgid ""
"``ipam: kubernetes``: Enables Kubernetes IPAM mode. Enabling this option "
"will automatically enable ``k8s-require-ipv4-pod-cidr`` if ``enable-"
"ipv4`` is ``true`` and ``k8s-require-ipv6-pod-cidr`` if ``enable-ipv6`` "
"is ``true``."
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:66
msgid ""
"``k8s-require-ipv4-pod-cidr: true``: instructs the Cilium agent to wait "
"until an IPv4 PodCIDR is made available via the Kubernetes node resource."
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:68
msgid ""
"``k8s-require-ipv6-pod-cidr: true``: instructs the Cilium agent to wait "
"until an IPv6 PodCIDR is made available via the Kubernetes node resource."
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:71
msgid "With helm the previous options can be defined as:"
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:73
msgid "``ipam: kubernetes``: ``--set ipam.mode=kubernetes``."
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:74
msgid ""
"``k8s-require-ipv4-pod-cidr: true``: ``--set "
"k8s.requireIPv4PodCIDR=true``, which only works with ``--set "
"ipam.mode=kubernetes``"
msgstr ""

#: ../../concepts/networking/ipam/kubernetes.rst:76
msgid ""
"``k8s-require-ipv6-pod-cidr: true``: ``--set "
"k8s.requireIPv6PodCIDR=true``, which only works with ``--set "
"ipam.mode=kubernetes``"
msgstr ""

#: ../../concepts/networking/masquerading.rst:10
#: ../../concepts/networking/routing.rst:275
msgid "Masquerading"
msgstr ""

#: ../../concepts/networking/masquerading.rst:12
msgid ""
"IPv4 addresses used for pods are typically allocated from RFC1918 private"
" address blocks and thus, not publicly routable. Cilium will "
"automatically masquerade the source IP address of all traffic that is "
"leaving the cluster to the IPv4 address of the node as the node's IP "
"address is already routable on the network."
msgstr ""

#: ../../concepts/networking/masquerading.rst:21
msgid ""
"For IPv6 addresses masquerading is performed only when using iptables "
"implementation mode."
msgstr ""

#: ../../concepts/networking/masquerading.rst:24
msgid ""
"This behavior can be disabled with the option ``enable-ipv4-masquerade: "
"false`` for IPv4 and ``enable-ipv6-masquerade: false`` for IPv6 traffic "
"leaving the host."
msgstr ""

#: ../../concepts/networking/masquerading.rst:35
msgid "Setting the routable CIDR"
msgstr ""

#: ../../concepts/networking/masquerading.rst:31
msgid ""
"The default behavior is to exclude any destination within the IP "
"allocation CIDR of the local node. If the pod IPs are routable across a "
"wider network, that network can be specified with the option: ``ipv4"
"-native-routing-cidr: 10.0.0.0/8`` (or ``ipv6-native-routing-cidr: "
"fd00::/100`` for IPv6 addresses) in which case all destinations within "
"that CIDR will **not** be masqueraded."
msgstr ""

#: ../../concepts/networking/masquerading.rst:38
msgid "Setting the masquerading interface"
msgstr ""

#: ../../concepts/networking/masquerading.rst:38
msgid "See :ref:`masq_modes` for configuring the masquerading interfaces."
msgstr ""

#: ../../concepts/networking/masquerading.rst:43
msgid "Implementation Modes"
msgstr ""

#: ../../concepts/networking/masquerading.rst:46
msgid "eBPF-based"
msgstr ""

#: ../../concepts/networking/masquerading.rst:48
msgid ""
"The eBPF-based implementation is the most efficient implementation. It "
"requires Linux kernel 4.19 and can be enabled with the "
"``bpf.masquerade=true`` helm option."
msgstr ""

#: ../../concepts/networking/masquerading.rst:52
msgid ""
"The current implementation depends on :ref:`the BPF NodePort feature "
"<kubeproxy-free>`. The dependency will be removed in the future (:gh-"
"issue:`13732`)."
msgstr ""

#: ../../concepts/networking/masquerading.rst:55
msgid ""
"Masquerading can take place only on those devices which run the eBPF "
"masquerading program. This means that a packet sent from a pod to an "
"outside will be masqueraded (to an output device IPv4 address), if the "
"output device runs the program. If not specified, the program will be "
"automatically attached to the devices selected by :ref:`the BPF NodePort "
"device detection metchanism <Nodeport Devices>`. To manually change this,"
" use the ``devices`` helm option. Use ``cilium status`` to determine "
"which devices the program is running on:"
msgstr ""

#: ../../concepts/networking/masquerading.rst:68
msgid ""
"From the output above, the program is running on the ``eth0`` and "
"``eth1`` devices."
msgstr ""

#: ../../concepts/networking/masquerading.rst:71
msgid ""
"The eBPF-based masquerading can masquerade packets of the following IPv4 "
"L4 protocols:"
msgstr ""

#: ../../concepts/networking/masquerading.rst:73
msgid "TCP"
msgstr ""

#: ../../concepts/networking/masquerading.rst:74
msgid "UDP"
msgstr ""

#: ../../concepts/networking/masquerading.rst:75
msgid "ICMP (only Echo request and Echo reply)"
msgstr ""

#: ../../concepts/networking/masquerading.rst:77
msgid ""
"By default, all packets from a pod destined to an IP address outside of "
"the ``ipv4-native-routing-cidr`` range are masqueraded, except for "
"packets destined to other cluster nodes. The exclusion CIDR is shown in "
"the above output of ``cilium status`` (``10.0.0.0/16``)."
msgstr ""

#: ../../concepts/networking/masquerading.rst:84
msgid ""
"When eBPF-masquerading is enabled, traffic from pods to the External IP "
"of cluster nodes will also not be masqueraded. The eBPF implementation "
"differs from the iptables-based masquerading on that aspect. This "
"limitation is tracked at :gh-issue:`17177`."
msgstr ""

#: ../../concepts/networking/masquerading.rst:89
msgid ""
"To allow more fine-grained control, Cilium implements `ip-masq-agent "
"<https://github.com/kubernetes-sigs/ip-masq-agent>`_ in eBPF which can be"
" enabled with the ``ipMasqAgent.enabled=true`` helm option."
msgstr ""

#: ../../concepts/networking/masquerading.rst:93
msgid ""
"The eBPF-based ip-masq-agent supports the ``nonMasqueradeCIDRs`` and "
"``masqLinkLocal`` options set in a configuration file. A packet sent from"
" a pod to a destination which belongs to any CIDR from the "
"``nonMasqueradeCIDRs`` is not going to be masqueraded. If the "
"configuration file is empty, the agent will provision the following non-"
"masquerade CIDRs:"
msgstr ""

#: ../../concepts/networking/masquerading.rst:99
msgid "``10.0.0.0/8``"
msgstr ""

#: ../../concepts/networking/masquerading.rst:100
msgid "``172.16.0.0/12``"
msgstr ""

#: ../../concepts/networking/masquerading.rst:101
msgid "``192.168.0.0/16``"
msgstr ""

#: ../../concepts/networking/masquerading.rst:102
msgid "``100.64.0.0/10``"
msgstr ""

#: ../../concepts/networking/masquerading.rst:103
msgid "``192.0.0.0/24``"
msgstr ""

#: ../../concepts/networking/masquerading.rst:104
msgid "``192.0.2.0/24``"
msgstr ""

#: ../../concepts/networking/masquerading.rst:105
msgid "``192.88.99.0/24``"
msgstr ""

#: ../../concepts/networking/masquerading.rst:106
msgid "``198.18.0.0/15``"
msgstr ""

#: ../../concepts/networking/masquerading.rst:107
msgid "``198.51.100.0/24``"
msgstr ""

#: ../../concepts/networking/masquerading.rst:108
msgid "``203.0.113.0/24``"
msgstr ""

#: ../../concepts/networking/masquerading.rst:109
msgid "``240.0.0.0/4``"
msgstr ""

#: ../../concepts/networking/masquerading.rst:111
msgid ""
"In addition, if the ``masqLinkLocal`` is not set or set to false, then "
"``169.254.0.0/16`` is appended to the non-masquerade CIDRs list."
msgstr ""

#: ../../concepts/networking/masquerading.rst:114
msgid ""
"The agent uses Fsnotify to track updates to the configuration file, so "
"the original ``resyncInterval`` option is unnecessary."
msgstr ""

#: ../../concepts/networking/masquerading.rst:117
msgid ""
"The example below shows how to configure the agent via :term:`ConfigMap` "
"and to verify it:"
msgstr ""

#: ../../concepts/networking/masquerading.rst:141
msgid "eBPF based masquerading is currently not supported for IPv6 traffic."
msgstr ""

#: ../../concepts/networking/masquerading.rst:144
msgid "iptables-based"
msgstr ""

#: ../../concepts/networking/masquerading.rst:146
msgid "This is the legacy implementation that will work on all kernel versions."
msgstr ""

#: ../../concepts/networking/masquerading.rst:148
msgid ""
"The default behavior will masquerade all traffic leaving on a non-Cilium "
"network device. This typically leads to the correct behavior. In order to"
" limit the network interface on which masquerading should be performed, "
"the option ``egress-masquerade-interfaces: eth0`` can be used."
msgstr ""

#: ../../concepts/networking/masquerading.rst:155
msgid ""
"It is possible to specify an interface prefix as well, by specifying "
"``eth+``, all interfaces matching the prefix ``eth`` will be used for "
"masquerading."
msgstr ""

#: ../../concepts/networking/routing.rst:9
msgid "Routing"
msgstr ""

#: ../../concepts/networking/routing.rst:15
msgid "Encapsulation"
msgstr ""

#: ../../concepts/networking/routing.rst:17
msgid ""
"When no configuration is provided, Cilium automatically runs in this mode"
" as it is the mode with the fewest requirements on the underlying "
"networking infrastructure."
msgstr ""

#: ../../concepts/networking/routing.rst:21
msgid ""
"In this mode, all cluster nodes form a mesh of tunnels using the UDP-"
"based encapsulation protocols :term:`VXLAN` or :term:`Geneve`. All "
"traffic between Cilium nodes is encapsulated."
msgstr ""

#: ../../concepts/networking/routing.rst:26
#: ../../concepts/networking/routing.rst:102
msgid "Requirements on the network"
msgstr ""

#: ../../concepts/networking/routing.rst:28
msgid ""
"Encapsulation relies on normal node to node connectivity. This means that"
" if Cilium nodes can already reach each other, all routing requirements "
"are already met."
msgstr ""

#: ../../concepts/networking/routing.rst:32
msgid "The underlying network and firewalls must allow encapsulated packets:"
msgstr ""

#: ../../concepts/networking/routing.rst:35
msgid "Encapsulation Mode"
msgstr ""

#: ../../concepts/networking/routing.rst:35
msgid "Port Range / Protocol"
msgstr ""

#: ../../concepts/networking/routing.rst:37
msgid "VXLAN (Default)"
msgstr ""

#: ../../concepts/networking/routing.rst:37
msgid "8472/UDP"
msgstr ""

#: ../../concepts/networking/routing.rst:38
msgid "Geneve"
msgstr ""

#: ../../concepts/networking/routing.rst:38
msgid "6081/UDP"
msgstr ""

#: ../../concepts/networking/routing.rst:42
#: ../../concepts/networking/routing.rst:148
msgid "Advantages of the model"
msgstr ""

#: ../../concepts/networking/routing.rst:48
msgid "Simplicity"
msgstr ""

#: ../../concepts/networking/routing.rst:45
msgid ""
"The network which connects the cluster nodes does not need to be made "
"aware of the PodCIDRs. Cluster nodes can spawn multiple routing or link-"
"layer domains. The topology of the underlying network is irrelevant as "
"long as cluster nodes can reach each other using IP/UDP."
msgstr ""

#: ../../concepts/networking/routing.rst:53
msgid "Addressing space"
msgstr ""

#: ../../concepts/networking/routing.rst:51
msgid ""
"Due to not depending on any underlying networking limitations, the "
"available addressing space is potentially much larger and allows to run "
"any number of pods per node if the PodCIDR size is configured "
"accordingly."
msgstr ""

#: ../../concepts/networking/routing.rst:59
msgid "Auto-configuration"
msgstr ""

#: ../../concepts/networking/routing.rst:56
msgid ""
"When running together with an orchestration system such as Kubernetes, "
"the list of all nodes in the cluster including their associated "
"allocation prefix node is made available to each agent automatically. New"
" nodes joining the cluster will automatically be incorporated into the "
"mesh."
msgstr ""

#: ../../concepts/networking/routing.rst:66
msgid "Identity context"
msgstr ""

#: ../../concepts/networking/routing.rst:62
msgid ""
"Encapsulation protocols allow for the carrying of metadata along with the"
" network packet. Cilium makes use of this ability to transfer metadata "
"such as the source security identity. The identity transfer is an "
"optimization designed to avoid one identity lookup on the remote node."
msgstr ""

#: ../../concepts/networking/routing.rst:69
msgid "Disadvantages of the model"
msgstr ""

#: ../../concepts/networking/routing.rst:76
msgid "MTU Overhead"
msgstr ""

#: ../../concepts/networking/routing.rst:72
msgid ""
"Due to adding encapsulation headers, the effective MTU available for "
"payload is lower than with native-routing (50 bytes per network packet "
"for VXLAN). This results in a lower maximum throughput rate for a "
"particular network connection. This can be largely mitigated by enabling "
"jumbo frames (50 bytes of overhead for each 1500 bytes vs 50 bytes of "
"overhead for each 9000 bytes)."
msgstr ""

#: ../../concepts/networking/routing.rst:82
msgid "Native-Routing"
msgstr ""

#: ../../concepts/networking/routing.rst:84
msgid ""
"The native routing datapath is enabled with ``tunnel: disabled`` and "
"enables the native packet forwarding mode. The native packet forwarding "
"mode leverages the routing capabilities of the network Cilium runs on "
"instead of performing encapsulation."
msgstr ""

#: ../../concepts/networking/routing.rst:92
msgid ""
"In native routing mode, Cilium will delegate all packets which are not "
"addressed to another local endpoint to the routing subsystem of the Linux"
" kernel. This means that the packet will be routed as if a local process "
"would have emitted the packet. As a result, the network connecting the "
"cluster nodes must be capable of routing PodCIDRs."
msgstr ""

#: ../../concepts/networking/routing.rst:98
msgid ""
"Cilium automatically enables IP forwarding in the Linux kernel when "
"native routing is configured."
msgstr ""

#: ../../concepts/networking/routing.rst:104
msgid ""
"In order to run the native routing mode, the network connecting the hosts"
" on which Cilium is running on must be capable of forwarding IP traffic "
"using addresses given to pods or other workloads."
msgstr ""

#: ../../concepts/networking/routing.rst:108
msgid ""
"The Linux kernel on the node must be aware on how to forward packets of "
"pods or other workloads of all nodes running Cilium. This can be achieved"
" in two ways:"
msgstr ""

#: ../../concepts/networking/routing.rst:112
msgid ""
"The node itself does not know how to route all pod IPs but a router "
"exists on the network that knows how to reach all other pods. In this "
"scenario, the Linux node is configured to contain a default route to "
"point to such a router. This model is used for cloud provider network "
"integration. See :ref:`gke_datapath`, :ref:`aws_eni_datapath`, and "
":ref:`ipam_azure` for more details."
msgstr ""

#: ../../concepts/networking/routing.rst:119
msgid ""
"Each individual node is made aware of all pod IPs of all other nodes and "
"routes are inserted into the Linux kernel routing table to represent "
"this. If all nodes share a single L2 network, then this can be taken care"
" of by enabling the option ``auto-direct-node-routes: true``. Otherwise, "
"an additional system component such as a BGP daemon must be run to "
"distribute the routes.  See the guide :ref:`kube-router` on how to "
"achieve this using the kube-router project."
msgstr ""

#: ../../concepts/networking/routing.rst:130
msgid ""
"The following configuration options must be set to run the datapath in "
"native routing mode:"
msgstr ""

#: ../../concepts/networking/routing.rst:133
msgid "``tunnel: disabled``: Enable native routing mode."
msgstr ""

#: ../../concepts/networking/routing.rst:134
msgid ""
"``ipv4-native-routing-cidr: x.x.x.x/y``: Set the CIDR in which native "
"routing can be performed."
msgstr ""

#: ../../concepts/networking/routing.rst:143
msgid ""
"The AWS ENI datapath is enabled when Cilium is run with the option "
"``--ipam=eni``. It is a special purpose datapath that is useful when "
"running Cilium in an AWS environment."
msgstr ""

#: ../../concepts/networking/routing.rst:150
msgid ""
"Pods are assigned ENI IPs which are directly routable in the AWS VPC. "
"This simplifies communication of pod traffic within VPCs and avoids the "
"need for SNAT."
msgstr ""

#: ../../concepts/networking/routing.rst:154
msgid ""
"Pod IPs are assigned a security group. The security groups for pods are "
"configured per node which allows to create node pools and give different "
"security group assignments to different pods. See section :ref:`ipam_eni`"
" for more details."
msgstr ""

#: ../../concepts/networking/routing.rst:160
msgid "Disadvantages of this model"
msgstr ""

#: ../../concepts/networking/routing.rst:162
msgid ""
"The number of ENI IPs is limited per instance. The limit depends on the "
"EC2 instance type. This can become a problem when attempting to run a "
"larger number of pods on very small instance types."
msgstr ""

#: ../../concepts/networking/routing.rst:166
msgid ""
"Allocation of ENIs and ENI IPs requires interaction with the EC2 API "
"which is subject to rate limiting. This is primarily mitigated via the "
"operator design, see section :ref:`ipam_eni` for more details."
msgstr ""

#: ../../concepts/networking/routing.rst:174
msgid "Ingress"
msgstr ""

#: ../../concepts/networking/routing.rst:176
msgid ""
"Traffic is received on one of the ENIs attached to the instance which is "
"represented on the node as interface ``ethN``."
msgstr ""

#: ../../concepts/networking/routing.rst:179
msgid ""
"An IP routing rule ensures that traffic to all local pod IPs is done "
"using the main routing table::"
msgstr ""

#: ../../concepts/networking/routing.rst:184
msgid ""
"The main routing table contains an exact match route to steer traffic "
"into a veth pair which is hooked into the pod::"
msgstr ""

#: ../../concepts/networking/routing.rst:189
msgid ""
"All traffic passing ``lxc5a4def8d96c5`` on the way into the pod is "
"subject to Cilium's eBPF program to enforce network policies, provide "
"service reverse load-balancing, and visibility."
msgstr ""

#: ../../concepts/networking/routing.rst:194
msgid "Egress"
msgstr ""

#: ../../concepts/networking/routing.rst:196
msgid ""
"The pod's network namespace contains a default route which points to the "
"node's router IP via the veth pair which is named ``eth0`` inside of the "
"pod and ``lxcXXXXXX`` in the host namespace. The router IP is allocated "
"from the ENI space, allowing for sending of ICMP errors from the router "
"IP for Path MTU purposes."
msgstr ""

#: ../../concepts/networking/routing.rst:202
msgid ""
"After passing through the veth pair and before reaching the Linux routing"
" layer, all traffic is subject to Cilium's eBPF program to enforce "
"network policies, implement load-balancing and provide networking "
"features."
msgstr ""

#: ../../concepts/networking/routing.rst:206
msgid ""
"An IP routing rule ensures that traffic from individual endpoints are "
"using a routing table specific to the ENI from which the endpoint IP was "
"allocated::"
msgstr ""

#: ../../concepts/networking/routing.rst:212
msgid ""
"The ENI specific routing table contains a default route which redirects "
"to the router of the VPC via the ENI interface::"
msgstr ""

#: ../../concepts/networking/routing.rst:222
msgid "The AWS ENI datapath is enabled by setting the following option:"
msgstr ""

#: ../../concepts/networking/routing.rst:231
msgid ""
"``ipam: eni`` Enables the ENI specific IPAM backend and indicates to the "
"datapath that ENI IPs will be used."
msgstr ""

#: ../../concepts/networking/routing.rst:234
msgid ""
"``enable-endpoint-routes: \"true\"`` enables direct routing to the ENI "
"veth pairs without requiring to route via the ``cilium_host`` interface."
msgstr ""

#: ../../concepts/networking/routing.rst:237
msgid ""
"``auto-create-cilium-node-resource: \"true\"`` enables the automatic "
"creation of the ``CiliumNode`` custom resource with all required ENI "
"parameters. It is possible to disable this and provide the custom "
"resource manually."
msgstr ""

#: ../../concepts/networking/routing.rst:241
msgid ""
"``egress-masquerade-interfaces: eth+`` is the interface selector of all "
"interfaces which are subject to masquerading. Masquerading can be "
"disabled entirely with ``enable-ipv4-masquerade: \"false\"``."
msgstr ""

#: ../../concepts/networking/routing.rst:245
msgid ""
"See the section :ref:`ipam_eni` for details on how to configure ENI IPAM "
"specific parameters."
msgstr ""

#: ../../concepts/networking/routing.rst:251
msgid "Google Cloud"
msgstr ""

#: ../../concepts/networking/routing.rst:253
msgid ""
"When running Cilium on Google Cloud via either Google Kubernetes Engine "
"(GKE) or self-managed, it is possible to utilize the `Google Cloud's "
"networking layer <https://cloud.google.com/products/networking>`_ with "
"Cilium running in a :ref:`native_routing` configuration. This provides "
"native networking performance while benefiting from many additional "
"Cilium features such as policy enforcement, load-balancing with DSR, "
"efficient NodePort/ExternalIP/HostPort implementation, extensive "
"visibility features, and so on."
msgstr ""

#: ../../concepts/networking/routing.rst:270
msgid "Addressing"
msgstr ""

#: ../../concepts/networking/routing.rst:266
msgid ""
"Cilium will assign IPs to pods out of the PodCIDR assigned to the "
"specific Kubernetes node. By using `Alias IP ranges "
"<https://cloud.google.com/vpc/docs/alias-ip>`_, these IPs are natively "
"routable on Google Cloud's network without additional encapsulation or "
"route distribution."
msgstr ""

#: ../../concepts/networking/routing.rst:273
msgid ""
"All traffic not staying with the ``ipv4-native-routing-cidr`` (defaults "
"to the Cluster CIDR) will be masqueraded to the node's IP address to "
"become publicly routable."
msgstr ""

#: ../../concepts/networking/routing.rst:281
msgid "Load-balancing"
msgstr ""

#: ../../concepts/networking/routing.rst:278
msgid ""
"ClusterIP load-balancing will be performed using eBPF for all version of "
"GKE. Starting with >= GKE v1.15 or when running a Linux kernel >= 4.19, "
"all NodePort/ExternalIP/HostPort will be performed using a eBPF "
"implementation as well."
msgstr ""

#: ../../concepts/networking/routing.rst:284
msgid "Policy enforcement & visibility"
msgstr ""

#: ../../concepts/networking/routing.rst:284
msgid "All NetworkPolicy enforcement and visibility is provided using eBPF."
msgstr ""

#: ../../concepts/networking/routing.rst:289
msgid ""
"The following configuration options must be set to run the datapath on "
"GKE:"
msgstr ""

#: ../../concepts/networking/routing.rst:291
msgid ""
"``gke.enabled: true``: Enables the Google Kubernetes Engine (GKE) "
"datapath. Setting this to ``true`` will enable the following options:"
msgstr ""

#: ../../concepts/networking/routing.rst:294
msgid "``ipam: kubernetes``: Enable :ref:`k8s_hostscope` IPAM"
msgstr ""

#: ../../concepts/networking/routing.rst:295
msgid "``tunnel: disabled``: Enable native routing mode"
msgstr ""

#: ../../concepts/networking/routing.rst:296
msgid "``enable-endpoint-routes: true``: Enable per-endpoint routing on the node"
msgstr ""

#: ../../concepts/networking/routing.rst:297
msgid ""
"``enable-local-node-route: false``: Disable installation of the local "
"node route"
msgstr ""

#: ../../concepts/networking/routing.rst:299
msgid ""
"``ipv4-native-routing-cidr: x.x.x.x/y``: Set the CIDR in which native "
"routing is supported."
msgstr ""

#: ../../concepts/networking/routing.rst:302
msgid ""
"See the getting started guide :ref:`k8s_install_quick` to install Cilium "
"on Google Kubernetes Engine (GKE)."
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:11
msgid "Hubble Configuration"
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:13
msgid ""
"This page provides guidance to configure Hubble in a way that suits your "
"environment. Instructions to enable Hubble are provided as part of each "
"Cilium :ref:`gs_install` guide."
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:20
msgid "TLS certificates"
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:22
msgid ""
"When Hubble Relay is deployed, Hubble listens on a TCP port on the host "
"network. This allows Hubble Relay to communicate with all Hubble "
"instances in the cluster. Connections between Hubble instances and Hubble"
" Relay are secured using mutual TLS (mTLS) by default."
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:27
msgid ""
"TLS certificates can be provided by manually on the Helm install command "
"(user provided) or generate automatically via either:"
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:30
msgid ""
"`Helm "
"<https://helm.sh/docs/chart_template_guide/function_list/#gensignedcert>`__"
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:31
msgid ""
"cilium's `certgen <https://github.com/cilium/certgen>`__ (using a "
"Kubernetes ``CronJob``)"
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:32
msgid "`cert-manager <https://cert-manager.io/>`__"
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:35
msgid "User provided certificates"
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:37
msgid ""
"In order to use custom TLS certificates, ``hubble.tls.auto.enabled`` must"
" be set to ``false`` and TLS certificates manually provided.  This can be"
" done by specifying the options below to Helm at install or upgrade time."
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:54
msgid ""
"Options ``hubble.relay.tls.server.cert``, ``hubble.relay.tls.server.key``"
" ``hubble.ui.tls.client.cert`` and ``hubble.ui.tls.client.key`` only need"
" to be provided when ``hubble.relay.tls.server.enabled=true`` (default "
"``false``) which enable TLS for the Hubble Relay server."
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:61
msgid "Provided files must be **base64 encoded** PEM certificates."
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:63
msgid ""
"In addition, the **Common Name (CN)** and **Subject Alternative Name "
"(SAN)** of the certificate for Hubble server MUST be set to ``*.{cluster-"
"name}.hubble-grpc.cilium.io`` where ``{cluster-name}`` is the cluster "
"name defined by ``cluster.name`` (defaults to ``default``)."
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:69
msgid "Auto generated certificates via Helm"
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:71
msgid ""
"When using Helm, TLS certificates are (re-)generated every time Helm is "
"used for install or upgrade. As Hubble server and Hubble Relay support "
"TLS certificates hot reloading, including CA certificates, this does not "
"disrupt any existing connection. New connections are automatically "
"established using the new certificates without having to restart Hubble "
"server or Hubble Relay."
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:84
msgid ""
"The downside of the Helm method is that while certificates are "
"automatically generated, they are not automatically renewed.  "
"Consequently, running ``helm upgrade`` is required when certificates are "
"about to expire (i.e. before the configured "
"``hubble.tls.auto.certValidityDuration``)."
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:90
msgid "Auto generated certificates via certgen"
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:92
msgid ""
"Like the Helm method, certgen generates the TLS certificates at "
"installation time and a Kubernetes ``CronJob`` is scheduled to renew them"
" (regardless of their expiration date)."
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:104
msgid "Auto generated certificates via cert-manager"
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:106
msgid ""
"This method relies on `cert-manager <https://cert-manager.io/>`__ to "
"generate the TLS certificates. cert-manager has becomes the de facto way "
"to manage TLS on Kubernetes, and it has the following advantages compared"
" to the previously documented methods:"
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:111
msgid ""
"Support multiple issuers (e.g. a custom CA, `Vault "
"<https://www.vaultproject.io/>`__, `Let's Encrypt "
"<https://letsencrypt.org/>`__, `Google's Certificate Authority Service "
"<https://cloud.google.com/certificate-authority-service>`__, and more) "
"allowing to choose the issuer fitting your organization's requirements."
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:117
msgid ""
"Manages certificates via a `CRD <https://kubernetes.io/docs/concepts"
"/extend-kubernetes/api-extension/custom-resources/>`__ which is easier to"
" inspect with Kubernetes tools than PEM file."
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:121
msgid "**Installation steps**:"
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:123
msgid ""
"First, install `cert-manager <https://cert-"
"manager.io/docs/installation/>`__ and setup an `issuer <https://cert-"
"manager.io/docs/configuration/>`_. Please make sure that your issuer is "
"be able to create certificates under the ``cilium.io`` domain name."
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:127
msgid "Install/upgrade Cilium including the following Helm flags:"
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:138
msgid "**Troubleshooting**:"
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:140
msgid "While installing Cilium or cert-manager you may get the following error:"
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:146
msgid ""
"This happens when cert-manager's webhook (which is used to verify the "
"``Certificate``'s CRD resources) is not available. There are several ways"
" to resolve this issue. Pick one of the options below:"
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:152
msgid "Install CRDs first"
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:154
msgid ""
"Install cert-manager CRDs before Cilium and cert-manager (see `cert-"
"manager's documentation about installing CRDs with kubectl <https://cert-"
"manager.io/docs/installation/helm/#option-1-installing-crds-with-"
"kubectl>`__):"
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:160
msgid "Then install cert-manager, configure an issuer, and install Cilium."
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:162
msgid "Upgrade Cilium"
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:164
msgid "Upgrade Cilium from an installation with TLS disabled:"
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:172
msgid ""
"Then install cert-manager, configure an issuer, and upgrade Cilium "
"enabling TLS:"
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:178
msgid "Disable webhook"
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:180
msgid ""
"Disable cert-manager validation (assuming Cilium is installed in the "
"``kube-system`` namespace):"
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:186
msgid "Then install Cilium, cert-manager, and configure an issuer."
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:188
msgid "Host network webhook"
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:190
msgid ""
"Configure cert-manager to expose its webhook within the host network "
"namespace:"
msgstr ""

#: ../../concepts/observability/hubble-configuration.rst:198
msgid "Then configure an issuer and install Cilium."
msgstr ""

#: ../../concepts/observability/index.rst:11
msgid "Observability"
msgstr ""

#: ../../concepts/observability/intro.rst:13
msgid ""
"Observability is provided by Hubble which enables deep visibility into "
"the communication and behavior of services as well as the networking "
"infrastructure in a completely transparent manner. Hubble is able to "
"provide visibility at the node level, cluster level or even across "
"clusters in a :ref:`Cluster Mesh` scenario. For an introduction to Hubble"
" and how it relates to Cilium, read the section :ref:`intro`."
msgstr ""

#: ../../concepts/observability/intro.rst:20
msgid ""
"By default, the Hubble API is scoped to each individual node on which the"
" Cilium agent runs. In other words, networking visibility is only "
"provided for traffic observed by the local Cilium agent. In this "
"scenario, the only way to interact with the Hubble API is by using the "
"Hubble CLI (``hubble``) to query the Hubble API provided via a local Unix"
" Domain Socket.  The Hubble CLI binary is installed by default on Cilium "
"agent pods."
msgstr ""

#: ../../concepts/observability/intro.rst:27
msgid ""
"When Hubble Relay is deployed, Hubble provides full network visibility. "
"In this scenario, the Hubble Relay service provides a Hubble API which "
"scopes the entire cluster or even multiple clusters in a ClusterMesh "
"scenario. Hubble data can be accessed by pointing a Hubble CLI "
"(``hubble``) to the Hubble Relay service or via Hubble UI. Hubble UI is a"
" web interface which enables automatic discovery of the services "
"dependency graph at the L3/L4 and even L7 layer, allowing user-friendly "
"visualization and filtering of data flows as a service map."
msgstr ""

#: ../../concepts/overview.rst:9
msgid "Component Overview"
msgstr ""

#: ../../concepts/overview.rst:14
msgid ""
"A deployment of Cilium and Hubble consists of the following components "
"running in a cluster:"
msgstr ""

#: ../../concepts/overview.rst:19
msgid "Cilium"
msgstr ""

#: ../../concepts/overview.rst:30
msgid "Agent"
msgstr ""

#: ../../concepts/overview.rst:22
msgid ""
"The Cilium agent (``cilium-agent``) runs on each node in the cluster. At "
"a high-level, the agent accepts configuration via Kubernetes or APIs that"
" describes networking, service load-balancing, network policies, and "
"visibility & monitoring requirements."
msgstr ""

#: ../../concepts/overview.rst:27
msgid ""
"The Cilium agent listens for events from orchestration systems such as "
"Kubernetes to learn when containers or workloads are started and stopped."
" It manages the eBPF programs which the Linux kernel uses to control all "
"network access in / out of those containers."
msgstr ""

#: ../../concepts/overview.rst:37 ../../concepts/overview.rst:75
msgid "Client (CLI)"
msgstr ""

#: ../../concepts/overview.rst:33
msgid ""
"The Cilium CLI client (``cilium``) is a command-line tool that is "
"installed along with the Cilium agent. It interacts with the REST API of "
"the Cilium agent running on the same node. The CLI allows inspecting the "
"state and status of the local agent. It also provides tooling to directly"
" access the eBPF maps to validate their state."
msgstr ""

#: ../../concepts/overview.rst:50
msgid "Operator"
msgstr ""

#: ../../concepts/overview.rst:40
msgid ""
"The Cilium Operator is responsible for managing duties in the cluster "
"which should logically be handled once for the entire cluster, rather "
"than once for each node in the cluster. The Cilium operator is not in the"
" critical path for any forwarding or network policy decision. A cluster "
"will generally continue to function if the operator is temporarily "
"unavailable. However, depending on the configuration, failure in "
"availability of the operator can lead to:"
msgstr ""

#: ../../concepts/overview.rst:47
msgid ""
"Delays in :ref:`address_management` and thus delay in scheduling of new "
"workloads if the operator is required to allocate new IP addresses"
msgstr ""

#: ../../concepts/overview.rst:49
msgid ""
"Failure to update the kvstore heartbeat key which will lead agents to "
"declare kvstore unhealthiness and restart."
msgstr ""

#: ../../concepts/overview.rst:56
msgid "CNI Plugin"
msgstr ""

#: ../../concepts/overview.rst:53
msgid ""
"The CNI plugin (``cilium-cni``) is invoked by Kubernetes when a pod is "
"scheduled or terminated on a node. It interacts with the Cilium API of "
"the node to trigger the necessary datapath configuration to provide "
"networking, load-balancing and network policies for the pod."
msgstr ""

#: ../../concepts/overview.rst:59
msgid "Hubble"
msgstr ""

#: ../../concepts/overview.rst:65
msgid "Server"
msgstr ""

#: ../../concepts/overview.rst:62
msgid ""
"The Hubble server runs on each node and retrieves the eBPF-based "
"visibility from Cilium. It is embedded into the Cilium agent in order to "
"achieve high performance and low-overhead. It offers a gRPC service to "
"retrieve flows and Prometheus metrics."
msgstr ""

#: ../../concepts/overview.rst:71
msgid "Relay"
msgstr ""

#: ../../concepts/overview.rst:68
msgid ""
"Relay (``hubble-relay``) is a standalone component which is aware of all "
"running Hubble servers and offers cluster-wide visibility by connecting "
"to their respective gRPC APIs and providing an API that represents all "
"servers in the cluster."
msgstr ""

#: ../../concepts/overview.rst:74
msgid ""
"The Hubble CLI (``hubble``) is a command-line tool able to connect to "
"either the gRPC API of ``hubble-relay`` or the local server to retrieve "
"flow events."
msgstr ""

#: ../../concepts/overview.rst:79
msgid "Graphical UI (GUI)"
msgstr ""

#: ../../concepts/overview.rst:78
msgid ""
"The graphical user interface (``hubble-ui``) utilizes relay-based "
"visibility to provide a graphical service dependency and connectivity "
"map."
msgstr ""

#: ../../concepts/overview.rst:82
msgid "eBPF"
msgstr ""

#: ../../concepts/overview.rst:84
msgid ""
"eBPF is a Linux kernel bytecode interpreter originally introduced to "
"filter network packets, e.g. tcpdump and socket filters. It has since "
"been extended with additional data structures such as hashtable and "
"arrays as well as additional actions to support packet mangling, "
"forwarding, encapsulation, etc. An in-kernel verifier ensures that eBPF "
"programs are safe to run and a JIT compiler converts the bytecode to CPU "
"architecture specific instructions for native execution efficiency. eBPF "
"programs can be run at various hooking points in the kernel such as for "
"incoming and outgoing packets."
msgstr ""

#: ../../concepts/overview.rst:93
msgid ""
"eBPF continues to evolve and gain additional capabilities with each new "
"Linux release.  Cilium leverages eBPF to perform core datapath filtering,"
" mangling, monitoring and redirection, and requires eBPF capabilities "
"that are in any Linux kernel version 4.8.0 or newer. On the basis that "
"4.8.x is already declared end of life and 4.9.x has been nominated as a "
"stable release we recommend to run at least kernel 4.9.17 (the latest "
"current stable Linux kernel as of this writing is 4.10.x)."
msgstr ""

#: ../../concepts/overview.rst:101
msgid ""
"Cilium is capable of probing the Linux kernel for available features and "
"will automatically make use of more recent features as they are detected."
msgstr ""

#: ../../concepts/overview.rst:104
msgid "For more detail on kernel versions, see: :ref:`admin_kernel_version`."
msgstr ""

#: ../../concepts/overview.rst:107
msgid "Data Store"
msgstr ""

#: ../../concepts/overview.rst:109
msgid ""
"Cilium requires a data store to propagate state between agents. It "
"supports the following data stores:"
msgstr ""

#: ../../concepts/overview.rst:116
msgid "Kubernetes CRDs (Default)"
msgstr ""

#: ../../concepts/overview.rst:113
msgid ""
"The default choice to store any data and propagate state is to use "
"Kubernetes custom resource definitions (CRDs). CRDs are offered by "
"Kubernetes for cluster components to represent configurations and state "
"via Kubernetes resources."
msgstr ""

#: ../../concepts/overview.rst:133
msgid "Key-Value Store"
msgstr ""

#: ../../concepts/overview.rst:119
msgid ""
"All requirements for state storage and propagation can be met with "
"Kubernetes CRDs as configured in the default configuration of Cilium. A "
"key-value store can optionally be used as an optimization to improve the "
"scalability of a cluster as change notifications and storage requirements"
" are more efficient with direct key-value store usage."
msgstr ""

#: ../../concepts/overview.rst:125
msgid "The currently supported key-value stores are:"
msgstr ""

#: ../../concepts/overview.rst:127
msgid "`etcd <https://github.com/etcd-io/etcd>`_"
msgstr ""

#: ../../concepts/overview.rst:131
msgid ""
"It is possible to leverage the etcd cluster of Kubernetes directly or to "
"maintain a dedicated etcd cluster."
msgstr ""

#: ../../concepts/security/identity.rst:11
msgid "Identity-based"
msgstr ""

#: ../../concepts/security/identity.rst:13
msgid ""
"Container management systems such as Kubernetes deploy a networking model"
" which assigns an individual IP address to each pod (group of "
"containers). This ensures simplicity in architecture, avoids unnecessary "
"network address translation (NAT) and provides each individual container "
"with a full range of port numbers to use. The logical consequence of this"
" model is that depending on the size of the cluster and total number of "
"pods, the networking layer has to manage a large number of IP addresses."
msgstr ""

#: ../../concepts/security/identity.rst:21
msgid ""
"Traditionally security enforcement architectures have been based on IP "
"address filters.  Let's walk through a simple example: If all pods with "
"the label ``role=frontend`` should be allowed to initiate connections to "
"all pods with the label ``role=backend`` then each cluster node which "
"runs at least one pod with the label ``role=backend`` must have a "
"corresponding filter installed which allows all IP addresses of all "
"``role=frontend`` pods to initiate a connection to the IP addresses of "
"all local ``role=backend`` pods. All other connection requests should be "
"denied. This could look like this: If the destination address is "
"*10.1.1.2* then allow the connection only if the source address is one of"
" the following *[10.1.2.2,10.1.2.3,20.4.9.1]*."
msgstr ""

#: ../../concepts/security/identity.rst:32
msgid ""
"Every time a new pod with the label ``role=frontend`` or ``role=backend``"
" is either started or stopped, the rules on every cluster node which run "
"any such pods must be updated by either adding or removing the "
"corresponding IP address from the list of allowed IP addresses. In large "
"distributed applications, this could imply updating thousands of cluster "
"nodes multiple times per second depending on the churn rate of deployed "
"pods. Worse, the starting of new ``role=frontend`` pods must be delayed "
"until all servers running ``role=backend`` pods have been updated with "
"the new security rules as otherwise connection attempts from the new pod "
"could be mistakenly dropped. This makes it difficult to scale "
"efficiently."
msgstr ""

#: ../../concepts/security/identity.rst:43
msgid ""
"In order to avoid these complications which can limit scalability and "
"flexibility, Cilium entirely separates security from network addressing. "
"Instead, security is based on the identity of a pod, which is derived "
"through labels.  This identity can be shared between pods. This means "
"that when the first ``role=frontend`` pod is started, Cilium assigns an "
"identity to that pod which is then allowed to initiate connections to the"
" identity of the ``role=backend`` pod. The subsequent start of additional"
" ``role=frontend`` pods only requires to resolve this identity via a key-"
"value store, no action has to be performed on any of the cluster nodes "
"hosting ``role=backend`` pods. The starting of a new pod must only be "
"delayed until the identity of the pod has been resolved which is a much "
"simpler operation than updating the security rules on all other cluster "
"nodes."
msgstr ""

#: ../../concepts/security/index.rst:11
msgid "Network Security"
msgstr ""

#: ../../concepts/security/intro.rst:11
msgid ""
"Cilium provides security on multiple levels. Each can be used "
"individually or combined together."
msgstr ""

#: ../../concepts/security/intro.rst:14
msgid ""
":ref:`arch_id_security`: Connectivity policies between endpoints (Layer "
"3), e.g. any endpoint with label ``role=frontend`` can connect to any "
"endpoint with label ``role=backend``."
msgstr ""

#: ../../concepts/security/intro.rst:17
msgid ""
"Restriction of accessible ports (Layer 4) for both incoming and outgoing "
"connections, e.g. endpoint with label ``role=frontend`` can only make "
"outgoing connections on port 443 (https) and endpoint ``role=backend`` "
"can only accept connections on port 443 (https)."
msgstr ""

#: ../../concepts/security/intro.rst:21
msgid ""
"Fine grained access control on application protocol level to secure HTTP "
"and remote procedure call (RPC) protocols, e.g the endpoint with label "
"``role=frontend`` can only perform the REST API call ``GET "
"/userdata/[0-9]+``, all other API interactions with ``role=backend`` are "
"restricted."
msgstr ""

#: ../../concepts/security/policyenforcement.rst:9
msgid "Policy Enforcement"
msgstr ""

#: ../../concepts/security/policyenforcement.rst:11
msgid ""
"All security policies are described assuming stateful policy enforcement "
"for session based protocols. This means that the intent of the policy is "
"to describe allowed direction of connection establishment. If the policy "
"allows ``A => B`` then reply packets from ``B`` to ``A`` are "
"automatically allowed as well.  However, ``B`` is not automatically "
"allowed to initiate connections to ``A``. If that outcome is desired, "
"then both directions must be explicitly allowed."
msgstr ""

#: ../../concepts/security/policyenforcement.rst:19
msgid ""
"Security policies may be enforced at *ingress* or *egress*. For "
"*ingress*, this means that each cluster node verifies all incoming "
"packets and determines whether the packet is allowed to be transmitted to"
" the intended endpoint. Correspondingly, for *egress* each cluster node "
"verifies outgoing packets and determines whether the packet is allowed to"
" be transmitted to its intended destination."
msgstr ""

#: ../../concepts/security/policyenforcement.rst:26
msgid ""
"In order to enforce identity based security in a multi host cluster, the "
"identity of the transmitting endpoint is embedded into every network "
"packet that is transmitted in between cluster nodes. The receiving "
"cluster node can then extract the identity and verify whether a "
"particular identity is allowed to communicate with any of the local "
"endpoints."
msgstr ""

#: ../../concepts/security/policyenforcement.rst:33
msgid "Default Security Policy"
msgstr ""

#: ../../concepts/security/policyenforcement.rst:35
msgid ""
"If no policy is loaded, the default behavior is to allow all "
"communication unless policy enforcement has been explicitly enabled. As "
"soon as the first policy rule is loaded, policy enforcement is enabled "
"automatically and any communication must then be white listed or the "
"relevant packets will be dropped."
msgstr ""

#: ../../concepts/security/policyenforcement.rst:41
msgid ""
"Similarly, if an endpoint is not subject to an *L4* policy, communication"
" from and to all ports is permitted. Associating at least one *L4* policy"
" to an endpoint will block all connectivity to ports unless explicitly "
"allowed."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:11
msgid "Envoy"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:15
msgid "Go Extensions"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:17
msgid "This feature is currently in beta phase."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:19
msgid ""
"This is a guide for developers who are interested in writing a Go "
"extension to the Envoy proxy as part of Cilium."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:24
msgid ""
"As depicted above, this framework allows a developer to write a small "
"amount of Go code (green box) focused on parsing a new API protocol, and "
"this Go code is able to take full advantage of Cilium features including "
"high-performance redirection to/from Envoy, rich L7-aware policy language"
" and access logging, and visibility into encrypted traffic via kTLS "
"(coming soon!). In sum, you as the developer need only worry about the "
"logic of parsing the protocol, and Cilium + Envoy + eBPF do the heavy-"
"lifting."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:32
msgid ""
"This guide uses simple examples based on a hypothetical \"r2d2\" protocol"
" (see `proxylib/r2d2/r2d2parser.go "
"<https://github.com/cilium/cilium/blob/master/proxylib/r2d2/r2d2parser.go>`_)"
" that might be used to talk to a simple protocol droid a long time ago in"
" a galaxy far, far away. But it also points to other real protocols like "
"Memcached and Cassandra that already exist in the cilium/proxylib "
"directory."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:39
msgid "Step 1: Decide on a Basic Policy Model"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:41
msgid ""
"To get started, take some time to think about what it means to provide "
"protocol-aware security in the context of your chosen protocol.   Most "
"protocols follow a common pattern of a client who performs an "
"''operation'' on a ''resource''.   For example:"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:45
msgid ""
"A standard RESTful HTTP request has a GET/POST/PUT/DELETE methods "
"(operation) and URLs (resource)."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:46
msgid ""
"A database protocol like MySQL has SELECT/INSERT/UPDATE/DELETE actions "
"(operation) on a combined database + table name (resource)."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:47
msgid ""
"A queueing protocol like Kafka has produce/consume (operation) on a "
"particular queue (resource)."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:49
msgid ""
"A common policy model is to allow the user to whitelist certain "
"operations on one or more resources. In some cases, the resources need to"
" support regexes to avoid explicit matching on variable content like ids "
"(e.g., /users/<uuid> would match /users/.*)"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:53
msgid ""
"In our examples, the ''r2d2'' example, we'll use a basic set of "
"operations (READ/WRITE/HALT/RESET). The READ and WRITE commands also "
"support a 'filename' resource, while HALT and RESET have no resource."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:57
msgid "Step 2: Understand Protocol, Encoding, Framing and Types"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:59
msgid ""
"Next, get your head wrapped around how a protocol looks terms of the raw "
"data, as this is what you'll be parsing."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:61
msgid ""
"Try looking for official definitions of the protocol or API.   Official "
"docs will not only help you quickly learn how the protocol works, but "
"will also help you by documenting tricky corner cases that wouldn't be "
"obvious just from regular use of the protocol.   For example, here are "
"example specs for `Redis Protocol <https://redis.io/topics/protocol>`_ , "
"`Cassandra Protocol "
"<https://github.com/apache/cassandra/blob/trunk/doc/native_protocol_v4.spec>`_,"
" and `AWS SQS "
"<https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/Welcome.html>`_"
" ."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:67
msgid "These specs help you understand protocol aspects like:"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:69
msgid ""
"**encoding / framing** : how to recognize the beginning/end of individual"
" requests/replies within a TCP stream. This typically involves reading a "
"header that encodes the overall request length, though some simple "
"protocols use a delimiter like ''\\r\\n\\'' to separate messages."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:73
msgid ""
"**request/reply fields** : for most protocols, you will need to parse out"
" fields at various offsets into the request data in order to extract "
"security-relevant values for visibility + filtering.  In some cases, "
"access control requires filtering requests from clients to servers, but "
"in some cases, parsing replies will also be required if reply data is "
"required to understand future requests (e.g., prepared-statements in "
"database protocols)."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:78
msgid ""
"**message flow** : specs often describe various dependencies between "
"different requests.  Basic protocols tend to follow a simple serial "
"request/reply model, but more advanced protocols will support pipelining "
"(i.e., sending multiple requests before any replies have been received)."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:82
msgid ""
"**protocol errors** : when a Cilium proxy denies a request based on "
"policy, it should return a protocol-specific error to the client (e.g., "
"in HTTP, a proxy should return a ''403 Access Denied'' error).  Looking "
"at the protocol spec will typically indicate how you should return an "
"equivalent ''Access Denied'' error."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:86
msgid ""
"Sometimes, the protocol spec does not give you a full sense of the set of"
" commands that can be sent over the protocol.  In that case, looking at "
"higher-level user documentation can fill in some of these knowledge gaps."
"  Here are examples for `Redis Commands <https://redis.io/commands>`_ and"
" `Cassandra CQL Commands "
"<https://docs.datastax.com/en/archived/cql/3.1/cql/cql_reference/cqlCommandsTOC.html>`_"
" ."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:90
msgid ""
"Another great trick is to use `Wireshark <https://www.wireshark.org>`_  "
"to capture raw packet data between a client and server.   For many "
"protocols, the `Wireshark Sample Captures "
"<https://wiki.wireshark.org/SampleCaptures>`_ has already saved captures "
"for us.  Otherwise, you can easily use tcpdump to capture a file.  For "
"example, for MySQL traffic on port 3306, you could run the following in a"
" container running the MySQL client or server: “tcpdump -s 0 port 3306 -w"
" mysql.pcap”.  `More Info <https://linuxexplore.com/2012/06/07/use-"
"tcpdump-to-capture-in-a-pcap-file-wireshark-dump/>`_"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:96
msgid ""
"In our example r2d2 protocol, we'll keep the spec as simple as possible."
"  It is a text-only based protocol, with each request being a line "
"terminated by ''\\r\\n''.  A request starts with a case-insensitive "
"string command (\"READ\",\"WRITE\",\"HALT\",\"RESET\").   If the command "
"is \"READ\" or \"WRITE\", the command must be followed by a space, and a "
"non-empty filename that contains only non whitespace ASCII characters."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:102
msgid "Step 3: Search for Existing Parser Code / Libraries"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:104
msgid ""
"Look for open source Go library/code that can help. Is there existing "
"open source Go code that parse your protocol that you can leverage, "
"either directly as library or a motivating example?  For example, the "
"`tidwall/recon library <https://github.com/tidwall/redcon>`_ parses Redis"
" in Go, and `Vitess <https://github.com/vitessio/vitess>`_ parses MySQL "
"in Go.   `Wireshark dissectors "
"<https://github.com/boundary/wireshark/tree/master/epan/dissectors>`_ "
"also has a wealth of protocol parsers written in C that can serve as "
"useful guidance.    Note:  finding client-only protocol parsing code is "
"typically less helpful than finding a proxy implementation, or a full "
"parser library.   This is because the set of requests a client parsers is"
" typically the inverse set of the requests a Cilium proxy needs to parse,"
" since the proxy mimics the server rather than the client.   Still, "
"viewing a Go client can give you a general idea of how to parse the "
"general serialization format of the protocol."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:118
msgid "Step 4: Follow the Cilium Developer Guide"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:120
msgid ""
"It is easiest to start Cilium development by following the "
":ref:`dev_guide`"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:122
msgid "After cloning Cilium:"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:130
msgid ""
"While this dev VM is running, you can open additional terminals to the "
"Cilium dev VM by running ''vagrant ssh'' from within the cilium source "
"directory."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:135
msgid "Step 5: Create New Proxy Skeleton"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:137
msgid ""
"From inside the proxylib directory, copy the rd2d directory and rename "
"the files. Replace ''newproto'' with your protocol:"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:148
msgid ""
"Within both newproto.go and newproto_test.go update references to r2d2 "
"with your protocol name.   Search for both ''r2d2'' and ''R2D2''."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:151
msgid "Also, edit proxylib.go and add the following import line:"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:159
msgid "Step 6: Update OnData Method"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:161
msgid ""
"Implementing a parser requires you as the developer to implement three "
"primary functions, shown as blue in the diagram below.   We will cover "
"OnData() in this section, and the other functions in section `Step 9:  "
"Add Policy Loading and Matching`_."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:167
msgid ""
"The beating heart of your parsing is implementing the onData function.  "
"You can think of any proxy as have two data streams, one in the request "
"direction (i.e., client to server) and one in the reply direction (i.e., "
"server to client).   OnData is called when there is data to process, and "
"the value of the boolean 'reply' parameter indicates the direction of the"
" stream for a given call to OnData.   The data passed to OnData is a "
"slice of byte slices (i.e., an array of byte arrays)."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:173
msgid ""
"The return values of the OnData function tell the Go framework tell how "
"data in the stream should be processed, with four primary outcomes:"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:176
msgid ""
"**PASS x** :  The next x bytes in the data stream passed to OnData "
"represent a request/reply that should be passed on to the server/client."
"   The common case here is that this is a request that should be allowed "
"by policy, or that no policy is applied.  Note: x bytes may be less than "
"the total amount of data passed to OnData, in which case the remaining "
"bytes will still be in the data stream when onData is invoked next.  x "
"bytes may also be more than the data that has been passed to OnData. For "
"example, in the case of a protocol where the parser filters only on "
"values in a protocol header, it is often possible to make a filtering "
"decision, and then pass (or drop) the size of the full request/reply "
"without having the entire request passed to Go."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:185
msgid ""
"**MORE x** :  The buffers passed to OnData to do not represent all of the"
" data required to frame and filter the request/reply.  Instead, the "
"parser needs to see at least x additional bytes beyond the current data "
"to make a decision. In some cases, the full request must be read to "
"understand framing and filtering, but in others a decision can be made "
"simply by reading a protocol header.   When parsing data, be defensive, "
"and recognize that it is technically possible that data arrives one byte "
"at a time. Two common scenarios exist here:"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:192
msgid ""
"**Text-based Protocols** : For text-based protocols that use a delimiter "
"like \"\\r\\n\", it is common to simply check if the delimiter exists, "
"and return MORE 1 if it does not, as technically one more character could"
" result in the delimiter being present. See the sample r2d2 parser as a "
"basic example of this."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:197
msgid ""
"**Binary-based protocols** : Many binary protocols have a fixed header "
"length, which containers a field that then indicates the remaining length"
" of the request.  In the binary case, first check to make sure a full "
"header is received.  Typically the header will indicate both the full "
"request length (i.e., framing), as well as the request type, which "
"indicates how much of the full request must be read in order to perform "
"filtering (in many cases, this is less than the full request).  A binary "
"parser will typically return MORE if the data passed to OnData is less "
"than the header length.   After reading a full header, the simple "
"approach is for the parser to return MORE to wait for the full request to"
" be received and parsed  (see the existing CassandraParser as an "
"example). However, as an optimization, the parser can attempt to only "
"request the minimum number of bytes required beyond the header to make a "
"policy decision, and then PASS or DROP the remaining bytes without "
"requiring them to be passed to the Go parser."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:209
msgid ""
"**DROP x** :  Remove the first x bytes from the data stream passed to "
"OnData, as they represent a request/reply that should not be forwarded to"
" the client or server based on policy.  Don't worry about making onData "
"return a drop right away, as we'll return to DROP in a later step below."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:213
msgid ""
"**ERROR y** : The connection contains data that does not match the "
"protocol spec, and prevents you from further parsing the data stream.   "
"The framework will terminate the connection.   An example would be a "
"request length that falls outside the min/max specified by the protocol "
"spec, or values for a field that fall outside the values indicated by the"
" spec (e.g., wrong versions, unknown commands).  If you are still able to"
" properly frame the requests, you can also choose to simply drop the "
"request and return a protocol error (e.g., similar to an ''HTTP 400 Bad "
"Request'' error.   But in all cases, you should write your parser "
"defensively, such that you never forward a request that you do not "
"understand, as such a request could become an avenue for subverting the "
"intended security visibility and filtering policies.  See "
"proxylib/types.h for the set of valid error codes."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:222
msgid ""
"See proxylib/proxylib/parserfactory.go for the official OnData interface "
"definition."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:224
msgid ""
"Keep it simple, and work iteratively.  Start out just getting the framing"
" right.  Can you write a parser that just prints out the length and "
"contents of a request, and then PASS each request with no policy "
"enforcement?"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:227
msgid ""
"One simple trick is to comment out the r2d2 parsing logic in OnData, but "
"leave it in the file as a reference, as your protocol will likely require"
" similar code as we add more functionality below."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:231
msgid "Step 7: Use Unit Testing To Drive Development"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:233
msgid ""
"Use unit tests to drive your development.    Its tempting to want to "
"first test your parser by firing up a client and server and developing on"
" the fly.   But in our experience you’ll iterate faster by using the "
"great unit test framework created along with the Go proxy framework.   "
"This framework lets you pass in an example set of requests as byte arrays"
" to a CheckOnDataOK method, which are passed to the parser's OnData "
"method. CheckOnDataOK takes a set of expected return values, and compares"
" them to the actual return values from OnData processing the byte arrays."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:240
msgid ""
"Take some time to look at the unit tests for the r2d2 parser, and then "
"for more complex parsers like Cassandra and Memcached.   For simple text-"
"based protocols, you can simply write ASCII strings to represent protocol"
" messages, and convert them to []byte arrays and pass them to "
"CheckOnDataOK.   For binary protocols, one can either create byte arrays "
"directly, or use a mechanism to convert a hex string to byte[] array "
"using a helper function like hexData in cassandra/cassandraparser_test.go"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:246
msgid ""
"A great way to get the exact data to pass in is to copy the data from the"
" Wireshark captures mentioned above in Step #2.   You can see the full "
"application layer data streams in Wireshark by right-clicking on a packet"
" and selecting “Follow As… TCP Stream”.  If the protocol is text-based, "
"you can copy the data as ASCII (see r2d2/r2d2parser_test.go as an example"
" of this).   For binary data, it can be easier to instead select “raw” in"
" the drop-down, and use a basic utility to convert from ascii strings to "
"binary raw data (see cassandra/cassandraparser_test.go for an example of "
"this)."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:253
msgid "To run the unit tests, go to proxylib/newproto and run:"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:259
msgid ""
"This will build the latest version of your parser and unit test files and"
" run the unit tests."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:262
msgid "Step 8: Add More Advanced Parsing"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:264
msgid ""
"Thinking back to step #1, what are the critical fields to parse out of "
"the request in order to understand the “operation” and “resource” of each"
" request.  Can you print those out for each request?"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:267
msgid ""
"Use the unit test framework to pass in increasingly complex requests, and"
" confirm that the parser prints out the right values, and that the unit "
"tests are properly slicing the datastream into requests and parsing out "
"the required fields."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:270
msgid ""
"A couple scenarios to make sure your parser handles properly via unit "
"tests:"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:272
msgid "data chunks that are less than a full request (return MORE)"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:273
msgid ""
"requests that are spread across multiple data chunks. (return MORE ,then "
"PASS)"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:274
msgid ""
"multiple requests that are bundled into a single data chunk (return PASS,"
" then another PASS)"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:275
msgid "rejection of malformed requests (return ERROR)."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:277
msgid ""
"For certain advanced cases, it is required for a parser to store state "
"across requests. In this case, data can be stored using data structures "
"that are included as part of the main parser struct.  See CassandraParser"
" in cassandra/cassandraparser.go as an example of how the parser uses a "
"string to store the current 'keyspace' in use, and uses Go maps to keep "
"state required for handling prepared queries."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:284
msgid "Step 9:  Add Policy Loading and Matching"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:286
msgid ""
"Once you have the parsing of most protocol messages ironed out, its time "
"to start enforcing policy."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:288
msgid ""
"First, create a Go object that will represent a single rule in the policy"
" language. For example, this is the rule for the r2d2 protocol, which "
"performs exact match on the command string, and a regex on the filename:"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:299
msgid "There are two key methods to update:"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:301
msgid ""
"Matches :   This function implements the basic logic of comparing data "
"from a single request against a single policy rule, and return true if "
"that rule matches (i.e., allows) that request."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:304
msgid ""
"<NewProto>RuleParser : Reads key value pairs from policy, validates those"
" entries, and stores them as a <NewProto>Rule object."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:307
msgid ""
"See r2d2/r2d2parser.go for examples of both functions for the r2d2 "
"protocol."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:309
msgid ""
"You'll also need to update OnData to call p.connection.Matches(), and if "
"this function return false, return DROP for a request.  Note: despite the"
" similar names between the Matches() function you create in your "
"newprotoparser.go and p.connection.Matches(), do not confuse the two.  "
"Your OnData function should always call p.connection.Matches() rather "
"than invoking your own Matches() directly, as p.connection.Matches() "
"calls the parser's Matches() function only on the subset of L7 rules that"
" apply for the given Cilium source identity for this particular "
"connection."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:317
msgid ""
"Once you add the logic to call Matches() and return DROP in OnData, you "
"will need to update unit tests to have policies that allow the traffic "
"you expect to be passed.   The following is an example of how "
"r2d2/r2d2parser_test.go adds an allow-all policy for a given test:"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:334
msgid ""
"The following is an example of a policy that would allow READ commands "
"with a file regex of \".*\":"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:363
msgid "Step 10: Inject Error Response"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:365
msgid ""
"Simply dropping the request from the request data stream prevents the "
"request from reaching the server, but it would leave the client hanging, "
"waiting for a response that would never come since the server did not see"
" the request."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:368
msgid ""
"Instead, the proxy should return an application-layer reply indicating "
"that access was denied, similar to how an HTTP proxy would return a ''403"
" Access Denied'' error.  Look back at the protocol spec discussed in Step"
" 2 to understand what an access denied message looks like for this "
"protocol, and use the p.connection.Inject() method to send this error "
"reply back to the client.   See r2d2/r2d2parser.go for an example."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:377
msgid ""
"Note:  p.connection.Inject() will inject the data it is passed into the "
"reply datastream.  In order for the client to parse this data correctly, "
"it must be injected at a proper framing boundary (i.e., in between other "
"reply messages that may be in the reply data stream).  If the client is "
"following a basic serial request/reply model per connection, this is "
"essentially guaranteed as at the time of a request that is denied, there "
"are no other replies potentially in the reply datastream.   But if the "
"protocol supports pipelining (i.e., multiple requests in flight) replies "
"must be properly framed and PASSed on a per request basis, and the timing"
" of the call to p.connection.Inject() must be controlled such that the "
"client will properly match the Error response with the correct request."
"   See the Memcached parser as an example of how to accomplish this."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:387
msgid "Step 11: Add Access Logging"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:389
msgid ""
"Cilium also has the notion of an ''Access Log'', which records each "
"request handled by the proxy and indicates whether the request was "
"allowed or denied."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:392
msgid ""
"A call to ''p.connection.Log()'' implements access logging. See the "
"OnData function in r2d2/r2d2parser.go as an example:"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:409
msgid "Step 12: Manual Testing"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:411
msgid ""
"Find the standard docker container for running the protocol server.  "
"Often the same image also has a CLI client that you can use as a client."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:413
msgid ""
"Start both a server and client container running in the cilium dev VM, "
"and attach them to the already created “cilium-net”.  For example, with "
"Cassandra, we run:"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:422
msgid ""
"Note that we run both containers with labels that will make it easy to "
"refer to these containers in a cilium network policy.   Note that we have"
" the client container run the sleep command, as we will use 'docker exec'"
" to access the client CLI."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:426
msgid ""
"Use ''cilium endpoint list'' to identify the IP address of the protocol "
"server."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:437
msgid ""
"One can then invoke the client CLI using that server IP address "
"(10.11.51.247 in the above example):"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:443
msgid ""
"Note that in the above example, ingress policy is not enforced for the "
"Cassandra server endpoint, so no data will flow through the Cassandra "
"parser.  A simple ''allow all'' L7 Cassandra policy can be used to send "
"all data to the Cassandra server through the Go Cassandra parser.  This "
"policy has a single empty rule, which matches all requests.  An allow all"
" policy looks like:"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:463
msgid ""
"A policy can be imported into cilium using ''cilium policy import'', "
"after which another call to ''cilium endpoint list'' confirms that "
"ingress policy is now in place on the server.  If the above policy was "
"saved to a file cass-allow-all.json, one would run:"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:478
msgid ""
"Note that policy is now showing as ''Enabled'' for the Cassandra server "
"on ingress."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:480
msgid "To remove this or any other policy, run:"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:486
msgid ""
"To install a new policy, first delete, and then run ''cilium policy "
"import'' again.  For example, the following policy would allow select "
"statements on a specific set of tables to this Cassandra server, but deny"
" all other queries."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:507
msgid ""
"When performing manual testing, remember that each time you change your "
"Go proxy code, you must re-run ``make`` and ``sudo make install`` and "
"then restart the cilium-agent process.  If the only changes you have made"
" since last compiling cilium are in your cilium/proxylib directory, you "
"can safely just run ``make`` and ``sudo make install``  in that "
"directory, which saves time. For example:"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:521
msgid ""
"If you rebase or other files change, you need to run both commands from "
"the top level directory."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:523
msgid ""
"Cilium agent default to running as a service in the development VM.  "
"However, the default options do not include the ''--debug-verbose=flow'' "
"flag, which is critical to getting visibility in troubleshooting Go proxy"
" frameworks. So it is easiest to stop the cilium service and run the "
"cilium-agent directly as a command in a terminal window, and adding the "
"''--debug-verbose=flow'' flag."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:536
msgid "Step 13: Add Runtime Tests"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:538
msgid ""
"Before submitting this change to the Cilium community, it is recommended "
"that you add runtime tests that will run as part of Cilium's continuous "
"integration testing.   Usually these runtime test can be based on the "
"same container images and test commands you used for manual testing."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:542
msgid ""
"The best approach for adding runtime tests is typically to start out by "
"copying-and-pasting an existing L7 protocol runtime test and then "
"updating it to run the container images and CLI commands specific to the "
"new protocol. See cilium/test/runtime/cassandra.go as an example that "
"matches the use of Cassandra described above in the manual testing "
"section.   Note that the json policy files used by the runtime tests are "
"stored in cilium/test/runtime/manifests, and the Cassandra example "
"policies in those directories are easy to use as a based for similar "
"policies you may create for your new protocol."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:550
msgid "Step 14: Review Spec for Corner Cases"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:552
msgid ""
"Many protocols have advanced features or corner cases that will not "
"manifest themselves as part of basic testing. Once you have written a "
"first rev of the parser, it is a good idea to go back and review the "
"protocol's spec or list of commands to see what if any aspects may fall "
"outside the scope of your initial parser. For example, corner cases like "
"the handling of empty or nil lists may not show up in your testing, but "
"may cause your parser to fail.   Add more unit tests to cover these "
"corner cases. It is OK for the first rev of your parser not to handle all"
" types of requests, or to have a simplified policy structure in terms of "
"which fields can be matched.   However, it is important to know what "
"aspects of the protocol you are not parsing, and ensure that it does not "
"lead to any security concerns. For example, failing to parse prepared "
"statements in a database protocol and instead just passing PREPARE and "
"EXECUTE commands through would lead to gaping security whole that would "
"render your other filtering meaningless in the face of a sophisticated "
"attacker."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:565
msgid "Step 15: Write Docs or Getting Started Guide (optional)"
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:567
msgid ""
"At a minimum, the policy examples included as part of the runtime tests "
"serve as basic documentation of the policy and its expected behavior.  "
"But we also encourage adding more user friendly examples and "
"documentation, for example, Getting Started Guides.  "
"cilium/Documentation/gettingstarted/cassandra.rst is a good example to "
"follow.   Also be sure to update Documentation/gettingstarted/index.rst "
"with a link to this new getting started guide."
msgstr ""

#: ../../concepts/security/proxy/envoy.rst:574
msgid ""
"With that, you are ready to post this change for feedback from the Cilium"
" community.  Congrats!"
msgstr ""

#: ../../concepts/security/proxy/index.rst:9
msgid "Proxy Injection"
msgstr ""

#: ../../concepts/security/proxy/index.rst:11
msgid ""
"Cilium is capable of transparently injecting a Layer 4 proxy into any "
"network connection. This is used as the foundation to enforce higher "
"level network policies (see :ref:`DNS based` and :ref:`l7_policy`)."
msgstr ""

#: ../../concepts/security/proxy/index.rst:15
msgid "The following proxies can be injected:"
msgstr ""

#: ../../concepts/terminology.rst:9
msgid "Terminology"
msgstr ""

#: ../../concepts/terminology.rst:16 ../../concepts/terminology.rst:254
msgid "Labels"
msgstr ""

#: ../../concepts/terminology.rst:18
msgid ""
"Labels are a generic, flexible and highly scalable way of addressing a "
"large set of resources as they allow for arbitrary grouping and creation "
"of sets. Whenever something needs to be described, addressed or selected,"
" it is done based on labels:"
msgstr ""

#: ../../concepts/terminology.rst:23
msgid ""
"`Endpoints` are assigned labels as derived from the container runtime, "
"orchestration system, or other sources."
msgstr ""

#: ../../concepts/terminology.rst:25
msgid ""
"`Network policies` select pairs of `endpoints` which are allowed to "
"communicate based on labels. The policies themselves are identified by "
"labels as well."
msgstr ""

#: ../../concepts/terminology.rst:30
msgid "What is a Label?"
msgstr ""

#: ../../concepts/terminology.rst:32
msgid ""
"A label is a pair of strings consisting of a ``key`` and ``value``. A "
"label can be formatted as a single string with the format ``key=value``. "
"The key portion is mandatory and must be unique. This is typically "
"achieved by using the reverse domain name notion, e.g. "
"``io.cilium.mykey=myvalue``. The value portion is optional and can be "
"omitted, e.g. ``io.cilium.mykey``."
msgstr ""

#: ../../concepts/terminology.rst:38
msgid "Key names should typically consist of the character set ``[a-z0-9-.]``."
msgstr ""

#: ../../concepts/terminology.rst:40
msgid ""
"When using labels to select resources, both the key and the value must "
"match, e.g. when a policy should be applied to all endpoints with the "
"label ``my.corp.foo`` then the label ``my.corp.foo=bar`` will not match "
"the selector."
msgstr ""

#: ../../concepts/terminology.rst:46
msgid "Label Source"
msgstr ""

#: ../../concepts/terminology.rst:48
msgid ""
"A label can be derived from various sources. For example, an `endpoint`_ "
"will derive the labels associated to the container by the local container"
" runtime as well as the labels associated with the pod as provided by "
"Kubernetes. As these two label namespaces are not aware of each other, "
"this may result in conflicting label keys."
msgstr ""

#: ../../concepts/terminology.rst:54
msgid ""
"To resolve this potential conflict, Cilium prefixes all label keys with "
"``source:`` to indicate the source of the label when importing labels, "
"e.g. ``k8s:role=frontend``, ``container:user=joe``, ``k8s:role=backend``."
" This means that when you run a Docker container using ``docker run [...]"
" -l foo=bar``, the label ``container:foo=bar`` will appear on the Cilium "
"endpoint representing the container. Similarly, a Kubernetes pod started "
"with the label ``foo: bar`` will be represented with a Cilium endpoint "
"associated with the label ``k8s:foo=bar``. A unique name is allocated for"
" each potential source. The following label sources are currently "
"supported:"
msgstr ""

#: ../../concepts/terminology.rst:64
msgid "``container:`` for labels derived from the local container runtime"
msgstr ""

#: ../../concepts/terminology.rst:65
msgid "``k8s:`` for labels derived from Kubernetes"
msgstr ""

#: ../../concepts/terminology.rst:66
msgid "``reserved:`` for special reserved labels, see :ref:`reserved_labels`."
msgstr ""

#: ../../concepts/terminology.rst:67
msgid "``unspec:`` for labels with unspecified source"
msgstr ""

#: ../../concepts/terminology.rst:69
msgid ""
"When using labels to identify other resources, the source can be included"
" to limit matching of labels to a particular type. If no source is "
"provided, the label source defaults to ``any:`` which will match all "
"labels regardless of their source. If a source is provided, the source of"
" the selecting and matching labels need to match."
msgstr ""

#: ../../concepts/terminology.rst:79
msgid "Endpoint"
msgstr ""

#: ../../concepts/terminology.rst:81
msgid ""
"Cilium makes application containers available on the network by assigning"
" them IP addresses. Multiple application containers can share the same IP"
" address; a typical example for this model is a Kubernetes :term:`Pod`. "
"All application containers which share a common address are grouped "
"together in what Cilium refers to as an endpoint."
msgstr ""

#: ../../concepts/terminology.rst:87
msgid ""
"Allocating individual IP addresses enables the use of the entire Layer 4 "
"port range by each endpoint. This essentially allows multiple application"
" containers running on the same cluster node to all bind to well known "
"ports such as ``80`` without causing any conflicts."
msgstr ""

#: ../../concepts/terminology.rst:92
msgid ""
"The default behavior of Cilium is to assign both an IPv6 and IPv4 address"
" to every endpoint. However, this behavior can be configured to only "
"allocate an IPv6 address with the ``--enable-ipv4=false`` option. If both"
" an IPv6 and IPv4 address are assigned, either address can be used to "
"reach the endpoint. The same behavior will apply with regard to policy "
"rules, load-balancing, etc. See :ref:`address_management` for more "
"details."
msgstr ""

#: ../../concepts/terminology.rst:100
msgid "Identification"
msgstr ""

#: ../../concepts/terminology.rst:102
msgid ""
"For identification purposes, Cilium assigns an internal endpoint id to "
"all endpoints on a cluster node. The endpoint id is unique within the "
"context of an individual cluster node."
msgstr ""

#: ../../concepts/terminology.rst:109
msgid "Endpoint Metadata"
msgstr ""

#: ../../concepts/terminology.rst:111
msgid ""
"An endpoint automatically derives metadata from the application "
"containers associated with the endpoint. The metadata can then be used to"
" identify the endpoint for security/policy, load-balancing and routing "
"purposes."
msgstr ""

#: ../../concepts/terminology.rst:115
msgid ""
"The source of the metadata will depend on the orchestration system and "
"container runtime in use. The following metadata retrieval mechanisms are"
" currently supported:"
msgstr ""

#: ../../concepts/terminology.rst:120
msgid "System"
msgstr ""

#: ../../concepts/terminology.rst:122
msgid "Kubernetes"
msgstr ""

#: ../../concepts/terminology.rst:122
msgid "Pod labels (via k8s API)"
msgstr ""

#: ../../concepts/terminology.rst:124
msgid "containerd (Docker)"
msgstr ""

#: ../../concepts/terminology.rst:124
msgid "Container labels (via Docker API)"
msgstr ""

#: ../../concepts/terminology.rst:127
msgid "Metadata is attached to endpoints in the form of `labels`."
msgstr ""

#: ../../concepts/terminology.rst:129
msgid ""
"The following example launches a container with the label "
"``app=benchmark`` which is then associated with the endpoint. The label "
"is prefixed with ``container:`` to indicate that the label was derived "
"from the container runtime."
msgstr ""

#: ../../concepts/terminology.rst:144
msgid ""
"An endpoint can have metadata associated from multiple sources. A typical"
" example is a Kubernetes cluster which uses containerd as the container "
"runtime. Endpoints will derive Kubernetes pod labels (prefixed with the "
"``k8s:`` source prefix) and containerd labels (prefixed with "
"``container:`` source prefix)."
msgstr ""

#: ../../concepts/terminology.rst:152 ../../concepts/terminology.rst:203
msgid "Identity"
msgstr ""

#: ../../concepts/terminology.rst:154
msgid ""
"All `endpoints` are assigned an identity. The identity is what is used to"
" enforce basic connectivity between endpoints. In traditional networking "
"terminology, this would be equivalent to Layer 3 enforcement."
msgstr ""

#: ../../concepts/terminology.rst:158
msgid ""
"An identity is identified by `labels` and is given a cluster wide unique "
"identifier. The endpoint is assigned the identity which matches the "
"endpoint's `security relevant labels`, i.e. all endpoints which share the"
" same set of `security relevant labels` will share the same identity. "
"This concept allows to scale policy enforcement to a massive number of "
"endpoints as many individual endpoints will typically share the same set "
"of security `labels` as applications are scaled."
msgstr ""

#: ../../concepts/terminology.rst:167
msgid "What is an Identity?"
msgstr ""

#: ../../concepts/terminology.rst:169
msgid ""
"The identity of an endpoint is derived based on the `labels` associated "
"with the pod or container which are derived to the `endpoint`_. When a "
"pod or container is started, Cilium will create an `endpoint`_ based on "
"the event received by the container runtime to represent the pod or "
"container on the network. As a next step, Cilium will resolve the "
"identity of the `endpoint`_ created. Whenever the `labels` of the pod or "
"container change, the identity is reconfirmed and automatically modified "
"as required."
msgstr ""

#: ../../concepts/terminology.rst:180
msgid "Security Relevant Labels"
msgstr ""

#: ../../concepts/terminology.rst:182
msgid ""
"Not all `labels` associated with a container or pod are meaningful when "
"deriving the `identity`. Labels may be used to store metadata such as the"
" timestamp when a container was launched. Cilium requires to know which "
"labels are meaningful and are subject to being considered when deriving "
"the identity. For this purpose, the user is required to specify a list of"
" string prefixes of meaningful labels. The standard behavior is to "
"include all labels which start with the prefix ``id.``, e.g.  "
"``id.service1``, ``id.service2``, ``id.groupA.service44``. The list of "
"meaningful label prefixes can be specified when starting the agent."
msgstr ""

#: ../../concepts/terminology.rst:195
msgid "Special Identities"
msgstr ""

#: ../../concepts/terminology.rst:197
msgid ""
"All endpoints which are managed by Cilium will be assigned an identity. "
"In order to allow communication to network endpoints which are not "
"managed by Cilium, special identities exist to represent those. Special "
"reserved identities are prefixed with the string ``reserved:``."
msgstr ""

#: ../../concepts/terminology.rst:203 ../../concepts/terminology.rst:254
msgid "Numeric ID"
msgstr ""

#: ../../concepts/terminology.rst:205
msgid "``reserved:unknown``"
msgstr ""

#: ../../concepts/terminology.rst:205
msgid "0"
msgstr ""

#: ../../concepts/terminology.rst:205
msgid "The identity could not be derived."
msgstr ""

#: ../../concepts/terminology.rst:207
msgid "``reserved:host``"
msgstr ""

#: ../../concepts/terminology.rst:207
msgid ""
"The local host. Any traffic that originates from or is designated to one "
"of the local host IPs."
msgstr ""

#: ../../concepts/terminology.rst:210
msgid "``reserved:world``"
msgstr ""

#: ../../concepts/terminology.rst:210
msgid "Any network endpoint outside of the cluster"
msgstr ""

#: ../../concepts/terminology.rst:212
msgid "``reserved:unmanaged``"
msgstr ""

#: ../../concepts/terminology.rst:212
msgid "3"
msgstr ""

#: ../../concepts/terminology.rst:212
msgid ""
"An endpoint that is not managed by Cilium, e.g. a Kubernetes pod that was"
" launched before Cilium was installed."
msgstr ""

#: ../../concepts/terminology.rst:216
msgid "``reserved:health``"
msgstr ""

#: ../../concepts/terminology.rst:216
msgid "This is health checking traffic generated by Cilium agents."
msgstr ""

#: ../../concepts/terminology.rst:219
msgid "``reserved:init``"
msgstr ""

#: ../../concepts/terminology.rst:219
msgid "5"
msgstr ""

#: ../../concepts/terminology.rst:219
msgid ""
"An endpoint for which the identity has not yet been resolved is assigned "
"the init identity. This represents the phase of an endpoint in which some"
" of the metadata required to derive the security identity is still "
"missing. This is typically the case in the bootstrapping phase."
msgstr ""

#: ../../concepts/terminology.rst:226
msgid ""
"The init identity is only allocated if the labels of the endpoint are not"
" known at creation time. This can be the case for the Docker plugin."
msgstr ""

#: ../../concepts/terminology.rst:230
msgid "``reserved:remote-node``"
msgstr ""

#: ../../concepts/terminology.rst:230
msgid "6"
msgstr ""

#: ../../concepts/terminology.rst:230
msgid ""
"The collection of all remote cluster hosts. Any traffic that originates "
"from or is designated to one of the IPs of any host in any connected "
"cluster other than the local node."
msgstr ""

#: ../../concepts/terminology.rst:238
msgid ""
"Cilium used to include both the local and all remote hosts in the "
"``reserved:host`` identity. This is still the default option unless a "
"recent default ConfigMap is used. The remote-node identity can be enabled"
" via the option ``enable-remote-node-identity``."
msgstr ""

#: ../../concepts/terminology.rst:244
msgid "Well-known Identities"
msgstr ""

#: ../../concepts/terminology.rst:246
msgid ""
"The following is a list of well-known identities which Cilium is aware of"
" automatically and will hand out a security identity without requiring to"
" contact any external dependencies such as the kvstore. The purpose of "
"this is to allow bootstrapping Cilium and enable network connectivity "
"with policy enforcement in the cluster for essential services without "
"depending on any dependencies."
msgstr ""

#: ../../concepts/terminology.rst:254
msgid "Namespace"
msgstr ""

#: ../../concepts/terminology.rst:254
msgid "ServiceAccount"
msgstr ""

#: ../../concepts/terminology.rst:254
msgid "Cluster Name"
msgstr ""

#: ../../concepts/terminology.rst:256 ../../concepts/terminology.rst:257
msgid "kube-dns"
msgstr ""

#: ../../concepts/terminology.rst:256 ../../concepts/terminology.rst:257
#: ../../concepts/terminology.rst:258 ../../concepts/terminology.rst:259
msgid "kube-system"
msgstr ""

#: ../../concepts/terminology.rst:256 ../../concepts/terminology.rst:257
#: ../../concepts/terminology.rst:258 ../../concepts/terminology.rst:259
#: ../../concepts/terminology.rst:260
msgid "<cilium-cluster>"
msgstr ""

#: ../../concepts/terminology.rst:256
msgid "102"
msgstr ""

#: ../../concepts/terminology.rst:256 ../../concepts/terminology.rst:258
msgid "``k8s-app=kube-dns``"
msgstr ""

#: ../../concepts/terminology.rst:257
msgid "kube-dns (EKS)"
msgstr ""

#: ../../concepts/terminology.rst:257
msgid "103"
msgstr ""

#: ../../concepts/terminology.rst:257
msgid "``k8s-app=kube-dns``, ``eks.amazonaws.com/component=kube-dns``"
msgstr ""

#: ../../concepts/terminology.rst:258
msgid "core-dns"
msgstr ""

#: ../../concepts/terminology.rst:258 ../../concepts/terminology.rst:259
msgid "coredns"
msgstr ""

#: ../../concepts/terminology.rst:258
msgid "104"
msgstr ""

#: ../../concepts/terminology.rst:259
msgid "core-dns (EKS)"
msgstr ""

#: ../../concepts/terminology.rst:259
msgid "106"
msgstr ""

#: ../../concepts/terminology.rst:259
msgid "``k8s-app=kube-dns``, ``eks.amazonaws.com/component=coredns``"
msgstr ""

#: ../../concepts/terminology.rst:260
msgid "cilium-operator"
msgstr ""

#: ../../concepts/terminology.rst:260
msgid "<cilium-namespace>"
msgstr ""

#: ../../concepts/terminology.rst:260
msgid "105"
msgstr ""

#: ../../concepts/terminology.rst:260
msgid "``name=cilium-operator``, ``io.cilium/app=operator``"
msgstr ""

#: ../../concepts/terminology.rst:263
msgid ""
"*Note*: if ``cilium-cluster`` is not defined with the ``cluster-name`` "
"option, the default value will be set to \"``default``\"."
msgstr ""

#: ../../concepts/terminology.rst:267
msgid "Identity Management in the Cluster"
msgstr ""

#: ../../concepts/terminology.rst:269
msgid ""
"Identities are valid in the entire cluster which means that if several "
"pods or containers are started on several cluster nodes, all of them will"
" resolve and share a single identity if they share the identity relevant "
"labels. This requires coordination between cluster nodes."
msgstr ""

#: ../../concepts/terminology.rst:277
msgid ""
"The operation to resolve an endpoint identity is performed with the help "
"of the distributed key-value store which allows to perform atomic "
"operations in the form *generate a new unique identifier if the following"
" value has not been seen before*. This allows each cluster node to create"
" the identity relevant subset of labels and then query the key-value "
"store to derive the identity. Depending on whether the set of labels has "
"been queried before, either a new identity will be created, or the "
"identity of the initial query will be returned."
msgstr ""

#: ../../concepts/terminology.rst:286
msgid "Node"
msgstr ""

#: ../../concepts/terminology.rst:288
msgid ""
"Cilium refers to a node as an individual member of a cluster. Each node "
"must be running the ``cilium-agent`` and will operate in a mostly "
"autonomous manner. Synchronization of state between Cilium agents running"
" on different nodes is kept to a minimum for simplicity and scale. It "
"occurs exclusively via the Key-Value store or with packet metadata."
msgstr ""

#: ../../concepts/terminology.rst:295
msgid "Node Address"
msgstr ""

#: ../../concepts/terminology.rst:297
msgid ""
"Cilium will automatically detect the node's IPv4 and IPv6 address. The "
"detected node address is printed out when the ``cilium-agent`` starts:"
msgstr ""

