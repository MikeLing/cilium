# SOME DESCRIPTIVE TITLE.
# Copyright (C) Cilium Authors
# This file is distributed under the same license as the Cilium package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
msgid ""
msgstr ""
"Project-Id-Version: Cilium\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-05-25 00:09+0800\n"
"PO-Revision-Date: 2022-06-01 00:19+0800\n"
"Last-Translator: \n"
"Language-Team: \n"
"Language: zh\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"
"X-Generator: Poedit 3.0.1\n"

#: ../../gettingstarted/alibabacloud-eni.rst:3
#: ../../gettingstarted/alibabacloud-eni.rst:37 ../../gettingstarted/aws.rst:3
#: ../../gettingstarted/bandwidth-manager.rst:3 ../../gettingstarted/bgp.rst:3
#: ../../gettingstarted/bird.rst:3 ../../gettingstarted/cassandra.rst:3
#: ../../gettingstarted/ciliumendpointslice.rst:3
#: ../../gettingstarted/cni-chaining-aws-cni.rst:3
#: ../../gettingstarted/cni-chaining-azure-cni.rst:3
#: ../../gettingstarted/cni-chaining-calico.rst:3
#: ../../gettingstarted/cni-chaining-generic-veth.rst:3
#: ../../gettingstarted/cni-chaining-limitations.rst:3
#: ../../gettingstarted/cni-chaining-portmap.rst:3
#: ../../gettingstarted/cni-chaining-weave.rst:3
#: ../../gettingstarted/cni-chaining.rst:3 ../../gettingstarted/dns.rst:3
#: ../../gettingstarted/egress-gateway.rst:3
#: ../../gettingstarted/elasticsearch.rst:3
#: ../../gettingstarted/encryption-ipsec.rst:3
#: ../../gettingstarted/encryption-wireguard.rst:3
#: ../../gettingstarted/encryption.rst:3
#: ../../gettingstarted/external-workloads.rst:3
#: ../../gettingstarted/grafana.rst:3 ../../gettingstarted/grpc.rst:3
#: ../../gettingstarted/host-firewall.rst:3
#: ../../gettingstarted/host-services.rst:3 ../../gettingstarted/http.rst:3
#: ../../gettingstarted/hubble.rst:3 ../../gettingstarted/hubble_cli.rst:3
#: ../../gettingstarted/hubble_setup.rst:3 ../../gettingstarted/index.rst:3
#: ../../gettingstarted/ipam-crd.rst:3 ../../gettingstarted/ipam.rst:3
#: ../../gettingstarted/istio.rst:3 ../../gettingstarted/k3s.rst:3
#: ../../gettingstarted/k8s-install-advanced.rst:3
#: ../../gettingstarted/k8s-install-default.rst:3
#: ../../gettingstarted/k8s-install-download-release.rst:3
#: ../../gettingstarted/k8s-install-external-etcd.rst:3
#: ../../gettingstarted/k8s-install-helm.rst:3
#: ../../gettingstarted/k8s-install-kops.rst:3
#: ../../gettingstarted/k8s-install-kubeadm.rst:3
#: ../../gettingstarted/k8s-install-kubespray.rst:3
#: ../../gettingstarted/k8s-install-openshift-okd.rst:3
#: ../../gettingstarted/k8s-install-rancher-existing-nodes.rst:3
#: ../../gettingstarted/k8s-install-rke.rst:3 ../../gettingstarted/kafka.rst:3
#: ../../gettingstarted/kata.rst:3 ../../gettingstarted/kind.rst:3
#: ../../gettingstarted/kube-router.rst:3
#: ../../gettingstarted/kubeproxy-free.rst:3
#: ../../gettingstarted/local-redirect-policy.rst:3
#: ../../gettingstarted/memcached.rst:3 ../../gettingstarted/microk8s.rst:3
#: ../../gettingstarted/policy-creation.rst:3
#: ../../gettingstarted/rancher-desktop.rst:3 ../../gettingstarted/taints.rst:3
#: ../../gettingstarted/tls-visibility.rst:3 ../../gettingstarted/vtep.rst:3
msgid ""
"WARNING: You are looking at unreleased Cilium documentation. Please use the "
"official rendered version released here: https://docs.cilium.io"
msgstr ""

#: ../../gettingstarted/alibabacloud-eni.rst:11
msgid "Setting Up Cilium in AlibabaCloud ENI Mode (beta)"
msgstr ""

#: ../../beta.rst:3
msgid ""
"This is a beta feature. Please provide feedback and file a GitHub issue if you "
"experience any problems."
msgstr ""

#: ../../gettingstarted/alibabacloud-eni.rst:17
msgid ""
"The AlibabaCloud ENI integration is still subject to some limitations. See :ref:"
"`alibabacloud_eni_limitations` for details."
msgstr ""

#: ../../gettingstarted/alibabacloud-eni.rst:21
msgid "Create a Cluster on AlibabaCloud"
msgstr ""

#: ../../gettingstarted/alibabacloud-eni.rst:23
msgid ""
"Setup a Kubernetes on AlibabaCloud. You can use any method you prefer. The "
"quickest way is to create an ACK (Alibaba Cloud Container Service for "
"Kubernetes) cluster and to replace the CNI plugin with Cilium. For more details "
"on how to set up an ACK cluster please follow the `official documentation "
"<https://www.alibabacloud.com/help/doc-detail/86745.htm>`_."
msgstr ""

#: ../../gettingstarted/alibabacloud-eni.rst:31
msgid "Disable ACK CNI (ACK Only)"
msgstr ""

#: ../../gettingstarted/alibabacloud-eni.rst:33
msgid "If you are running an ACK cluster, you should delete the ACK CNI."
msgstr ""

#: ../../gettingstarted/alibabacloud-eni.rst:41
msgid ""
"Cilium will manage ENIs instead of the ACK CNI, so any running DaemonSet from "
"the list below has to be deleted to prevent conflicts."
msgstr ""

#: ../../gettingstarted/alibabacloud-eni.rst:44
msgid "``kube-flannel-ds``"
msgstr ""

#: ../../gettingstarted/alibabacloud-eni.rst:45
msgid "``terway``"
msgstr ""

#: ../../gettingstarted/alibabacloud-eni.rst:46
msgid "``terway-eni``"
msgstr ""

#: ../../gettingstarted/alibabacloud-eni.rst:47
msgid "``terway-eniip``"
msgstr ""

#: ../../gettingstarted/alibabacloud-eni.rst:51
msgid ""
"If you are using ACK with Flannel (DaemonSet ``kube-flannel-ds``), the Cloud "
"Controller Manager (CCM) will create route (Pod CIDR) in VPC. If your cluster "
"is an Managed Kubernetes you cannot disable this behavior. Please consider "
"creating a new cluster."
msgstr ""

#: ../../gettingstarted/alibabacloud-eni.rst:60
msgid "The next step is to remove CRD below created by ``terway*`` CNI"
msgstr ""

#: ../../gettingstarted/alibabacloud-eni.rst:81
msgid "Create AlibabaCloud Secrets"
msgstr ""

#: ../../gettingstarted/alibabacloud-eni.rst:83
msgid ""
"Before installing Cilium, a new Kubernetes Secret with the AlibabaCloud Tokens "
"needs to be added to your Kubernetes cluster. This Secret will allow Cilium to "
"gather information from the AlibabaCloud API which is needed to implement "
"ToGroups policies."
msgstr ""

#: ../../gettingstarted/alibabacloud-eni.rst:88
msgid "AlibabaCloud Access Keys"
msgstr ""

#: ../../gettingstarted/alibabacloud-eni.rst:90
msgid ""
"To create a new access token the `following guide can be used <https://www."
"alibabacloud.com/help/doc-detail/93691.htm>`_. These keys need to have certain "
"`RAM Permissions <https://ram.console.aliyun.com/overview>`_:"
msgstr ""

#: ../../gettingstarted/alibabacloud-eni.rst:133 ../../gettingstarted/aws.rst:54
msgid ""
"As soon as you have the access tokens, the following secret needs to be added, "
"with each empty string replaced by the associated value as a base64-encoded "
"string:"
msgstr ""

#: ../../gettingstarted/alibabacloud-eni.rst:150 ../../gettingstarted/aws.rst:72
msgid ""
"The base64 command line utility can be used to generate each value, for example:"
msgstr ""

#: ../../gettingstarted/alibabacloud-eni.rst:157
msgid ""
"This secret stores the AlibabaCloud credentials, which will be used to connect "
"to the AlibabaCloud API."
msgstr ""

#: ../../gettingstarted/alibabacloud-eni.rst:166 ../../gettingstarted/bgp.rst:31
#: ../../gettingstarted/cni-chaining-azure-cni.rst:78
#: ../../gettingstarted/k8s-install-kubeadm.rst:51
#: ../../gettingstarted/k8s-install-rke.rst:43 ../../gettingstarted/kata.rst:54
#: ../../gettingstarted/kube-router.rst:80
msgid "Deploy Cilium"
msgstr ""

#: ../../gettingstarted/k8s-install-download-release.rst:9
msgid ""
"Make sure you have Helm 3 `installed <https://helm.sh/docs/intro/install/>`_. "
"Helm 2 is `no longer supported <https://helm.sh/blog/helm-v2-deprecation-"
"timeline/>`_."
msgstr ""

#: ../../gettingstarted/k8s-install-download-release.rst:14
msgid "Setup Helm repository:"
msgstr ""

#: ../../gettingstarted/k8s-install-download-release.rst:22
msgid ""
"Download the Cilium release tarball and change to the kubernetes install "
"directory:"
msgstr ""

#: ../../gettingstarted/alibabacloud-eni.rst:170
#: ../../gettingstarted/cni-chaining-azure-cni.rst:82
#: ../../gettingstarted/cni-chaining-calico.rst:73
#: ../../gettingstarted/cni-chaining-generic-veth.rst:80
#: ../../gettingstarted/cni-chaining-portmap.rst:39
#: ../../gettingstarted/cni-chaining-weave.rst:63
#: ../../gettingstarted/host-firewall.rst:21
#: ../../gettingstarted/host-services.rst:27
#: ../../gettingstarted/k8s-install-external-etcd.rst:51
#: ../../gettingstarted/k8s-install-helm.rst:44
#: ../../gettingstarted/k8s-install-helm.rst:64
#: ../../gettingstarted/k8s-install-helm.rst:120
#: ../../gettingstarted/k8s-install-helm.rst:152
#: ../../gettingstarted/k8s-install-kubeadm.rst:55
#: ../../gettingstarted/kata.rst:58
msgid "Deploy Cilium release via Helm:"
msgstr ""

#: ../../gettingstarted/alibabacloud-eni.rst:183
msgid ""
"You must ensure that the security groups associated with the ENIs (``eth1``, "
"``eth2``, ...) allow for egress traffic to go outside of the VPC. By default, "
"the security groups for pod ENIs are derived from the primary ENI (``eth0``)."
msgstr ""

#: ../../gettingstarted/k3s.rst:74
#: ../../gettingstarted/k8s-install-default.rst:349
#: ../../gettingstarted/k8s-install-validate.rst:2
#: ../../gettingstarted/rancher-desktop.rst:33
msgid "Validate the Installation"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:64
#: ../../gettingstarted/encryption-ipsec.rst:112
#: ../../gettingstarted/encryption-ipsec.rst:133
#: ../../gettingstarted/encryption-wireguard.rst:64
#: ../../gettingstarted/hubble.rst:34 ../../gettingstarted/hubble_setup.rst:30
#: ../../gettingstarted/k8s-install-rke.rst:58
#: ../../gettingstarted/k8s-install-validate.rst:6
msgid "Cilium CLI"
msgstr ""

#: ../../gettingstarted/cli-download.rst:1
msgid ""
"Install the latest version of the Cilium CLI. The Cilium CLI can be used to "
"install Cilium, inspect the state of a Cilium installation, and enable/disable "
"various features (e.g. clustermesh, Hubble)."
msgstr ""

#: ../../gettingstarted/cli-download.rst:6
#: ../../gettingstarted/hubble-install.rst:3
#: ../../gettingstarted/k8s-install-kops.rst:41
#: ../../gettingstarted/rancher-desktop-configure.rst:13
msgid "Linux"
msgstr ""

#: ../../gettingstarted/cli-download.rst:15
#: ../../gettingstarted/rancher-desktop-configure.rst:19
msgid "macOS"
msgstr ""

#: ../../gettingstarted/cli-download.rst:24
msgid "Other"
msgstr ""

#: ../../gettingstarted/cli-download.rst:26
msgid ""
"See the full page of `releases <https://github.com/cilium/cilium-cli/releases/"
"latest>`_."
msgstr ""

#: ../../gettingstarted/cli-status.rst:1
msgid "To validate that Cilium has been properly installed, you can run"
msgstr ""

#: ../../gettingstarted/cli-connectivity-test.rst:1
msgid ""
"Run the following command to validate that your cluster has proper network "
"connectivity:"
msgstr ""

#: ../../gettingstarted/cli-connectivity-test.rst:15
msgid ""
"Congratulations! You have a fully functional Kubernetes cluster with Cilium. 🎉"
msgstr ""

#: ../../gettingstarted/k8s-install-validate.rst:12
msgid "Manually"
msgstr ""

#: ../../gettingstarted/kubectl-status.rst:1
msgid "You can monitor as Cilium and all required components are being installed:"
msgstr ""

#: ../../gettingstarted/kubectl-status.rst:12
msgid "It may take a couple of minutes for all components to come up:"
msgstr ""

#: ../../gettingstarted/kubectl-connectivity-test.rst:1
msgid ""
"You can deploy the \"connectivity-check\" to test connectivity between pods. It "
"is recommended to create a separate namespace for this."
msgstr ""

#: ../../gettingstarted/kubectl-connectivity-test.rst:8
msgid "Deploy the check with:"
msgstr ""

#: ../../gettingstarted/kubectl-connectivity-test.rst:14
msgid ""
"It will deploy a series of deployments which will use various connectivity "
"paths to connect to each other. Connectivity paths include with and without "
"service load-balancing and various network policy combinations. The pod name "
"indicates the connectivity variant and the readiness and liveness gate "
"indicates success or failure of the test:"
msgstr ""

#: ../../gettingstarted/kubectl-connectivity-test.rst:41
msgid ""
"If you deploy the connectivity check to a single node cluster, pods that check "
"multi-node functionalities will remain in the ``Pending`` state. This is "
"expected since these pods need at least 2 nodes to be scheduled successfully."
msgstr ""

#: ../../gettingstarted/kubectl-connectivity-test.rst:45
msgid "Once done with the test, remove the ``cilium-test`` namespace:"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:228
#: ../../gettingstarted/hubble_setup.rst:143 ../../gettingstarted/next-steps.rst:2
msgid "Next Steps"
msgstr ""

#: ../../gettingstarted/next-steps.rst:4
msgid ":ref:`hubble_setup`"
msgstr ""

#: ../../gettingstarted/hubble_setup.rst:145 ../../gettingstarted/next-steps.rst:5
msgid ":ref:`hubble_cli`"
msgstr ""

#: ../../gettingstarted/hubble_setup.rst:146 ../../gettingstarted/next-steps.rst:6
msgid ":ref:`hubble_ui`"
msgstr ""

#: ../../gettingstarted/microk8s.rst:38 ../../gettingstarted/next-steps.rst:7
msgid ":ref:`gs_http`"
msgstr ""

#: ../../gettingstarted/next-steps.rst:8
msgid ":ref:`clustermesh`"
msgstr ""

#: ../../gettingstarted/alibabacloud-eni.rst:195
#: ../../gettingstarted/bandwidth-manager.rst:237
#: ../../gettingstarted/clustermesh/clustermesh.rst:259
#: ../../gettingstarted/clustermesh/policy.rst:50
#: ../../gettingstarted/encryption-ipsec.rst:357
#: ../../gettingstarted/encryption-wireguard.rst:214
#: ../../gettingstarted/external-workloads.rst:58
#: ../../gettingstarted/host-services.rst:62
#: ../../gettingstarted/kubeproxy-free.rst:1385
#: ../../gettingstarted/local-redirect-policy.rst:310
#: ../../gettingstarted/vtep.rst:142
msgid "Limitations"
msgstr ""

#: ../../gettingstarted/alibabacloud-eni.rst:197
msgid "The Alibaba ENI integration of Cilium is currently only enabled for IPv4."
msgstr ""

#: ../../gettingstarted/alibabacloud-eni.rst:198
msgid ""
"Only work with instance support ENI, refer to `Instance families <https://www."
"alibabacloud.com/help/doc-detail/25378.htm>`_."
msgstr ""

#: ../../gettingstarted/aws.rst:11
msgid "Locking down external access using AWS metadata"
msgstr ""

#: ../../gettingstarted/aws.rst:13
msgid ""
"This document serves as an introduction to using Cilium to enforce policies "
"based on AWS instances metadata. It is a detailed walk-through of getting a "
"single-node Cilium environment running on your machine. It is designed to take "
"15-30 minutes with some experience running Kubernetes."
msgstr ""

#: ../../gettingstarted/aws.rst:20 ../../gettingstarted/gsg_requirements.rst:8
msgid "Setup Cilium"
msgstr ""

#: ../../gettingstarted/aws.rst:22
msgid ""
"This guide will work with any approach to installing Cilium, including "
"minikube, as long as the cilium-operator pod in the deployment can reach the "
"AWS API server However, since the most common use of this mechanism is for "
"Kubernetes clusters running in AWS, we recommend trying it out along with the "
"guide: :ref:`k8s_install_quick` ."
msgstr ""

#: ../../gettingstarted/aws.rst:28
msgid "Create AWS secrets"
msgstr ""

#: ../../gettingstarted/aws.rst:30
msgid ""
"Before installing Cilium, a new Kubernetes Secret with the AWS Tokens needs to "
"be added to your Kubernetes cluster. This Secret will allow Cilium to gather "
"information from the AWS API which is needed to implement ToGroups policies."
msgstr ""

#: ../../gettingstarted/aws.rst:35
msgid "AWS Access keys and IAM role"
msgstr ""

#: ../../gettingstarted/aws.rst:37
msgid ""
"To create a new access token the `following guide can be used <https://docs.aws."
"amazon.com/cli/latest/userguide/cli-configure-quickstart.html#cli-configure-"
"quickstart-config>`_. These keys need to have certain permissions set:"
msgstr ""

#: ../../gettingstarted/aws.rst:79
msgid ""
"This secret stores the AWS credentials, which will be used to connect the AWS "
"API."
msgstr ""

#: ../../gettingstarted/aws.rst:86
msgid ""
"To validate that the credentials are correct, the following pod can be created "
"for debugging purposes:"
msgstr ""

#: ../../gettingstarted/aws.rst:121
msgid ""
"To list all of the available AWS instances, the following command can be used:"
msgstr ""

#: ../../gettingstarted/aws.rst:127
msgid ""
"Once the secret has been created and validated, the cilium-operator pod must be "
"restarted in order to pick up the credentials in the secret. To do this, "
"identify and delete the existing cilium-operator pod, which will be recreated "
"automatically:"
msgstr ""

#: ../../gettingstarted/aws.rst:142
msgid ""
"It is important for this demo that ``coredns`` is working correctly. To know "
"the status of ``coredns`` you can run the following command:"
msgstr ""

#: ../../gettingstarted/aws.rst:151 ../../gettingstarted/grpc.rst:27
msgid "Where at least one pod should be available."
msgstr ""

#: ../../gettingstarted/aws.rst:154
msgid "Configure AWS Security Groups"
msgstr ""

#: ../../gettingstarted/aws.rst:156
msgid ""
"Cilium's AWS Metadata filtering capability enables explicit whitelisting of "
"communication between a subset of pods (identified by Kubernetes labels) with a "
"set of destination EC2 VMs (identified by membership in an AWS security group)."
msgstr ""

#: ../../gettingstarted/aws.rst:160
msgid ""
"In this example, the destination EC2 VMs are a member of a single AWS security "
"group ('sg-0f2146100a88d03c3') and pods with label class=xwing should only be "
"able to make connections outside the cluster to the destination VMs in that "
"security group."
msgstr ""

#: ../../gettingstarted/aws.rst:165
msgid ""
"To enable this, the VMs acting as Kubernetes worker nodes must be able to send "
"traffic to the destination VMs that are being accessed by pods.  One approach "
"for achieving this is to put all Kubernetes worker VMs in a single 'k8s-worker' "
"security group, and then ensure that any security group that is referenced in a "
"Cilium toGroups policy has an allow all ingress rule (all ports) for "
"connections from the 'k8s-worker' security group.  Cilium filtering will then "
"ensure that the only pods allowed by policy can reach the destination VMs."
msgstr ""

#: ../../gettingstarted/aws.rst:174
msgid "Create a sample policy"
msgstr ""

#: ../../gettingstarted/aws.rst:177
msgid "Deploy a demo application:"
msgstr ""

#: ../../gettingstarted/aws.rst:179
msgid ""
"In this case we're going to use a demo application that is used in other "
"guides. These manifests will create three microservices applications: "
"*deathstar*, *tiefighter*, and *xwing*. In this case, we are only going to use "
"our *xwing* microservice to secure communications to existing AWS instances."
msgstr ""

#: ../../gettingstarted/aws.rst:193
msgid ""
"Kubernetes will deploy the pods and service in the background. Running "
"``kubectl get pods,svc`` will inform you about the progress of the operation.  "
"Each pod will go through several states until it reaches ``Running`` at which "
"point the pod is ready."
msgstr ""

#: ../../gettingstarted/aws.rst:212
msgid "Policy Language:"
msgstr ""

#: ../../gettingstarted/aws.rst:214
msgid ""
"**ToGroups** rules can be used to define policy in relation to cloud providers, "
"like AWS."
msgstr ""

#: ../../gettingstarted/aws.rst:239
msgid ""
"This policy allows traffic from pod *xwing* to any AWS instance that is in the "
"security group with ID ``sg-0f2146100a88d03c3``."
msgstr ""

#: ../../gettingstarted/aws.rst:243
msgid "Validate that derived policy is in place"
msgstr ""

#: ../../gettingstarted/aws.rst:245
msgid ""
"Every time that a new policy with ToGroups rules is added, an equivalent policy "
"(also called \"derivative policy\"), will be created. This policy will contain "
"the set of CIDRs that correspond to the specification in ToGroups, e.g., the "
"IPs of all instances that are part of a specified security group. The list of "
"IPs will be updated periodically."
msgstr ""

#: ../../gettingstarted/aws.rst:258
msgid "Eventually, the derivative policy will contain IPs in the ToCIDR section:"
msgstr ""

#: ../../gettingstarted/aws.rst:321
msgid "The derivative rule should contain the following information:"
msgstr ""

#: ../../gettingstarted/aws.rst:323
msgid ""
"*metadata.OwnerReferences*: that contains the information about the ToGroups "
"policy."
msgstr ""

#: ../../gettingstarted/aws.rst:326
msgid ""
"*specs.Egress.ToCIDRSet*:  the list of private and public IPs of the instances "
"that correspond to the spec of the parent policy."
msgstr ""

#: ../../gettingstarted/aws.rst:329
msgid ""
"*status*: whether or not the policy is enforced yet, and when the policy was "
"last updated."
msgstr ""

#: ../../gettingstarted/aws.rst:332
msgid ""
"The Cilium Endpoint status for the *xwing* should have policy enforcement "
"enabled only for egress connectivity:"
msgstr ""

#: ../../gettingstarted/aws.rst:341
msgid ""
"In this example, *xwing* pod can only connect to ``34.254.113.42/32`` and "
"``172.31.44.160/32`` and connectivity to other IP will be denied."
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:11
msgid "Bandwidth Manager"
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:13
msgid ""
"This guide explains how to configure Cilium's bandwidth manager to optimize TCP "
"and UDP workloads and efficiently rate limit individual Pods if needed through "
"the help of EDT (Earliest Departure Time) and eBPF. Cilium's bandwidth manager "
"is also prerequisite for enabling BBR congestion control for Pods as outlined :"
"ref:`below<BBR Pods>`."
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:19
msgid ""
"The bandwidth manager does not rely on CNI chaining and is natively integrated "
"into Cilium instead. Hence, it does not make use of the `bandwidth CNI <https://"
"kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-"
"plugins/#support-traffic-shaping>`_ plugin. Due to scalability concerns in "
"particular for multi-queue network interfaces, it is not recommended to use the "
"bandwidth CNI plugin which is based on TBF (Token Bucket Filter) instead of EDT."
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:26
msgid ""
"Cilium's bandwidth manager supports the ``kubernetes.io/egress-bandwidth`` Pod "
"annotation which is enforced on egress at the native host networking devices. "
"The bandwidth enforcement is supported for direct routing as well as tunneling "
"mode in Cilium."
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:31
msgid ""
"The ``kubernetes.io/ingress-bandwidth`` annotation is not supported and also "
"not recommended to use. Limiting bandwidth happens natively at the egress point "
"of networking devices in order to reduce or pace bandwidth usage on the wire. "
"Enforcing at ingress would add yet another layer of buffer queueing right in "
"the critical fast-path of a node via ``ifb`` device where ingress traffic first "
"needs to be redirected to the ``ifb``'s egress point in order to perform "
"shaping before traffic can go up the stack. At this point traffic has already "
"occupied the bandwidth usage on the wire, and the node has already spent "
"resources on processing the packet. ``kubernetes.io/ingress-bandwidth`` "
"annotation is ignored by Cilium's bandwidth manager."
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:44
msgid "Bandwidth Manager requires a v5.1.x or more recent Linux kernel."
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:48
msgid ""
"Cilium's bandwidth manager is disabled by default on new installations. To "
"install Cilium with the bandwidth manager enabled, run"
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:57
msgid "To enable the bandwidth manager on an existing installation, run"
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:67
msgid ""
"The native host networking devices are auto detected as native devices which "
"have the default route on the host or have Kubernetes ``InternalIP`` or "
"``ExternalIP`` assigned. ``InternalIP`` is preferred over ``ExternalIP`` if "
"both exist. To change and manually specify the devices, set their names in the "
"``devices`` helm option (e.g. ``devices='{eth0,eth1,eth2}'``). Each listed "
"device has to be named the same on all Cilium-managed nodes."
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:74
msgid "Verify that the Cilium Pods have come up correctly:"
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:83
msgid ""
"In order to verify whether the bandwidth manager feature has been enabled in "
"Cilium, the ``cilium status`` CLI command provides visibility through the "
"``BandwidthManager`` info line. It also dumps a list of devices on which the "
"egress bandwidth limitation is enforced:"
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:93
msgid ""
"To verify that egress bandwidth limits are indeed being enforced, one can "
"deploy two ``netperf`` Pods in different nodes — one acting as a server and one "
"acting as the client:"
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:141
msgid ""
"Once up and running, the ``netperf-client`` Pod can be used to test egress "
"bandwidth enforcement on the ``netperf-server`` Pod. As the test streaming "
"direction is from the ``netperf-server`` Pod towards the client, we need to "
"check ``TCP_MAERTS``:"
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:158
msgid ""
"As can be seen, egress traffic of the ``netperf-server`` Pod has been limited "
"to 10Mbit per second."
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:160
msgid ""
"In order to introspect current endpoint bandwidth settings from BPF side, the "
"following command can be run (replace ``cilium-xxxxx`` with the name of the "
"Cilium Pod that is co-located with the ``netperf-server`` Pod):"
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:170
msgid ""
"Each Pod is represented in Cilium as an :ref:`endpoint` which has an identity. "
"The above identity can then be correlated with the ``cilium endpoint list`` "
"command."
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:176
msgid "BBR for Pods"
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:178
msgid ""
"The base infrastructure around MQ/FQ setup provided by Cilium's bandwidth "
"manager also allows for use of TCP `BBR congestion control <https://queue.acm."
"org/detail.cfm?id=3022184>`_ for Pods."
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:182
msgid ""
"BBR is in particular suitable when Pods are exposed behind Kubernetes Services "
"which face external clients from the Internet. BBR achieves higher bandwidths "
"and lower latencies for Internet traffic, for example, it has been `shown "
"<https://cloud.google.com/blog/products/networking/tcp-bbr-congestion-control-"
"comes-to-gcp-your-internet-just-got-faster>`_ that BBR's throughput can reach "
"as much as 2,700x higher than today's best loss-based congestion control and "
"queueing delays can be 25x lower."
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:190
msgid "BBR for Pods requires a v5.18.x or more recent Linux kernel."
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:192
msgid ""
"To enable the bandwidth manager with BBR congestion control, deploy with the "
"following:"
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:203
msgid ""
"In order for BBR to work reliably for Pods, it requires a 5.18 or higher "
"kernel. As outlined in our `Linux Plumbers 2021 talk <https://lpc.events/"
"event/11/contributions/953/>`_, this is needed since older kernels do not "
"retain timestamps of network packets when switching from Pod to host network "
"namespace. Due to the latter, the kernel's pacing infrastructure does not "
"function properly in general (not specific to Cilium)."
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:209
msgid ""
"We helped with fixing this issue for recent kernels to retain timestamps and "
"therefore to get BBR for Pods working. Prior to that kernel, BBR was only "
"working for sockets which are in the initial network namespace (hostns). BBR "
"also needs eBPF Host-Routing in order to retain the network packet's socket "
"association all the way until the packet hits the FQ queueing discipline on the "
"physical device in the host namespace. (Without eBPF Host-Routing the packet's "
"socket association would otherwise be orphaned inside the host stacks "
"forwarding/routing layer.)"
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:217
msgid ""
"In order to verify whether the bandwidth manager with BBR has been enabled in "
"Cilium, the ``cilium status`` CLI command provides visibility again through the "
"``BandwidthManager`` info line:"
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:226
msgid ""
"Once this setting is enabled, it will use BBR as a default for all newly "
"spawned Pods. Ideally, BBR is selected upon initial Cilium installation when "
"the cluster is created such that all nodes and Pods in the cluster "
"homogeneously use BBR as otherwise there could be `potential unfairness issues "
"<https://blog.apnic.net/2020/01/10/when-to-use-and-not-use-bbr/>`_ for other "
"connections still using CUBIC. Also note that due to the nature of BBR's "
"probing you might observe a higher rate of TCP retransmissions compared to "
"CUBIC."
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:233
msgid ""
"We recommend to use BBR in particular for clusters where Pods are exposed as "
"Services which serve external clients connecting from the Internet."
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:239
msgid ""
"Bandwidth enforcement currently does not work in combination with L7 Cilium "
"Network Policies. In case they select the Pod at egress, then the bandwidth "
"enforcement will be disabled for those Pods."
msgstr ""

#: ../../gettingstarted/bandwidth-manager.rst:242
msgid ""
"Bandwidth enforcement doesn't work with nested network namespace environments "
"like Kind. This is because they typically don't have access to the global "
"sysctl under ``/proc/sys/net/core`` and the bandwidth enforcement depends on "
"them."
msgstr ""

#: ../../gettingstarted/bgp.rst:11
msgid "BGP (beta)"
msgstr ""

#: ../../gettingstarted/bgp.rst:13
msgid ""
"BGP provides a way to advertise routes using traditional networking protocols "
"to allow Cilium-managed services to be accessible outside the cluster."
msgstr ""

#: ../../gettingstarted/bgp.rst:16
msgid ""
"This document explains how to configure Cilium's native support for announcing "
"``LoadBalancer`` IPs of ``Services`` and a Kubernetes node's Pod CIDR range via "
"BGP. It leverages `MetalLB's <https://metallb.universe.tf/>`_ simple and "
"effective implementation of IP allocation and the minimal BGP protocol support "
"to do this. The configuration for Cilium is the same as MetalLB's configuration."
msgstr ""

#: ../../gettingstarted/bgp.rst:22
msgid ""
"Specifically, if a ``Service`` of type ``LoadBalancer`` is created, Cilium will "
"allocate an IP for it from a specified pool. Once the IP is allocated, the "
"Agents will announce via BGP depending on the ``Service``'s "
"``ExternalTrafficPolicy``. See MetalLB's `documentation <https://metallb."
"universe.tf/usage/#bgp>`_ on this specific topic."
msgstr ""

#: ../../gettingstarted/bgp.rst:35
msgid ""
"BGP support is enabled by providing the BGP configuration via a ConfigMap and "
"by setting a few Helm values. Otherwise, BGP is disabled by default."
msgstr ""

#: ../../gettingstarted/bgp.rst:38
msgid "Here's an example ConfigMap:"
msgstr ""

#: ../../gettingstarted/bgp.rst:59
msgid "Here are the required Helm values:"
msgstr ""

#: ../../gettingstarted/bgp.rst:69
msgid ""
"At least one ``bgp.announce.*`` value is required if ``bgp.enabled=true`` is "
"set."
msgstr ""

#: ../../gettingstarted/bgp.rst:71
msgid "Verify that Cilium Agent pod is running."
msgstr ""

#: ../../gettingstarted/bgp.rst:80
msgid "Create LoadBalancer and backend pods"
msgstr ""

#: ../../gettingstarted/bgp.rst:82
msgid ""
"Apply the following ``LoadBalancer`` ``Service`` and its corresponding backends:"
msgstr ""

#: ../../gettingstarted/bgp.rst:125
msgid "Observe that the Operator allocates an external IP for ``test-lb``:"
msgstr ""

#: ../../gettingstarted/bgp.rst:134
msgid "Verify that the backend is running:"
msgstr ""

#: ../../gettingstarted/bgp.rst:142
msgid "Validate BGP announcements"
msgstr ""

#: ../../gettingstarted/bgp.rst:144
msgid ""
"To see whether Cilium is announcing the external IP of the ``Service`` or the "
"Pod CIDR range of your Kubernetes nodes, check the node's routing table that's "
"running your BGP router."
msgstr ""

#: ../../gettingstarted/bgp.rst:147
msgid ""
"Alternatively, you can run ``tcpdump`` inside the Cilium pod (it'll need to be "
"``apt install``'d) to see BGP messages like so:"
msgstr ""

#: ../../gettingstarted/bgp.rst:158
msgid "Verify that traffic to the external IP is directed to the backends:"
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:2
msgid "Cilium BGP Control Plane"
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:5
msgid "Usage"
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:7
msgid ""
"Currently a single flag in the ``Cilium Agent`` exists to turn on the ``BGP "
"Control Plane`` feature set."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:14
msgid ""
"When set to ``true`` the ``BGP Control Plane`` ``Controllers`` will be "
"instantiated and will begin listening for ``CiliumBGPPeeringPolicy`` events."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:19
msgid "CiliumBGPPeeringPolicy CRD"
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:21
msgid ""
"All ``BGP`` peering topology information is carried in a "
"``CiliumBGPPeeringPolicy`` CRD."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:24
msgid ""
"``CiliumBGPPeeringPolicy`` can be applied to one or more nodes based on its "
"``nodeSelector`` fields."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:27
msgid ""
"A Cilium node may only have a single ``CiliumBGPPeeringPolicy`` apply to it and "
"if more than one does, it will apply no policy at all."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:30
msgid ""
"Each ``CiliumBGPPeeringPolicy`` defines one or more ``CiliumBGPVirtualRouter`` "
"configurations."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:33
msgid ""
"When these CRDs are written or read from the cluster the ``Controllers`` will "
"take notice and perform the necessary actions to drive the ``BGP Control "
"Plane`` to the desired state described by the policy."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:37
msgid "The policy in ``yaml`` form is defined below:"
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:57
msgid "Fields"
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:75
msgid ""
"Setting unique configuration details of a particular instantiated virtual "
"router on a particular Cilium node is explained in `Virtual Router Attributes "
"<#Virtual%20Router%20Attributes>`__"
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:80
msgid "Creating a BGP Topology"
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:83
msgid "Rules"
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:85
msgid ""
"Follow the rules below to have a ``CiliumBGPPeeringPolicy`` correctly apply to "
"a node."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:88
msgid "Only a single ``CiliumBGPPeeringPolicy`` can apply to a ``Cilium`` node."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:91
msgid ""
"If the ``BGP Control Plane`` on a node iterates through the "
"``CiliumBGPPeeringPolicy`` CRs currently written to the cluster and discovers "
"(n > 1) policies match its labels, it will return an error and remove any "
"existing BGP sessions. Only (n == 1) policies **must** match a node's label "
"sets."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:96
msgid ""
"Administrators should test a new BGP topology in a staging environment before "
"making permanent changes in production."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:99
msgid ""
"Within a ``CiliumBGPPeeringPolicy`` each ``CiliumBGPVirtualRouter`` defined "
"must have a unique ``localASN`` field."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:102
msgid ""
"A node cannot host two or more logical routers with the same local ASN. Local "
"ASNs are used as unique keys for a logical router."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:104
msgid ""
"A node can define the remote ASN on a per-neighbor basis to mitigate this "
"scenario. See ``CiliumBGPNeighbor`` CR sub-structure."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:108
msgid ""
"IPv6 single stack deployments **must** set an IPv4 encoded ``routerID`` field "
"in each defined ``CiliumBGPVirtualRouter`` object within a "
"``CiliumBGPPeeringPolicy``"
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:112
msgid ""
"Cilium running on a IPv6 single stack cluster cannot reliably generate a unique "
"32 bit BGP router ID, as it defines no unique IPv4 addresses for the node. The "
"administrator must define these IDs manually or an error applying the policy "
"will occur."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:116
msgid ""
"This is explained further in `Virtual Router Attributes <#Virtual%20Router"
"%20Attributes>`__"
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:120
msgid "Defining Topology"
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:122
msgid ""
"Within a ``CiliumBGPPeeringPolicy`` multiple ``CiliumBGPVirtualRouter``\\ (s) "
"can be defined."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:125
msgid "Each one can be thought of as a logical BGP router instance."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:127
msgid ""
"Defining more than one ``CiliumBGPVirtualRouter`` in a "
"``CiliumBGPVirtualRouter`` creates more than one logical BGP router on the "
"hosts which the policy matches."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:131
msgid ""
"It is possible to create a single ``CiliumBGPPeeringPolicy`` for all nodes by "
"giving each node in a cluster the same label and defining a single "
"``CiliumBGPPeeringPolicy`` which applies to this label."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:135
msgid ""
"It is also possible to provide each ``Kubernetes`` node its own "
"``CiliumBGPPeeringPolicy`` by giving each node a unique label and creating a "
"``CiliumBGPPeeringPolicy`` for each unique label."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:139
msgid ""
"This allows for selecting subsets of nodes which peer to a particular BGP "
"router while another subset of nodes peer to a separate BGP router, akin to an "
"\"AS-per-rack\" topology."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:144
msgid "Virtual Router Attributes"
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:146
msgid "A ``CiliumBGPPeeringPolicy`` can apply to multiple nodes."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:148
msgid ""
"When a ``CiliumBGPPeeringPolicy`` applies to one or more nodes each node will "
"instantiate one or more BGP routers as defined by the list of "
"``CiliumBGPVirutalRouter``."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:152
msgid ""
"However, there are times where fine-grained control over an instantiated "
"virtual router's configuration needs to take place."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:155
msgid ""
"To accomplish this a Kubernetes annotation is defined which applies to "
"Kubernetes Node resources."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:158
msgid ""
"A single annotation is used to specify a set of configuration attributes to "
"apply to a particular virtual router instantiated on a particular host."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:162
msgid "The syntax of the annotation is as follows:"
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:168
msgid ""
"The ``{asn}`` portion should be replaced by the virtual router's local ASN you "
"wish to apply these configuration attributes to."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:171
msgid "The following sections outline the currently supported attributes."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:175
msgid ""
"Each following section describes the syntax of applying a single attribute, "
"however the annotation's value supports a comma separated lists of attributes "
"and applying multiple attributes in a single annotation is supported."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:182
msgid ""
"When duplicate ``key=value`` attributes are defined the last one will be "
"selected."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:186
msgid "Router ID Attribute"
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:188
msgid ""
"When Cilium is running on an ``IPv4`` or a dual-stack ``IPv4/6`` cluster the "
"``BGP Control Plane`` will utilize the ``IPv4`` addressed used by Cilium for "
"external reach ability."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:192
msgid ""
"This will typically be Kubernetes' reported external IP address but can also be "
"configured with a Cilium agent flag."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:195
msgid ""
"When running in ``IPv6`` single stack or when the administrator needs to "
"manually define the instantiated BGP server's router ID a Kubernetes annotation "
"can be placed on the node."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:199
msgid "The annotation takes the following syntax:"
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:205
msgid ""
"The above annotation syntax should replace ``{asn}`` with the local ASN of the "
"``CiliumBGPVirtualRouter`` you are setting the provided router ID for."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:209
msgid ""
"When the ``BGPControlPlane`` evaluates a ``CiliumBGPPeeringPolicy`` with a "
"``CiliumBGPVirtualRouter`` it also searches for an annotation which targets the "
"aforementioned ``CiliumBGPVirtualRouter`` local ASN."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:213
msgid ""
"If found it will use the provided router ID and not attempt to use the IPv4 "
"address assigned to the node."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:217
msgid "Local Listening Port"
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:219
msgid ""
"By default the ``GoBGP BGPRouterManager`` will instantiate each virtual router "
"without a listening port."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:222
msgid ""
"It is possible to deploy a virtual router which creates a local listening port "
"where BGP connections may take place."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:225
msgid "If this is desired the following annotation can be provided"
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:232
msgid "Architecture"
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:234
msgid ""
"The ``BGP Control Plane`` is split into a ``Agent-Side Control Plane`` and a "
"``Operator-Side`` control plane (not yet implemented)."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:237
msgid ""
"Both control planes are implemented by a ``Controller`` which follows the "
"``Kubernetes`` controller pattern."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:240
msgid ""
"Both control planes primary listen for ``CiliumBGPPeeringPolicy`` CRDs, long "
"with other Cilium and Kubernetes resources useful for implementing a BGP "
"control plane."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:245
msgid "Agent-Side Architecture"
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:248
msgid "Controller"
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:250
msgid ""
"The ``Agent-Side Control Plane`` implements a controller located in ``pkg/bgpv1/"
"agent/controller.go``."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:253
msgid ""
"The controller listens for ``CiliumBGPPeeringPolicy``, determines if a policy "
"applies to its current host and if it does, captures some information about "
"Cilium's current state then calls down to the implemented ``BGPRouterManager``."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:259
msgid "BGPRouterManager"
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:261
msgid ""
"The ``BGPRouterManager`` is an interface used to define a declarative API "
"between the ``Controller`` and instantiated BGP routers."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:264
msgid ""
"The interface defines a single declarative method whose argument is the desired "
"``CiliumBGPPeeringPolicy`` (among a few others)."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:267
msgid ""
"The ``BGPRouterManager`` is in charge of pushing the ``BGP Control Plane`` to "
"the desired ``CiliumBGPPeeringPolicy`` or returning an error if it is not "
"possible."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:272
msgid "GoBGP Implementation"
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:274
msgid ""
"The first implementation of ``BGPRouterManager`` utilizes the ``gobgp`` package."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:277
msgid "You can find this implementation in ``pkg/bgpv1/gobgp``."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:279
msgid ""
"This implementation will - evaluate the desired ``CiliumBGPPeeringPolicy`` - "
"create/remove the desired BGP routers - advertise/withdraw the desired BGP "
"routes - enable/disable any BGP server specific features - inform the caller if "
"the policy cannot be applied"
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:285
msgid ""
"The GoBGP implementation is capable of evaluating each "
"``CiliumBGPVirtualRouter`` in isolation."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:288
msgid ""
"This means when applying a ``CiliumBGPPeeringPolicy`` the GoBGP "
"``BGPRouterManager`` will attempt to create each ``CiliumBGPVirtualRouter``."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:292
msgid ""
"If a particular ``CiliumBGPVirtualRouter`` fails to instantiate the error is "
"logged and the ``BGPRouterManager`` will continue to the next "
"``CiliumBGPVirtualRouter``, utilizing the aforementioned logic."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:296
msgid "GoBGP BGPRouterManager Architecture"
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:299
msgid ""
"It's worth expanding on how the ``gobgp`` implementation of the "
"``BGPRouterManager`` works internally."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:302
msgid ""
"This ``BGPRouterManager`` views each ``CiliumBGPVirtualRouter`` as a BGP router "
"instance."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:305
msgid ""
"Each ``CiliumBGPVirtualRouter`` defines a local ASN, a router ID and a list of "
"``CiliumBGPNeighbors`` to peer with."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:308
msgid ""
"This is enough for the ``BGPRouterManager`` to create a ``BgpServer`` instance, "
"which is the nomenclature defining a BGP speaker in ``gobgp``-package-parlance."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:312
msgid ""
"This ``BGPRouterManager`` groups ``BgpServer`` instances by their local ASNs."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:315
msgid ""
"This leads to the following rule: - A ``CiliumBGPPeeringPolicy`` applying to "
"node ``A`` must not have two or more ``CiliumBGPVirtualRouters`` with the same "
"``localASN`` fields."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:319
msgid ""
"The ``gobgp`` ``BGPRouterManager`` employs a set of ``ConfigReconcilerFunc``\\ "
"(s) which perform the order-dependent reconciliation actions for each "
"``BgpServer`` it must reconcile."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:323
msgid "A ``ConfigReconcilerFunc`` is simply a function with a typed signature."
msgstr ""

#: ../../gettingstarted/bgp-control-plane.rst:329
msgid ""
"See the source code at ``pkg/bgpv1/gobgp/reconcile.go`` for a more in depth "
"explanation of how each ``ConfigReconcilerFunc`` is called."
msgstr ""

#: ../../gettingstarted/bird.rst:10
msgid "Using BIRD to run BGP"
msgstr ""

#: ../../gettingstarted/bird.rst:12
msgid ""
"`BIRD is an open-source implementation for routing Internet Protocol packets on "
"Unix-like operating systems <https://en.wikipedia.org/wiki/"
"Bird_Internet_routing_daemon>`_. If you are not familiar with it, you had best "
"have a glance at the `User's Guide`_ first."
msgstr ""

#: ../../gettingstarted/bird.rst:18
msgid ""
"BIRD provides a way to advertise routes using traditional networking protocols "
"to allow Cilium-managed endpoints to be accessible outside the cluster. This "
"guide assumes that Cilium is already deployed in the cluster, and that the "
"remaining piece is how to ensure that the pod CIDR ranges are externally "
"routable."
msgstr ""

#: ../../gettingstarted/bird.rst:24
msgid ""
"`BIRD <https://bird.network.cz>`_ maintains two release families at present: "
"``1.x`` and ``2.x``, and the configuration format varies a lot between them. "
"Unless you have already deployed the ``1.x``, we suggest using ``2.x`` "
"directly, as the ``2.x`` will live longer. The following examples will denote "
"``bird`` as the ``bird2`` software and use configuration in the format that "
"``bird2`` understands."
msgstr ""

#: ../../gettingstarted/bird.rst:31
msgid ""
"This guide shows how to install and configure bird on CentOS 7.x to make it "
"collaborate with Cilium. Installation and configuration on other platforms "
"should be very similar."
msgstr ""

#: ../../gettingstarted/bird.rst:36
msgid "Install bird"
msgstr ""

#: ../../gettingstarted/bird.rst:46
msgid "Test the installation:"
msgstr ""

#: ../../gettingstarted/bird.rst:70
msgid "Basic configuration"
msgstr ""

#: ../../gettingstarted/bird.rst:72
msgid ""
"It's hard to discuss bird configurations without considering specific BGP "
"schemes. However, BGP scheme design is beyond the scope of this guide. If you "
"are interested in this topic, refer to `BGP in the Data Center <https://www."
"oreilly.com/library/view/bgp-in-the/9781491983416/>`_ (O'Reilly, 2017) for a "
"quick start."
msgstr ""

#: ../../gettingstarted/bird.rst:78
msgid "In the following, we will restrict our BGP scenario as follows:"
msgstr ""

#: ../../gettingstarted/bird.rst:83
msgid "physical network: simple 3-tier hierarchical architecture"
msgstr ""

#: ../../gettingstarted/bird.rst:84
msgid "nodes connect to physical network via layer 2 switches"
msgstr ""

#: ../../gettingstarted/bird.rst:85
msgid "announcing each node's PodCIDR to physical network via ``bird``"
msgstr ""

#: ../../gettingstarted/bird.rst:86
msgid "for each node, do not import route announcements from physical network"
msgstr ""

#: ../../gettingstarted/bird.rst:88
msgid "In this design, the BGP connections look like this:"
msgstr ""

#: ../../gettingstarted/bird.rst:93
msgid "This scheme is simple in that:"
msgstr ""

#: ../../gettingstarted/bird.rst:95
msgid ""
"core routers learn PodCIDRs from ``bird``, which makes the Pod IP addresses "
"routable within the entire network."
msgstr ""

#: ../../gettingstarted/bird.rst:97
msgid ""
"``bird`` doesn't learn routes from core routers and other nodes, which keeps "
"the kernel routing table of each node clean and small, and suffering no "
"performance issues."
msgstr ""

#: ../../gettingstarted/bird.rst:101
msgid ""
"In this scheme, each node just sends pod egress traffic to node's default "
"gateway (the core routers), and lets the latter do the routing."
msgstr ""

#: ../../gettingstarted/bird.rst:104
msgid "Below is the a reference configuration for fulfilling the above purposes:"
msgstr ""

#: ../../gettingstarted/bird.rst:162
msgid ""
"Save the above file as ``/etc/bird.conf``, and replace the placeholders with "
"your own:"
msgstr ""

#: ../../gettingstarted/bird.rst:176
msgid "Restart ``bird`` and check the logs:"
msgstr ""

#: ../../gettingstarted/bird.rst:189
msgid "Verify the changes, you should get something like this:"
msgstr ""

#: ../../gettingstarted/bird.rst:199
msgid ""
"This indicates that the PodCIDR ``10.5.48.0/24`` on this node has been "
"successfully imported into BIRD."
msgstr ""

#: ../../gettingstarted/bird.rst:215
msgid ""
"Here we see that the uplink0 BGP session is established and our PodCIDR from "
"above has been exported and accepted by the BGP peer."
msgstr ""

#: ../../gettingstarted/bird.rst:219
msgid "Monitoring"
msgstr ""

#: ../../gettingstarted/bird.rst:221
msgid ""
"`bird_exporter <https://github.com/czerwonk/bird_exporter>`_ could collect bird "
"daemon states, and export Prometheus-style metrics."
msgstr ""

#: ../../gettingstarted/bird.rst:224
msgid ""
"It also provides a simple Grafana dashboard, but you could also create your "
"own, e.g. `Trip.com's <https://ctripcloud.github.io/cilium/network/2020/01/19/"
"trip-first-step-towards-cloud-native-networking.html>`_ looks like this:"
msgstr ""

#: ../../gettingstarted/bird.rst:230
msgid "Advanced Configurations"
msgstr ""

#: ../../gettingstarted/bird.rst:232
msgid ""
"You may need some advanced configurations to make your BGP scheme production-"
"ready. This section lists some of these parameters, but we will not dive into "
"details, that's BIRD `User's Guide`_'s responsibility."
msgstr ""

#: ../../gettingstarted/bird.rst:237
msgid "BFD"
msgstr ""

#: ../../gettingstarted/bird.rst:239
msgid ""
"`Bidirectional Forwarding Detection (BFD) <https://www.cisco.com/c/en/us/td/"
"docs/ios-xml/ios/iproute_bgp/configuration/xe-16/irg-xe-16-book/bgp-support-for-"
"bfd.html>`_ is a detection protocol designed to accelerate path failure "
"detection."
msgstr ""

#: ../../gettingstarted/bird.rst:243 ../../gettingstarted/bird.rst:284
#: ../../gettingstarted/bird.rst:319
msgid "**This feature also relies on peer side's configuration.**"
msgstr ""

#: ../../gettingstarted/bird.rst:266
msgid "Verify, you should see something like this:"
msgstr ""

#: ../../gettingstarted/bird.rst:278
msgid "ECMP"
msgstr ""

#: ../../gettingstarted/bird.rst:280
msgid ""
"For some special purposes (e.g. L4LB), you may configure a same CIDR on "
"multiple nodes. In this case, you need to configure `Equal-Cost Multi-Path "
"(ECMP) routing <https://en.wikipedia.org/wiki/Equal-cost_multi-path_routing>`_."
msgstr ""

#: ../../gettingstarted/bird.rst:298
msgid "See the user manual for more detailed information."
msgstr ""

#: ../../gettingstarted/bird.rst:300
msgid ""
"You need to check the ECMP correctness on physical network (Core router in the "
"above scenario):"
msgstr ""

#: ../../gettingstarted/bird.rst:317
msgid "Graceful restart"
msgstr ""

#: ../../gettingstarted/bird.rst:321
msgid "Add ``graceful restart`` to each ``bgp`` section:"
msgstr ""

#: ../../gettingstarted/cassandra.rst:11
msgid "How to Secure a Cassandra Database"
msgstr ""

#: ../../gettingstarted/cassandra.rst:13
msgid ""
"This document serves as an introduction to using Cilium to enforce Cassandra-"
"aware security policies.  It is a detailed walk-through of getting a single-"
"node Cilium environment running on your machine. It is designed to take 15-30 "
"minutes."
msgstr ""

#: ../../gettingstarted/cassandra.rst:18
msgid ""
"**NOTE:** Cassandra-aware policy support is still in beta phase.  It is not yet "
"ready for production use.   Additionally, the Cassandra-specific policy "
"language is highly likely to change in a future Cilium version."
msgstr ""

#: ../../gettingstarted/gsg_requirements.rst:1
msgid ""
"If you haven't read the :ref:`intro` yet, we'd encourage you to do that first."
msgstr ""

#: ../../gettingstarted/gsg_requirements.rst:3
#: ../../gettingstarted/hubble_cli.rst:16 ../../gettingstarted/index.rst:114
msgid ""
"The best way to get help if you get stuck is to ask a question on the `Cilium "
"Slack channel <https://cilium.herokuapp.com>`_.  With Cilium contributors "
"across the globe, there is almost always someone available to help."
msgstr ""

#: ../../gettingstarted/gsg_requirements.rst:10
msgid ""
"If you have not set up Cilium yet, follow the guide :ref:`k8s_install_standard` "
"for instructions on how to quickly bootstrap a Kubernetes cluster and install "
"Cilium. If in doubt, pick the minikube route, you will be good to go in less "
"than 5 minutes."
msgstr ""

#: ../../gettingstarted/cassandra.rst:25 ../../gettingstarted/dns.rst:19
#: ../../gettingstarted/elasticsearch.rst:19 ../../gettingstarted/grpc.rst:30
#: ../../gettingstarted/gsg_sw_demo.rst:2 ../../gettingstarted/kafka.rst:21
#: ../../gettingstarted/tls-visibility.rst:48
msgid "Deploy the Demo Application"
msgstr ""

#: ../../gettingstarted/cassandra.rst:27
msgid ""
"Now that we have Cilium deployed and ``kube-dns`` operating correctly we can "
"deploy our demo Cassandra application.  Since our first `HTTP-aware Cilium  "
"Star Wars demo <https://cilium.io/blog/2017/5/4/demo-may-the-force-be-with-you/"
">`_ showed how the Galactic Empire used HTTP-aware security policies to protect "
"the Death Star from the Rebel Alliance, this Cassandra demo is Star Wars-themed "
"as well."
msgstr ""

#: ../../gettingstarted/cassandra.rst:33
msgid ""
"`Apache Cassanadra <http://cassandra.apache.org>`_ is a popular NOSQL database "
"focused on delivering high-performance transactions (especially on writes) "
"without sacrificing on availability or scale. Cassandra operates as a cluster "
"of servers, and Cassandra clients query these services via a the `native "
"Cassandra protocol <https://github.com/apache/cassandra/blob/trunk/doc/"
"native_protocol_v4.spec>`_ . Cilium understands the Cassandra protocol, and "
"thus is able to provide deep visibility and control over which clients are able "
"to access particular tables inside a Cassandra cluster, and which actions (e."
"g., \"select\", \"insert\", \"update\", \"delete\") can be performed on tables."
msgstr ""

#: ../../gettingstarted/cassandra.rst:41
msgid ""
"With Cassandra, each table belongs to a \"keyspace\", allowing multiple groups "
"to use a single cluster without conflicting. Cassandra queries specify the full "
"table name qualified by the keyspace using the syntax \"<keyspace>.<table>\"."
msgstr ""

#: ../../gettingstarted/cassandra.rst:44
msgid ""
"In our simple example, the Empire uses a Cassandra cluster to store two "
"different types of information:"
msgstr ""

#: ../../gettingstarted/cassandra.rst:46
msgid ""
"**Employee Attendance Records** : Use to store daily attendance data "
"(attendance.daily_records)."
msgstr ""

#: ../../gettingstarted/cassandra.rst:47
msgid ""
"**Deathstar Scrum Reports** : Daily scrum reports from the teams working on the "
"Deathstar (deathstar.scrum_reports)."
msgstr ""

#: ../../gettingstarted/cassandra.rst:49 ../../gettingstarted/kafka.rst:43
msgid ""
"To keep the setup small, we will just launch a small number of pods to "
"represent this setup:"
msgstr ""

#: ../../gettingstarted/cassandra.rst:51
msgid ""
"**cass-server** : A single pod running the Cassandra service, representing a "
"Cassandra cluster (label app=cass-server)."
msgstr ""

#: ../../gettingstarted/cassandra.rst:53
msgid ""
"**empire-hq** : A pod representing the Empire's Headquarters, which is the only "
"pod that should be able to read all attendance data, or read/write the "
"Deathstar scrum notes (label app=empire-hq)."
msgstr ""

#: ../../gettingstarted/cassandra.rst:55
msgid ""
"**empire-outpost** : A random outpost in the empire.  It should be able to "
"insert employee attendance records, but not read records for other empire "
"facilities.   It also should not have any access to the deathstar keyspace "
"(label app=empire-outpost)."
msgstr ""

#: ../../gettingstarted/cassandra.rst:59
msgid ""
"All pods other than *cass-server* are Cassandra clients, which need access to "
"the *cass-server* container on TCP port 9042 in order to send Cassandra "
"protocol messages."
msgstr ""

#: ../../gettingstarted/cassandra.rst:64
msgid ""
"The file ``cass-sw-app.yaml`` contains a Kubernetes Deployment for each of the "
"pods described above, as well as a Kubernetes Service *cassandra-svc* for the "
"Cassandra cluster."
msgstr ""

#: ../../gettingstarted/cassandra.rst:75 ../../gettingstarted/grpc.rst:105
#: ../../gettingstarted/memcached.rst:70
msgid ""
"Kubernetes will deploy the pods and service in the background. Running "
"``kubectl get svc,pods`` will inform you about the progress of the operation. "
"Each pod will go through several states until it reaches ``Running`` at which "
"point the setup is ready."
msgstr ""

#: ../../gettingstarted/cassandra.rst:94
msgid "Step 3: Test Basic Cassandra Access"
msgstr ""

#: ../../gettingstarted/cassandra.rst:96
msgid ""
"First, we'll create the keyspaces and tables mentioned above, and populate them "
"with some initial data:"
msgstr ""

#: ../../gettingstarted/cassandra.rst:102
msgid ""
"Next, create two environment variables that refer to the *empire-hq* and "
"*empire-outpost* pods:"
msgstr ""

#: ../../gettingstarted/cassandra.rst:110
msgid ""
"Now we will run the 'cqlsh' Cassandra client in the *empire-outpost* pod, "
"telling it to access the Cassandra cluster identified by the 'cassandra-svc' "
"DNS name:"
msgstr ""

#: ../../gettingstarted/cassandra.rst:121
msgid ""
"Next, using the cqlsh prompt, we'll show that the outpost can add records to "
"the \"daily_records\" table in the \"attendance\" keyspace:"
msgstr ""

#: ../../gettingstarted/cassandra.rst:128
msgid ""
"We have confirmed that outposts are able to report daily attendance records as "
"intended. We're off to a good start!"
msgstr ""

#: ../../gettingstarted/cassandra.rst:131
msgid "The Danger of a Compromised Cassandra Client"
msgstr ""

#: ../../gettingstarted/cassandra.rst:133
msgid ""
"But what if a rebel spy gains access to any of the remote outposts that act as "
"a Cassandra client? Since every client has access to the Cassandra API on port "
"9042, it can do some bad stuff. For starters, the outpost container can not "
"only add entries to the attendance.daily_reports table, but it could read all "
"entries as well."
msgstr ""

#: ../../gettingstarted/cassandra.rst:138
msgid "To see this, we can run the following command:"
msgstr ""

#: ../../gettingstarted/cassandra.rst:161
msgid ""
"Uh oh!  The rebels now has strategic information about empire troop strengths "
"at each location in the galaxy."
msgstr ""

#: ../../gettingstarted/cassandra.rst:163
msgid ""
"But even more nasty from a security perspective is that the outpost container "
"can also access information in any keyspace, including the deathstar keyspace.  "
"For example, run:"
msgstr ""

#: ../../gettingstarted/cassandra.rst:178
msgid ""
"We see that any outpost can actually access the deathstar scrum notes, which "
"mentions a pretty serious issue with the exhaust port."
msgstr ""

#: ../../gettingstarted/cassandra.rst:181
msgid "Securing Access to Cassandra with Cilium"
msgstr ""

#: ../../gettingstarted/cassandra.rst:183
msgid ""
"Obviously, it would be much more secure to limit each pod's access to the "
"Cassandra server to be least privilege (i.e., only what is needed for the app "
"to operate correctly and nothing more)."
msgstr ""

#: ../../gettingstarted/cassandra.rst:186
msgid ""
"We can do that with the following Cilium security policy.   As with Cilium HTTP "
"policies, we can write policies that identify pods by labels, and then limit "
"the traffic in/out of this pod.  In this case, we'll create a policy that "
"identifies the tables that each client should be able to access, the actions "
"that are allowed on those tables, and deny the rest."
msgstr ""

#: ../../gettingstarted/cassandra.rst:191
msgid ""
"As an example, a policy could limit containers with label *app=empire-outpost* "
"to only be able to insert entries into the table \"attendance.daily_reports\", "
"but would block any attempt by a compromised outpost to read all attendance "
"information or access other keyspaces."
msgstr ""

#: ../../gettingstarted/cassandra.rst:197
msgid ""
"Here is the *CiliumNetworkPolicy* rule that limits access of pods with label "
"*app=empire-outpost* to only insert records into \"attendance.daily_reports\":"
msgstr ""

#: ../../gettingstarted/cassandra.rst:202 ../../gettingstarted/kafka.rst:227
msgid ""
"A *CiliumNetworkPolicy* contains a list of rules that define allowed requests, "
"meaning that requests that do not match any rules are denied as invalid."
msgstr ""

#: ../../gettingstarted/cassandra.rst:205
msgid ""
"The rule explicitly matches Cassandra connections destined to TCP 9042 on cass-"
"server pods, and allows query actions like select/insert/update/delete only on "
"a specified set of tables. The above rule applies to inbound (i.e., \"ingress"
"\") connections to cass-server pods (as indicated by \"app:cass-server\" in the "
"\"endpointSelector\" section).  The rule applies different rules based on "
"whether the client pod has labels \"app: empire-outpost\" or \"app: empire-hq\" "
"as indicated by the \"fromEndpoints\" section."
msgstr ""

#: ../../gettingstarted/cassandra.rst:211
msgid ""
"The policy limits the *empire-outpost* pod to performing \"select\" queries on "
"the \"system\" and \"system_schema\" keyspaces (required by cqlsh on startup) "
"and \"insert\" queries to the \"attendance.daily_records\" table."
msgstr ""

#: ../../gettingstarted/cassandra.rst:214
msgid ""
"The full policy adds another rule that allows all queries from the *empire-hq* "
"pod."
msgstr ""

#: ../../gettingstarted/cassandra.rst:216
msgid ""
"Apply this Cassandra-aware network security policy using ``kubectl`` in a new "
"window:"
msgstr ""

#: ../../gettingstarted/cassandra.rst:222
msgid ""
"If we then again try to perform the attacks from the *empire-outpost* pod, "
"we'll see that they are denied:"
msgstr ""

#: ../../gettingstarted/cassandra.rst:229
msgid ""
"This is because the policy only permits pods with labels app: empire-outpost to "
"insert into attendance.daily_records, it does not permit select on that table, "
"or any action on other tables (with the exception of the system.* and "
"system_schema.* keyspaces).  Its worth noting that we don't simply drop the "
"message (which could easily be confused with a network error), but rather we "
"respond with the Cassandra Unauthorized error message. (similar to how HTTP "
"would return an error code of 403 unauthorized)."
msgstr ""

#: ../../gettingstarted/cassandra.rst:235
msgid ""
"Likewise, if the outpost pod ever tries to access a table in another keyspace, "
"like deathstar, this request will also be denied:"
msgstr ""

#: ../../gettingstarted/cassandra.rst:243
msgid "This is blocked as well, thanks to the Cilium network policy."
msgstr ""

#: ../../gettingstarted/cassandra.rst:245
msgid ""
"Use another window to confirm that the *empire-hq* pod still has full access to "
"the cassandra cluster:"
msgstr ""

#: ../../gettingstarted/cassandra.rst:255
msgid ""
"The power of Cilium's identity-based security allows *empire-hq* to still have "
"full access to both tables:"
msgstr ""

#: ../../gettingstarted/cassandra.rst:271
msgid "Similarly, the deathstar can still access the scrum notes:"
msgstr ""

#: ../../gettingstarted/cassandra.rst:282
msgid "Cassandra-Aware Visibility (Bonus)"
msgstr ""

#: ../../gettingstarted/cassandra.rst:284
msgid ""
"As a bonus, you can re-run the above queries with policy enforced and view how "
"Cilium provides Cassandra-aware visibility, including whether requests are "
"forwarded or denied.   First, use \"kubectl exec\" to access the cilium pod."
msgstr ""

#: ../../gettingstarted/cassandra.rst:293
msgid ""
"Next, start Cilium monitor, and limit the output to only \"l7\" type messages "
"using the \"-t\" flag:"
msgstr ""

#: ../../gettingstarted/cassandra.rst:301
msgid ""
"In the other windows, re-run the above queries, and you will see that Cilium "
"provides full visibility at the level of each Cassandra request, indicating:"
msgstr ""

#: ../../gettingstarted/cassandra.rst:304
msgid "The Kubernetes label-based identity of both the sending and receiving pod."
msgstr ""

#: ../../gettingstarted/cassandra.rst:305
msgid ""
"The details of the Cassandra request, including the 'query_action' (e.g., "
"'select', 'insert') and 'query_table' (e.g., 'system.local', 'attendance."
"daily_records')"
msgstr ""

#: ../../gettingstarted/cassandra.rst:307
msgid ""
"The 'verdict' indicating whether the request was allowed by policy ('Forwarded' "
"or 'Denied')."
msgstr ""

#: ../../gettingstarted/cassandra.rst:309
msgid ""
"Example output is below.   All requests are from *empire-outpost* to *cass-"
"server*.   The first two requests are allowed, a 'select' into 'system.local' "
"and an 'insert' into 'attendance.daily_records'. The second two requests are "
"denied, a 'select' into 'attendance.daily_records' and a select into 'deathstar."
"scrum_notes' :"
msgstr ""

#: ../../gettingstarted/cassandra.rst:321
#: ../../gettingstarted/elasticsearch.rst:192
#: ../../gettingstarted/host-firewall.rst:182 ../../gettingstarted/kafka.rst:276
msgid "Clean Up"
msgstr ""

#: ../../gettingstarted/cassandra.rst:323
msgid ""
"You have now installed Cilium, deployed a demo app, and tested L7 Cassandra-"
"aware network security policies.  To clean up, run:"
msgstr ""

#: ../../gettingstarted/cassandra.rst:331 ../../gettingstarted/grpc.rst:246
#: ../../gettingstarted/kafka.rst:287
msgid "After this, you can re-run the tutorial from Step 1."
msgstr ""

#: ../../gettingstarted/ciliumendpointslice.rst:11
msgid "CiliumEndpointSlice (beta)"
msgstr ""

#: ../../gettingstarted/ciliumendpointslice.rst:15
msgid ""
"This document describes CiliumEndpointSlice (CES), which enables batching of "
"CiliumEndpoint (CEP) objects in the cluster to achieve better scalability. The "
"concept of CiliumEndpointSlice is described in:"
msgstr ""

#: ../../gettingstarted/ciliumendpointslice.rst:25
msgid ""
"When enabled, Cilium operator watches CEP objects and group/batch slim versions "
"of them into CES objects. Cilium agent watches CES objects to learn about "
"remote endpoints in this mode. API-server stress due to remote endpoint info "
"propagation should be reduced in this case, allowing for better scalability, at "
"the cost of potentially longer delay before identity of new endpoint is "
"recognized throughout the cluster."
msgstr ""

#: ../../gettingstarted/ciliumendpointslice.rst:33
msgid "Deploy Cilium with CES"
msgstr ""

#: ../../gettingstarted/ciliumendpointslice.rst:34
msgid ""
"The CES feature relies on use of CEP. This feature is disabled by default in "
"current beta status. You can enable the feature by setting the "
"``enableCiliumEndpointSlice`` value to ``true``."
msgstr ""

#: ../../gettingstarted/ciliumendpointslice.rst:43
msgid "Verify that Cilium agent and operator pods are running."
msgstr ""

#: ../../gettingstarted/ciliumendpointslice.rst:55
msgid "Validate that the CiliumEndpointSlice CRD has been registered."
msgstr ""

#: ../../gettingstarted/ciliumendpointslice.rst:65
#: ../../gettingstarted/encryption.rst:25
msgid "Known Issues and Workarounds"
msgstr ""

#: ../../gettingstarted/ciliumendpointslice.rst:68
msgid "Potential Upgrade Impact"
msgstr ""

#: ../../gettingstarted/ciliumendpointslice.rst:69
msgid ""
"When performing an upgrade from a non-CES mode to CES mode, the operator will "
"need to process all existing CEPs in the cluster and create CESs for them. "
"Depending on the load of the cluster, this could take some time."
msgstr ""

#: ../../gettingstarted/ciliumendpointslice.rst:73
msgid ""
"During this period, new endpoints might experience longer delay before their "
"identities will propagate to remote nodes. If a newly created pod is affected "
"by this delay, then it may temporarily fail to establish connectivity to remote "
"pods in the cluster."
msgstr ""

#: ../../gettingstarted/ciliumendpointslice.rst:78
msgid ""
"A mitigation strategy is to upgrade the operator from non-CES to CES before "
"upgrading agents on each node. This will let it start creating CESs right away, "
"while the agents are still watching CEP resources."
msgstr ""

#: ../../gettingstarted/ciliumendpointslice.rst:83
msgid "Potential Racing Condition when Identity of An Existing Endpoint Changes"
msgstr ""

#: ../../gettingstarted/ciliumendpointslice.rst:84
msgid ""
"When there's an identity change for any existing resources, e.g., a pod, a "
"deployment or a statefulset, although unlikely, the endpoints that undergo this "
"change might experience connection disruption."
msgstr ""

#: ../../gettingstarted/ciliumendpointslice.rst:88
msgid ""
"Root cause for this potential disruption is that when identity of CEPs change, "
"the operator will try to re-group/re-batch them into a different set of CESs. "
"This breaks the atomic operation of an UPGRADE into that of an DELETE and an "
"ADD. If the agent gets the DELETE (from old CES) first, it will remove the "
"corresponding CEP's information from the ipcache, resulting in traffic to/from "
"said CEP with an UNKNOWN identity."
msgstr ""

#: ../../gettingstarted/ciliumendpointslice.rst:95
msgid ""
"In current implementation, Cilium adds a delay (default: 1s) before sending out "
"the DELETE event. This should greatly reduce the probability of connection "
"disruption in most cases."
msgstr ""

#: ../../gettingstarted/ciliumendpointslice.rst:100
msgid "Egress Gateway Won't Work with CES"
msgstr ""

#: ../../gettingstarted/ciliumendpointslice.rst:101
msgid ""
"This is a temporary issue until :gh-issue:`17669` is fixed. If you are running "
"Egress Gateway feature, enabling CES would break all egress masquerading until "
"the fix is in place."
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:6
#: ../../gettingstarted/kind.rst:169
msgid "Setting up Cluster Mesh"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:8
msgid ""
"This is a step-by-step guide on how to build a mesh of Kubernetes clusters by "
"connecting them together, enable pod-to-pod connectivity across all clusters, "
"define global services to load-balance between clusters and enforce security "
"policies to restrict access."
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:14
#: ../../gettingstarted/clustermesh/policy.rst:11
#: ../../gettingstarted/clustermesh/services.rst:11
#: ../../gettingstarted/external-workloads.rst:22
#: ../../gettingstarted/k8s-install-kops.rst:25
#: ../../gettingstarted/local-redirect-policy.rst:37
msgid "Prerequisites"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:17
msgid "Cluster Addressing Requirements"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:19
msgid ""
"PodCIDR ranges in all clusters and all nodes must be non-conflicting and unique "
"IP addresses."
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:22
msgid ""
"Nodes in all clusters must have IP connectivity between each other. This "
"requirement is typically met by establishing peering or VPN tunnels between the "
"networks of the nodes of each cluster."
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:26
msgid ""
"The network between clusters must allow the inter-cluster communication. The "
"exact ports are documented in the :ref:`firewall_requirements` section."
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:30
#: ../../gettingstarted/k8s-install-default.rst:226
msgid "Install the Cilium CLI"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:35
msgid "Prepare the Clusters"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:37
msgid ""
"For the rest of this tutorial, we will assume that you intend to connect two "
"clusters together with the kubectl configuration context stored in the "
"environment variables ``$CLUSTER1`` and ``$CLUSTER2``. This context name is the "
"same as you typically pass to ``kubectl --context``."
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:43
msgid "Specify the Cluster Name and ID"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:45
msgid ""
"Each cluster must be assigned a unique human-readable name as well as a numeric "
"cluster ID (1-255). It is best to assign both these attributes at installation "
"time of Cilium:"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:49
msgid "ConfigMap options ``cluster-name`` and ``cluster-id``"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:50
msgid "Helm options ``cluster.name`` and ``cluster.id``"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:51
msgid "``cilium install`` options ``--cluster-name`` and ``--cluster-id``"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:55
msgid ""
"If you change the cluster ID and/or cluster name in a cluster with running "
"workloads, you will need to restart all workloads. The cluster ID is used to "
"generate the security identity and it will need to be re-created in order to "
"establish access across clusters."
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:61
msgid "Shared Certificate Authority"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:63
msgid ""
"If you are planning to run Hubble Relay across clusters, it is best to share a "
"certificate authority (CA) between the clusters as it will enable mTLS across "
"clusters to just work."
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:67
msgid ""
"The easiest way to establish this is to pass ``--inherit-ca`` to the "
"``install`` command when installing additional clusters:"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:74
msgid ""
"If you are not using ``cilium install`` for the installation, simply propagate "
"the Kubernetes secret containing the CA from one cluster to the other."
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:80
msgid "Enable Cluster Mesh"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:82
msgid ""
"Enable all required components by running ``cilium clustermesh enable`` in the "
"context of both clusters. This will deploy the ``clustermesh-apiserver`` into "
"the cluster and generate all required certificates and import them as "
"Kubernetes secrets. It will also attempt to auto-detect the best service type "
"for the LoadBalancer to expose the Cluster Mesh control plane to other clusters."
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:94
msgid "You should be seeing output similar to the following:"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:123
msgid ""
"In some cases, the service type cannot be automatically detected and you need "
"to specify it manually. This can be done with the option ``--service-type``. "
"The possible values are:"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:128
msgid "LoadBalancer:"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:127
msgid ""
"A Kubernetes service of type LoadBalancer is used to expose the control plane. "
"This uses a stable LoadBalancer IP and is typically the best option."
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:135
msgid "NodePort:"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:131
msgid ""
"A Kubernetes service of type NodePort is used to expose the control plane. This "
"requires stable Node IPs. If a node disappears, the Cluster Mesh may have to "
"reconnect to a different node. If all nodes have become unavailable, you may "
"have to re-connect the clusters to extract new node IPs."
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:138
msgid "ClusterIP:"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:138
msgid ""
"A Kubernetes service of type ClusterIP is used to expose the control plane. "
"This requires the ClusterIPs are routable between clusters."
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:141
msgid ""
"Wait for the Cluster Mesh components to come up by invoking ``cilium "
"clustermesh status --wait``. If you are using a service of type LoadBalancer "
"then this will also wait for the LoadBalancer to be assigned an IP."
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:160
msgid "Connect Clusters"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:162
msgid ""
"Finally, connect the clusters. This step only needs to be done in one "
"direction. The connection will automatically be established in both directions:"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:170
msgid "The output should look something like this:"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:188
msgid ""
"It may take a bit for the clusters to be connected. You can run ``cilium "
"clustermesh status --wait`` to wait for the connection to be successful:"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:195
msgid "The output will look something like this:"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:213
msgid ""
"If this step does not complete successfully, proceed to the troubleshooting "
"section."
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:217
msgid "Test Pod Connectivity Between Clusters"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:219
msgid ""
"Congratulations, you have successfully connected your clusters together. You "
"can validate the connectivity by running the connectivity test in multi cluster "
"mode:"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:230
msgid "Logical next steps to explore from here are:"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:232
msgid ":ref:`gs_clustermesh_services`"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:233
msgid ":ref:`gs_clustermesh_network_policy`"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:236
#: ../../gettingstarted/encryption-ipsec.rst:214
#: ../../gettingstarted/encryption-wireguard.rst:135
#: ../../gettingstarted/kind.rst:77 ../../gettingstarted/kubeproxy-free.rst:1349
msgid "Troubleshooting"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:238
msgid "Use the following list of steps to troubleshoot issues with ClusterMesh:"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:240
msgid ""
"Validate that the ``cilium-xxx`` as well as the ``cilium-operator-xxx`` pods "
"are healthy and ready."
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:248
msgid "Validate the Cluster Mesh is enabled correctly and operational:"
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:255
msgid ""
"If you cannot resolve the issue with the above commands, see the :ref:"
"`troubleshooting_clustermesh` for a more detailed troubleshooting guide."
msgstr ""

#: ../../gettingstarted/clustermesh/clustermesh.rst:261
msgid ""
"The number of clusters that can be connected together is currently limited to "
"255. This limitation will be lifted in the future when running in direct "
"routing mode or when running in encapsulation mode with encryption enabled."
msgstr ""

#: ../../gettingstarted/clustermesh/policy.rst:5
msgid "Network Policy"
msgstr ""

#: ../../gettingstarted/clustermesh/policy.rst:7
msgid ""
"This tutorial will guide you how to define NetworkPolicies affecting multiple "
"clusters."
msgstr ""

#: ../../gettingstarted/clustermesh/policy.rst:13
#: ../../gettingstarted/clustermesh/services.rst:13
msgid ""
"You need to have a functioning Cluster Mesh setup, please follow the guide :ref:"
"`gs_clustermesh` to set it up."
msgstr ""

#: ../../gettingstarted/clustermesh/policy.rst:17
msgid "Security Policies"
msgstr ""

#: ../../gettingstarted/clustermesh/policy.rst:19
msgid ""
"As addressing and network security are decoupled, network security enforcement "
"automatically spans across clusters. Note that Kubernetes security policies are "
"not automatically distributed across clusters, it is your responsibility to "
"apply ``CiliumNetworkPolicy`` or ``NetworkPolicy`` in all clusters."
msgstr ""

#: ../../gettingstarted/clustermesh/policy.rst:25
msgid "Allowing Specific Communication Between Clusters"
msgstr ""

#: ../../gettingstarted/clustermesh/policy.rst:27
msgid ""
"The following policy illustrates how to allow particular pods to communicate "
"between two clusters. The cluster name refers to the name given via the ``--"
"cluster-name`` agent option or ``cluster-name`` ConfigMap option."
msgstr ""

#: ../../gettingstarted/clustermesh/policy.rst:52
msgid ""
"L7 security policies currently only work across multiple clusters if worker "
"nodes have routes installed allowing to route pod IPs of all clusters. This is "
"obtained when running in direct routing mode by running a routing daemon or ``--"
"auto-direct-node-routes`` but won't work automatically when using tunnel/"
"encapsulation mode."
msgstr ""

#: ../../gettingstarted/clustermesh/services.rst:5
msgid "Load-balancing & Service Discovery"
msgstr ""

#: ../../gettingstarted/clustermesh/services.rst:7
msgid ""
"This tutorial will guide you to perform load-balancing and service discovery "
"across multiple Kubernetes clusters when using Cilium."
msgstr ""

#: ../../gettingstarted/clustermesh/services.rst:17
msgid "Load-balancing with Global Services"
msgstr ""

#: ../../gettingstarted/clustermesh/services.rst:19
msgid ""
"Establishing load-balancing between clusters is achieved by defining a "
"Kubernetes service with identical name and namespace in each cluster and adding "
"the annotation ``io.cilium/global-service: \"true\"`` to declare it global. "
"Cilium will automatically perform load-balancing to pods in both clusters."
msgstr ""

#: ../../gettingstarted/clustermesh/services.rst:28
msgid "Disabling Global Service Sharing"
msgstr ""

#: ../../gettingstarted/clustermesh/services.rst:30
msgid ""
"By default, a Global Service will load-balance across backends in multiple "
"clusters. This implicitly configures ``io.cilium/shared-service: \"true\"``. To "
"prevent service backends from being shared to other clusters, this option "
"should be disabled."
msgstr ""

#: ../../gettingstarted/clustermesh/services.rst:34
msgid "Below example will expose remote endpoint without sharing local endpoints."
msgstr ""

#: ../../gettingstarted/clustermesh/services.rst:54
msgid "Deploying a Simple Example Service"
msgstr ""

#: ../../gettingstarted/clustermesh/services.rst:56
msgid "In cluster 1, deploy:"
msgstr ""

#: ../../gettingstarted/clustermesh/services.rst:63
msgid "In cluster 2, deploy:"
msgstr ""

#: ../../gettingstarted/clustermesh/services.rst:70
#: ../../gettingstarted/clustermesh/services.rst:106
msgid "From either cluster, access the global service:"
msgstr ""

#: ../../gettingstarted/clustermesh/services.rst:76
msgid "You will see replies from pods in both clusters."
msgstr ""

#: ../../gettingstarted/clustermesh/services.rst:78
msgid ""
"In cluster 1, add ``io.cilium/shared-service=\"false\"`` to existing global "
"service"
msgstr ""

#: ../../gettingstarted/clustermesh/services.rst:84
msgid "From cluster 1, access the global service one more time:"
msgstr ""

#: ../../gettingstarted/clustermesh/services.rst:90
msgid "You will still see replies from pods in both clusters."
msgstr ""

#: ../../gettingstarted/clustermesh/services.rst:92
msgid "From cluster 2, access the global service again:"
msgstr ""

#: ../../gettingstarted/clustermesh/services.rst:98
msgid ""
"You will see replies from pods only from cluster 2, as the global service in "
"cluster 1 is no longer shared."
msgstr ""

#: ../../gettingstarted/clustermesh/services.rst:100
msgid ""
"In cluster 1, remove ``io.cilium/shared-service`` annotation of existing global "
"service"
msgstr ""

#: ../../gettingstarted/clustermesh/services.rst:112
msgid "You will see replies from pods in both clusters again."
msgstr ""

#: ../../gettingstarted/cni-chaining.rst:11
msgid "CNI Chaining"
msgstr ""

#: ../../gettingstarted/cni-chaining.rst:13
msgid "CNI chaining allows to use Cilium in combination with other CNI plugins."
msgstr ""

#: ../../gettingstarted/cni-chaining.rst:15
msgid ""
"With Cilium CNI chaining, the base network connectivity and IP address "
"management is managed by the non-Cilium CNI plugin, but Cilium attaches eBPF "
"programs to the network devices created by the non-Cilium plugin to provide L3/"
"L4 network visibility, policy enforcement and other advanced features."
msgstr ""

#: ../../gettingstarted/cni-chaining-aws-cni.rst:11
msgid "AWS VPC CNI plugin"
msgstr ""

#: ../../gettingstarted/cni-chaining-aws-cni.rst:13
msgid ""
"This guide explains how to set up Cilium in combination with the AWS VPC CNI "
"plugin. In this hybrid mode, the AWS VPC CNI plugin is responsible for setting "
"up the virtual network devices as well as for IP address management (IPAM) via "
"ENIs. After the initial networking is setup for a given pod, the Cilium CNI "
"plugin is called to attach eBPF programs to the network devices set up by the "
"AWS VPC CNI plugin in order to enforce network policies, perform load-balancing "
"and provide encryption."
msgstr ""

#: ../../gettingstarted/cni-chaining-limitations.rst:9
msgid ""
"Some advanced Cilium features may be limited when chaining with other CNI "
"plugins, such as:"
msgstr ""

#: ../../gettingstarted/cni-chaining-limitations.rst:12
msgid ":ref:`Layer 7 Policy <l7_policy>` (see :gh-issue:`12454`)"
msgstr ""

#: ../../gettingstarted/cni-chaining-limitations.rst:13
msgid ":ref:`encryption_ipsec` (see :gh-issue:`15596`)"
msgstr ""

#: ../../gettingstarted/cni-chaining-aws-cni.rst:25
msgid ""
"Please ensure that you are running version `1.7.9 <https://github.com/aws/"
"amazon-vpc-cni-k8s/releases/tag/v1.7.9>`_ or newer of the AWS VPC CNI plugin to "
"guarantee compatibility with Cilium."
msgstr ""

#: ../../gettingstarted/cni-chaining-aws-cni.rst:33
msgid ""
"If you are running an older version, as in the above example, you can upgrade "
"it with:"
msgstr ""

#: ../../gettingstarted/cni-chaining-aws-cni.rst:43
msgid "Setting up a cluster on AWS"
msgstr ""

#: ../../gettingstarted/cni-chaining-aws-cni.rst:45
msgid ""
"Follow the instructions in the :ref:`k8s_install_quick` guide to set up an EKS "
"cluster, or use any other method of your preference to set up a Kubernetes "
"cluster on AWS."
msgstr ""

#: ../../gettingstarted/cni-chaining-aws-cni.rst:49
msgid ""
"Ensure that the `aws-vpc-cni-k8s <https://github.com/aws/amazon-vpc-cni-k8s>`_ "
"plugin is installed — which will already be the case if you have created an EKS "
"cluster. Also, ensure the version of the plugin is up-to-date as per the above."
msgstr ""

#: ../../gettingstarted/cni-chaining-aws-cni.rst:55
msgid "Deploy Cilium via Helm:"
msgstr ""

#: ../../gettingstarted/cni-chaining-aws-cni.rst:65
msgid ""
"This will enable chaining with the AWS VPC CNI plugin. It will also disable "
"tunneling, as it's not required since ENI IP addresses can be directly routed "
"in the VPC. For the same reason, masquerading can be disabled as well."
msgstr ""

#: ../../gettingstarted/cni-chaining-aws-cni.rst:70
#: ../../gettingstarted/cni-chaining-portmap.rst:58
msgid "Restart existing pods"
msgstr ""

#: ../../gettingstarted/cni-chaining-aws-cni.rst:72
msgid ""
"The new CNI chaining configuration *will not* apply to any pod that is already "
"running in the cluster. Existing pods will be reachable, and Cilium will load-"
"balance *to* them, but not *from* them. Policy enforcement will also not be "
"applied. For these reasons, you must restart these pods so that the chaining "
"configuration can be applied to them."
msgstr ""

#: ../../gettingstarted/cni-chaining-aws-cni.rst:78
msgid ""
"The following command can be used to check which pods need to be restarted:"
msgstr ""

#: ../../gettingstarted/cni-chaining-aws-cni.rst:97
msgid "Advanced"
msgstr ""

#: ../../gettingstarted/cni-chaining-aws-cni.rst:100
msgid "Enabling security groups for pods (EKS)"
msgstr ""

#: ../../gettingstarted/cni-chaining-aws-cni.rst:102
msgid ""
"Cilium can be used alongside the `security groups for pods <https://docs.aws."
"amazon.com/eks/latest/userguide/security-groups-for-pods.html>`_ feature of EKS "
"in supported clusters when running in chaining mode. Follow the instructions "
"below to enable this feature:"
msgstr ""

#: ../../gettingstarted/cni-chaining-aws-cni.rst:108
msgid ""
"The following guide requires `jq <https://stedolan.github.io/jq/>`_ and the "
"`AWS CLI <https://aws.amazon.com/cli/>`_ to be installed and configured."
msgstr ""

#: ../../gettingstarted/cni-chaining-aws-cni.rst:111
msgid ""
"Make sure that the ``AmazonEKSVPCResourceController`` managed policy is "
"attached to the IAM role associated with the EKS cluster:"
msgstr ""

#: ../../gettingstarted/cni-chaining-aws-cni.rst:124
msgid ""
"Then, and as mentioned above, make sure that the version of the AWS VPC CNI "
"plugin running in the cluster is up-to-date:"
msgstr ""

#: ../../gettingstarted/cni-chaining-aws-cni.rst:133
msgid ""
"Next, patch the ``kube-system/aws-node`` DaemonSet in order to enable security "
"groups for pods:"
msgstr ""

#: ../../gettingstarted/cni-chaining-aws-cni.rst:142
msgid ""
"After the rollout is complete, all nodes in the cluster should have the ``vps."
"amazonaws.com/has-trunk-attached`` label set to ``true``:"
msgstr ""

#: ../../gettingstarted/cni-chaining-aws-cni.rst:151
msgid ""
"From this moment everything should be in place. For details on how to actually "
"associate security groups to pods, please refer to the `official documentation "
"<https://docs.aws.amazon.com/eks/latest/userguide/security-groups-for-pods."
"html>`_."
msgstr ""

#: ../../gettingstarted/cni-chaining-azure-cni.rst:11
msgid "Azure CNI"
msgstr ""

#: ../../gettingstarted/cni-chaining-azure-cni.rst:15
msgid ""
"This is not the best option to run Cilium on AKS or Azure. Please refer to :ref:"
"`k8s_install_quick` for the best guide to run Cilium in Azure Cloud. Follow "
"this guide if you specifically want to run Cilium in combination with the Azure "
"CNI in a chaining configuration."
msgstr ""

#: ../../gettingstarted/cni-chaining-azure-cni.rst:22
msgid ""
"This guide explains how to set up Cilium in combination with Azure CNI in a "
"chaining configuration. In this hybrid mode, the Azure CNI plugin is "
"responsible for setting up the virtual network devices as well as address "
"allocation (IPAM). After the initial networking is setup, the Cilium CNI plugin "
"is called to attach eBPF programs to the network devices set up by Azure CNI to "
"enforce network policies, perform load-balancing, and encryption."
msgstr ""

#: ../../gettingstarted/cni-chaining-azure-cni.rst:31
msgid "Create an AKS + Cilium CNI configuration"
msgstr ""

#: ../../gettingstarted/cni-chaining-azure-cni.rst:33
msgid ""
"Create a ``chaining.yaml`` file based on the following template to specify the "
"desired CNI chaining configuration. This :term:`ConfigMap` will be installed as "
"the CNI configuration file on all nodes and defines the chaining configuration. "
"In the example below, the Azure CNI, portmap, and Cilium are chained together."
msgstr ""

#: ../../gettingstarted/cni-chaining-azure-cni.rst:70
#: ../../gettingstarted/cni-chaining-calico.rst:62
#: ../../gettingstarted/cni-chaining-generic-veth.rst:69
#: ../../gettingstarted/cni-chaining-weave.rst:52
msgid "Deploy the :term:`ConfigMap`:"
msgstr ""

#: ../../gettingstarted/cni-chaining-azure-cni.rst:96
msgid ""
"This will create both the main cilium daemonset, as well as the cilium-node-"
"init daemonset, which handles tasks like mounting the eBPF filesystem and "
"updating the existing Azure CNI plugin to run in 'transparent' mode."
msgstr ""

#: ../../gettingstarted/k8s-install-restart-pods.rst:2
msgid "Restart unmanaged Pods"
msgstr ""

#: ../../gettingstarted/k8s-install-restart-pods.rst:4
msgid ""
"If you did not create a cluster with the nodes tainted with the taint ``node."
"cilium.io/agent-not-ready``, then unmanaged pods need to be restarted manually. "
"Restart all already running pods which are not running in host-networking mode "
"to ensure that Cilium starts managing them. This is required to ensure that all "
"pods which have been running before Cilium was deployed have network "
"connectivity provided by Cilium and NetworkPolicy applies to them:"
msgstr ""

#: ../../gettingstarted/k8s-install-restart-pods.rst:27
msgid ""
"This may error out on macOS due to ``-r`` being unsupported by ``xargs``. In "
"this case you can safely run this command without ``-r`` with the symptom that "
"this will hang if there are no pods to restart. You can stop this with ``ctrl-"
"c``."
msgstr ""

#: ../../gettingstarted/cni-chaining-calico.rst:9
msgid "Calico"
msgstr ""

#: ../../gettingstarted/cni-chaining-calico.rst:11
msgid ""
"This guide instructs how to install Cilium in chaining configuration on top of "
"`Calico <https://github.com/projectcalico/calico>`_."
msgstr ""

#: ../../gettingstarted/cni-chaining-calico.rst:17
#: ../../gettingstarted/cni-chaining-weave.rst:17
msgid "Create a CNI configuration"
msgstr ""

#: ../../gettingstarted/cni-chaining-calico.rst:19
#: ../../gettingstarted/cni-chaining-generic-veth.rst:42
#: ../../gettingstarted/cni-chaining-weave.rst:19
msgid ""
"Create a ``chaining.yaml`` file based on the following template to specify the "
"desired CNI chaining configuration:"
msgstr ""

#: ../../gettingstarted/cni-chaining-calico.rst:69
#: ../../gettingstarted/cni-chaining-generic-veth.rst:76
#: ../../gettingstarted/cni-chaining-portmap.rst:35
#: ../../gettingstarted/cni-chaining-weave.rst:59
msgid "Deploy Cilium with the portmap plugin enabled"
msgstr ""

#: ../../gettingstarted/cni-chaining-calico.rst:88
#: ../../gettingstarted/cni-chaining-weave.rst:77
msgid ""
"The new CNI chaining configuration will *not* apply to any pod that is already "
"running the cluster. Existing pods will be reachable and Cilium will load-"
"balance to them but policy enforcement will not apply to them and load-"
"balancing is not performed for traffic originating from existing pods."
msgstr ""

#: ../../gettingstarted/cni-chaining-calico.rst:93
#: ../../gettingstarted/cni-chaining-weave.rst:82
msgid ""
"You must restart these pods in order to invoke the chaining configuration on "
"them."
msgstr ""

#: ../../gettingstarted/cni-chaining-generic-veth.rst:11
msgid "Generic Veth Chaining"
msgstr ""

#: ../../gettingstarted/cni-chaining-generic-veth.rst:13
msgid ""
"The generic veth chaining plugin enables CNI chaining on top of any CNI plugin "
"that is using a veth device model. The majority of CNI plugins use such a model."
msgstr ""

#: ../../gettingstarted/cni-chaining-generic-veth.rst:20
msgid "Validate that the current CNI plugin is using veth"
msgstr ""

#: ../../gettingstarted/cni-chaining-generic-veth.rst:22
msgid "Log into one of the worker nodes using SSH"
msgstr ""

#: ../../gettingstarted/cni-chaining-generic-veth.rst:23
msgid ""
"Run ``ip -d link`` to list all network devices on the node. You should be able "
"spot network devices representing the pods running on that node."
msgstr ""

#: ../../gettingstarted/cni-chaining-generic-veth.rst:25
msgid "A network device might look something like this:"
msgstr ""

#: ../../gettingstarted/cni-chaining-generic-veth.rst:32
msgid ""
"The ``veth`` keyword on line 3 indicates that the network device type is "
"virtual ethernet."
msgstr ""

#: ../../gettingstarted/cni-chaining-generic-veth.rst:34
msgid ""
"If the CNI plugin you are chaining with is currently not using veth then the "
"``generic-veth`` plugin is not suitable. In that case, a full CNI chaining "
"plugin is required which understands the device model of the underlying plugin. "
"Writing such a plugin is trivial, contact us on :ref:`slack` for more details."
msgstr ""

#: ../../gettingstarted/cni-chaining-generic-veth.rst:40
msgid "Create a CNI configuration to define your chaining configuration"
msgstr ""

#: ../../gettingstarted/cni-chaining-portmap.rst:12
msgid "Portmap (HostPort)"
msgstr ""

#: ../../gettingstarted/cni-chaining-portmap.rst:14
msgid ""
"Starting from Cilium 1.8, the Kubernetes HostPort feature is supported natively "
"through Cilium's eBPF-based kube-proxy replacement. CNI chaining is therefore "
"not needed anymore. For more information, see section :ref:"
"`kubeproxyfree_hostport`."
msgstr ""

#: ../../gettingstarted/cni-chaining-portmap.rst:18
msgid ""
"However, for the case where Cilium is deployed as "
"``kubeProxyReplacement=disabled``, the HostPort feature can then be enabled via "
"CNI chaining with the portmap plugin which implements HostPort. This guide "
"documents how to enable the latter for the chaining case."
msgstr ""

#: ../../gettingstarted/cni-chaining-portmap.rst:23
msgid ""
"For more general information about the Kubernetes HostPort feature, check out "
"the upstream documentation: `Kubernetes hostPort-CNI plugin documentation "
"<https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/"
"network-plugins/#support-hostport>`_."
msgstr ""

#: ../../gettingstarted/cni-chaining-portmap.rst:30
msgid ""
"Before using HostPort, read the `Kubernetes Configuration Best Practices "
"<https://kubernetes.io/docs/concepts/configuration/overview/>`_ to understand "
"the implications of this feature."
msgstr ""

#: ../../gettingstarted/cni-chaining-portmap.rst:49
msgid ""
"You can combine the ``cni.chainingMode=portmap`` option with any of the other "
"installation guides."
msgstr ""

#: ../../gettingstarted/cni-chaining-portmap.rst:52
msgid ""
"As Cilium is deployed as a DaemonSet, it will write a new CNI configuration "
"``05-cilium.conflist`` and remove the standard ``05-cilium.conf``. The new "
"configuration now enables HostPort. Any new pod scheduled is now able to make "
"use of the HostPort functionality."
msgstr ""

#: ../../gettingstarted/cni-chaining-portmap.rst:60
msgid ""
"The new CNI chaining configuration will *not* apply to any pod that is already "
"running the cluster. Existing pods will be reachable and Cilium will load-"
"balance to them but policy enforcement will not apply to them and load-"
"balancing is not performed for traffic originating from existing pods. You must "
"restart these pods in order to invoke the chaining configuration on them."
msgstr ""

#: ../../gettingstarted/cni-chaining-weave.rst:9
msgid "Weave Net"
msgstr ""

#: ../../gettingstarted/cni-chaining-weave.rst:11
msgid ""
"This guide instructs how to install Cilium in chaining configuration on top of "
"`Weave Net <https://github.com/weaveworks/weave>`_."
msgstr ""

#: ../../gettingstarted/dns.rst:11
msgid "Locking down external access with DNS-based policies"
msgstr ""

#: ../../gettingstarted/dns.rst:13
msgid ""
"This document serves as an introduction for using Cilium to enforce DNS-based "
"security policies for Kubernetes pods."
msgstr ""

#: ../../gettingstarted/dns.rst:21
msgid ""
"DNS-based policies are very useful for controlling access to services running "
"outside the Kubernetes cluster. DNS acts as a persistent service identifier for "
"both external services provided by AWS, Google, Twilio, Stripe, etc., and "
"internal services such as database clusters running in private subnets outside "
"Kubernetes. CIDR or IP-based policies are cumbersome and hard to maintain as "
"the IPs associated with external services can change frequently. The Cilium DNS-"
"based policies provide an easy mechanism to specify access control while Cilium "
"manages the harder aspects of tracking DNS to IP mapping."
msgstr ""

#: ../../gettingstarted/dns.rst:23 ../../gettingstarted/tls-visibility.rst:55
msgid "In this guide we will learn about:"
msgstr ""

#: ../../gettingstarted/dns.rst:25
msgid ""
"Controlling egress access to services outside the cluster using DNS-based "
"policies"
msgstr ""

#: ../../gettingstarted/dns.rst:26
msgid "Using patterns (or wildcards) to whitelist a subset of DNS domains"
msgstr ""

#: ../../gettingstarted/dns.rst:27
msgid ""
"Combining DNS, port and L7 rules for restricting access to external service"
msgstr ""

#: ../../gettingstarted/dns.rst:29
msgid ""
"In line with our Star Wars theme examples, we will use a simple scenario where "
"the Empire's ``mediabot`` pods need access to Twitter for managing the Empire's "
"tweets. The pods shouldn't have access to any other external service."
msgstr ""

#: ../../gettingstarted/dns.rst:41
msgid "Apply DNS Egress Policy"
msgstr ""

#: ../../gettingstarted/dns.rst:43
msgid ""
"The following Cilium network policy allows ``mediabot`` pods to only access "
"``api.twitter.com``."
msgstr ""

#: ../../gettingstarted/dns.rst:47 ../../gettingstarted/grafana.rst:111
#: ../../gettingstarted/k8s-install-default.rst:237
#: ../../gettingstarted/k8s-install-helm.rst:25
msgid "Generic"
msgstr ""

#: ../../gettingstarted/dns.rst:51
#: ../../gettingstarted/k8s-install-default.rst:292
#: ../../gettingstarted/k8s-install-helm.rst:196
msgid "OpenShift"
msgstr ""

#: ../../gettingstarted/dns.rst:57
msgid ""
"OpenShift users will need to modify the policies to match the namespace "
"``openshift-dns`` (instead of ``kube-system``), remove the match on the ``k8s:"
"k8s-app=kube-dns`` label, and change the port to 5353."
msgstr ""

#: ../../gettingstarted/dns.rst:61 ../../gettingstarted/tls-visibility.rst:266
msgid "Let's take a closer look at the policy:"
msgstr ""

#: ../../gettingstarted/dns.rst:63
msgid ""
"The first egress section uses ``toFQDNs: matchName`` specification to allow "
"egress to ``api.twitter.com``. The destination DNS should match exactly the "
"name specified in the rule. The ``endpointSelector`` allows only pods with "
"labels ``class: mediabot, org:empire`` to have the egress access."
msgstr ""

#: ../../gettingstarted/dns.rst:64
msgid ""
"The second egress section (``toEndpoints``) allows ``mediabot`` pods to access "
"``kube-dns`` service. Note that ``rules: dns`` instructs Cilium to inspect and "
"allow DNS lookups matching specified patterns. In this case, inspect and allow "
"all DNS queries."
msgstr ""

#: ../../gettingstarted/dns.rst:66
msgid ""
"Note that with this policy the ``mediabot`` doesn't have access to any internal "
"cluster service other than ``kube-dns``. Refer to :ref:`Network Policy` to "
"learn more about policies for controlling access to internal cluster services."
msgstr ""

#: ../../gettingstarted/dns.rst:68 ../../gettingstarted/tls-visibility.rst:282
msgid "Let's apply the policy:"
msgstr ""

#: ../../gettingstarted/dns.rst:74
msgid ""
"Testing the policy, we see that ``mediabot`` has access to ``api.twitter.com`` "
"but doesn't have access to any other external service, e.g., ``help.twitter."
"com``."
msgstr ""

#: ../../gettingstarted/dns.rst:86
msgid "DNS Policies Using Patterns"
msgstr ""

#: ../../gettingstarted/dns.rst:88
msgid ""
"The above policy controlled DNS access based on exact match of the DNS domain "
"name. Often, it is required to allow access to a subset of domains. Let's say, "
"in the above example, ``mediabot`` pods need access to any Twitter sub-domain, "
"e.g., the pattern ``*.twitter.com``. We can achieve this easily by changing the "
"``toFQDN`` rule to use ``matchPattern`` instead of ``matchName``."
msgstr ""

#: ../../gettingstarted/dns.rst:96
msgid ""
"Test that ``mediabot`` has access to multiple Twitter services for which the "
"DNS matches the pattern ``*.twitter.com``. It is important to note and test "
"that this doesn't allow access to ``twitter.com`` because the ``*.`` in the "
"pattern requires one subdomain to be present in the DNS name. You can simply "
"add more ``matchName`` and ``matchPattern`` clauses to extend the access. (See :"
"ref:`DNS based` policies to learn more about specifying DNS rules using "
"patterns and names.)"
msgstr ""

#: ../../gettingstarted/dns.rst:112
msgid "Combining DNS, Port and L7 Rules"
msgstr ""

#: ../../gettingstarted/dns.rst:114
msgid ""
"The DNS-based policies can be combined with port (L4) and API (L7) rules to "
"further restrict the access. In our example, we will restrict ``mediabot`` pods "
"to access Twitter services only on ports ``443``. The ``toPorts`` section in "
"the policy below achieves the port-based restrictions along with the DNS-based "
"policies."
msgstr ""

#: ../../gettingstarted/dns.rst:122
msgid ""
"Testing, the access to ``https://help.twitter.com`` on port ``443`` will "
"succeed but the access to ``http://help.twitter.com`` on port ``80`` will be "
"denied."
msgstr ""

#: ../../gettingstarted/dns.rst:133 ../../gettingstarted/tls-visibility.rst:321
msgid ""
"Refer to :ref:`l4_policy` and :ref:`l7_policy` to learn more about Cilium L4 "
"and L7 network policies."
msgstr ""

#: ../../gettingstarted/dns.rst:136
#: ../../gettingstarted/external-workloads.rst:281
#: ../../gettingstarted/http.rst:397 ../../gettingstarted/policy-creation.rst:220
#: ../../gettingstarted/tls-visibility.rst:324
msgid "Clean-up"
msgstr ""

#: ../../gettingstarted/egress-gateway.rst:11
msgid "Egress Gateway (beta)"
msgstr ""

#: ../../gettingstarted/egress-gateway.rst:17
msgid "Egress Gateway requires a 5.2 or more recent kernel."
msgstr ""

#: ../../gettingstarted/egress-gateway.rst:19
msgid ""
"The egress gateway allows users to redirect egress pod traffic through "
"specific, gateway nodes. Packets are masqueraded to the gateway node IP."
msgstr ""

#: ../../gettingstarted/egress-gateway.rst:22
msgid ""
"This document explains how to enable the egress gateway and configure egress "
"NAT policies to route and SNAT the egress traffic for a specific workload."
msgstr ""

#: ../../gettingstarted/egress-gateway.rst:28
#: ../../gettingstarted/hubble_setup.rst:20
msgid ""
"This guide assumes that Cilium has been correctly installed in your Kubernetes "
"cluster. Please see :ref:`k8s_quick_install` for more information. If unsure, "
"run ``cilium status`` and validate that Cilium is up and running."
msgstr ""

#: ../../gettingstarted/egress-gateway.rst:34
msgid "Enable Egress Gateway"
msgstr ""

#: ../../gettingstarted/egress-gateway.rst:36
msgid ""
"The feature is disabled by default. The egress gateway requires BPF "
"masquerading, which itself requires BPF NodePort to be enabled. An easy way to "
"enable all requirements is as follows."
msgstr ""

#: ../../gettingstarted/egress-gateway.rst:42
#: ../../gettingstarted/encryption-ipsec.rst:73
#: ../../gettingstarted/encryption-ipsec.rst:118
#: ../../gettingstarted/encryption-ipsec.rst:139
#: ../../gettingstarted/encryption-wireguard.rst:73
#: ../../gettingstarted/hubble.rst:45 ../../gettingstarted/hubble_setup.rst:82
#: ../../gettingstarted/vtep.rst:49
msgid "Helm"
msgstr ""

#: ../../gettingstarted/egress-gateway.rst:44
msgid ""
"If you installed Cilium via ``helm install``, you may enable the Egress gateway "
"feature with the following command:"
msgstr ""

#: ../../gettingstarted/egress-gateway.rst:56 ../../gettingstarted/vtep.rst:65
msgid "ConfigMap"
msgstr ""

#: ../../gettingstarted/egress-gateway.rst:58
msgid ""
"Egress Gateway support can be enabled by setting the following options in the "
"``cilium-config`` ConfigMap:"
msgstr ""

#: ../../gettingstarted/egress-gateway.rst:68
msgid "Create an External Service (Optional)"
msgstr ""

#: ../../gettingstarted/egress-gateway.rst:70
msgid ""
"This feature will change the default behavior how a packet leaves a cluster. As "
"a result, from the external service's point of view, it will see different "
"source IP address from the cluster. If you don't have an external service to "
"experiment with, nginx is a very simple example that can demonstrate the "
"functionality, while nginx's access log shows which IP address the request is "
"coming from."
msgstr ""

#: ../../gettingstarted/egress-gateway.rst:76
msgid ""
"Create an nginx service on a Linux node that is external to the existing "
"Kubernetes cluster, and use it as the destination of the egress traffic."
msgstr ""

#: ../../gettingstarted/egress-gateway.rst:97
msgid "Create Client Pods"
msgstr ""

#: ../../gettingstarted/egress-gateway.rst:99
msgid ""
"Deploy a client pod that will generate traffic which will be redirected based "
"on the configurations specified in the CiliumEgressNATPolicy."
msgstr ""

#: ../../gettingstarted/egress-gateway.rst:113
msgid ""
"Verify access log from nginx node or other external services that the request "
"is coming from one of the node in Kubernetes cluster. For example, in nginx "
"node, the access log will contain something like the following:"
msgstr ""

#: ../../gettingstarted/egress-gateway.rst:123
msgid ""
"In the previous example, the client pod is running on the node "
"``192.168.60.11``, so the result makes sense. This is the default Kubernetes "
"behavior without egress NAT."
msgstr ""

#: ../../gettingstarted/egress-gateway.rst:127
msgid "Configure Egress IPs"
msgstr ""

#: ../../gettingstarted/egress-gateway.rst:129
msgid ""
"Deploy the following deployment to assign additional egress IP to the gateway "
"node. The node that runs the pod will have additional IP addresses configured "
"on the external interface (``enp0s8`` as in the example), and become the egress "
"gateway. In the following example, ``192.168.60.100`` and ``192.168.60.101`` "
"becomes the egress IP which can be consumed by Egress NAT Policy. Please make "
"sure these IP addresses are routable on the interface they are assigned to, "
"otherwise the return traffic won't be able to route back."
msgstr ""

#: ../../gettingstarted/egress-gateway.rst:138
msgid "Create Egress NAT Policy"
msgstr ""

#: ../../gettingstarted/egress-gateway.rst:140
msgid ""
"Apply the following Egress NAT Policy, which basically means: when the pod is "
"running in the namespace ``default`` and the pod itself has label ``org: "
"empire`` and ``class: mediabot``, if it's trying to talk to IP CIDR "
"``192.168.60.13/32``, then use egress IP ``192.168.60.100``. In this example, "
"it tells Cilium to forward the packet from client pod to the gateway node with "
"egress IP ``192.168.60.100``, and masquerade with that IP address."
msgstr ""

#: ../../gettingstarted/egress-gateway.rst:148
msgid "Let's switch back to the client pod and verify it works."
msgstr ""

#: ../../gettingstarted/egress-gateway.rst:156
msgid ""
"Verify access log from nginx node or service of your chose that the request is "
"coming from egress IP now instead of one of the nodes in Kubernetes cluster. In "
"the nginx's case, you will see logs like the following shows that the request "
"is coming from ``192.168.60.100`` now, instead of ``192.168.60.11``."
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:9
msgid "Getting Started Securing Elasticsearch"
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:11
msgid ""
"This document serves as an introduction for using Cilium to enforce "
"Elasticsearch-aware security policies.  It is a detailed walk-through of "
"getting a single-node Cilium environment running on your machine. It is "
"designed to take 15-30 minutes."
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:21
msgid ""
"Following the Cilium tradition, we will use a Star Wars-inspired example. The "
"Empire has a large scale Elasticsearch cluster which is used for storing a "
"variety of data including:"
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:23
msgid ""
"``index: troop_logs``: Stormtroopers performance logs collected from every "
"outpost which are used to identify and eliminate weak performers!"
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:24
msgid ""
"``index: spaceship_diagnostics``: Spaceships diagnostics data collected from "
"every spaceship which is used for R&D and improvement of the spaceships."
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:26
msgid ""
"Every outpost has an Elasticsearch client service to upload the Stormtroopers "
"logs. And every spaceship has a service to upload diagnostics. Similarly, the "
"Empire headquarters has a service to search and analyze the troop logs and "
"spaceship diagnostics data. Before we look into the security concerns, let's "
"first create this application scenario in minikube."
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:28
msgid "Deploy the app using command below, which will create"
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:30
msgid ""
"An ``elasticsearch`` service with the selector label ``component:"
"elasticsearch`` and a pod running Elasticsearch."
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:31
msgid ""
"Three Elasticsearch clients one each for ``empire-hq``, ``outpost`` and "
"``spaceship``."
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:62
msgid "Security Risks for Elasticsearch Access"
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:64
msgid ""
"For Elasticsearch clusters the **least privilege security** challenge is to "
"give clients access only to particular indices, and to limit the operations "
"each client is allowed to perform on each index. In this example, the "
"``outpost`` Elasticsearch clients only need access to upload troop logs; and "
"the ``empire-hq`` client only needs search access to both the indices.  From "
"the security perspective, the outposts are weak spots and susceptible to be "
"captured by the rebels. Once compromised, the clients can be used to search and "
"manipulate the critical data in Elasticsearch. We can simulate this attack, but "
"first let's run the commands for legitimate behavior for all the client "
"services."
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:66
msgid "``outpost`` client uploading troop logs"
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:74
msgid "``spaceship`` uploading diagnostics"
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:82
msgid "``empire-hq`` running search queries for logs and diagnostics"
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:99
msgid ""
"Now imagine an outpost captured by the rebels. In the commands below, the "
"rebels first search all the indices and then manipulate the diagnostics data "
"from a compromised outpost."
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:115
msgid ""
"Rebels manipulate spaceship diagnostics data so that the spaceship defects are "
"not known to the empire-hq! (Hint: Rebels have changed the ``stats`` for the "
"tiefighter spaceship, a change hard to detect but with adverse impact!)"
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:128
msgid "Securing Elasticsearch Using Cilium"
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:134
msgid ""
"Following the least privilege security principle, we want to the allow the "
"following legitimate actions and nothing more:"
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:136
msgid "``outpost`` service only has upload access to ``index: troop_logs``"
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:137
msgid ""
"``spaceship`` service only has upload access to ``index: spaceship_diagnostics``"
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:138
msgid "``empire-hq`` service only has search access for both the indices"
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:140
msgid ""
"Fortunately, the Empire DevOps team is using Cilium for their Kubernetes "
"cluster. Cilium provides L7 visibility and security policies to control "
"Elasticsearch API access. Cilium follows the **white-list, least privilege "
"model** for security. That is to say, a *CiliumNetworkPolicy* contains a list "
"of rules that define **allowed requests** and any request that does not match "
"the rules is denied."
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:142
msgid ""
"In this example, the policy rules are defined for inbound traffic (i.e., "
"\"ingress\") connections to the *elasticsearch* service. Note that endpoints "
"selected as backend pods for the service are defined by the *selector* labels. "
"*Selector* labels use the same concept as Kubernetes to define a service. In "
"this example, label ``component: elasticsearch`` defines the pods that are part "
"of the *elasticsearch* service in Kubernetes."
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:144
msgid ""
"In the policy file below, you will see the following rules for controlling the "
"indices access and actions performed:"
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:146
msgid ""
"``fromEndpoints`` with labels ``app:spaceship`` only ``HTTP`` ``PUT`` is "
"allowed on paths matching regex ``^/spaceship_diagnostics/stats/.*$``"
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:147
msgid ""
"``fromEndpoints`` with labels ``app:outpost`` only ``HTTP`` ``PUT`` is allowed "
"on paths matching regex ``^/troop_logs/log/.*$``"
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:148
msgid ""
"``fromEndpoints`` with labels ``app:empire`` only ``HTTP`` ``GET`` is allowed "
"on paths matching regex ``^/spaceship_diagnostics/_search/??.*$`` and ``^/"
"troop_logs/search/??.*$``"
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:152
msgid "Apply this Elasticsearch-aware network security policy using ``kubectl``:"
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:159
msgid ""
"Let's test the security policies. Firstly, the search access is blocked for "
"both outpost and spaceship. So from a compromised outpost, rebels will not be "
"able to search and obtain knowledge about troops and spaceship diagnostics. "
"Secondly, the outpost clients don't have access to create or update the "
"``index: spaceship_diagnostics``."
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:179
msgid ""
"We can re-run any of the below commands to show that the security policy still "
"allows all legitimate requests (i.e., no 403 errors are returned)."
msgstr ""

#: ../../gettingstarted/elasticsearch.rst:194
msgid ""
"You have now installed Cilium, deployed a demo app, and finally deployed & "
"tested Elasticsearch-aware network security policies. To clean up, run:"
msgstr ""

#: ../../gettingstarted/encryption.rst:11
msgid "Transparent Encryption"
msgstr ""

#: ../../gettingstarted/encryption.rst:13
msgid ""
"Cilium supports the transparent encryption of Cilium-managed host traffic and "
"traffic between Cilium-managed endpoints either using IPsec or WireGuard®:"
msgstr ""

#: ../../gettingstarted/encryption.rst:28
msgid "Egress traffic to not yet discovered remote endpoints may be unencrypted"
msgstr ""

#: ../../gettingstarted/encryption.rst:30
msgid ""
"To determine if a packet needs to be encrypted or not, transparent encryption "
"relies on the same mechanisms as policy enforcement to decide if the "
"destination of an outgoing packet belongs to a Cilium-managed endpoint on a "
"remote node. This means that if an endpoint is allowed to initiate traffic to "
"targets outside of the cluster, it is possible for that endpoint to send "
"packets to arbitrary IP addresses before Cilium learns that a particular IP "
"address belongs to a remote Cilium-managed endpoint or newly joined remote "
"Cilium host in the cluster. In such a case there is a time window during which "
"Cilium will send out the initial packets unencrypted, as it has to assume the "
"destination IP address is outside of the cluster. Once the information about "
"the newly created endpoint has propagated in the cluster and Cilium knows that "
"the IP address is an endpoint on a remote node, it will start encrypting "
"packets using the encryption key of the remote node."
msgstr ""

#: ../../gettingstarted/encryption.rst:44
msgid ""
"The workaround for this issue is to ensure that the endpoint is not allowed to "
"send unencrypted traffic to arbitrary targets outside of the cluster. This can "
"be achieved by defining an egress policy which either completely disallows "
"traffic to ``reserved:world`` identities, or only allows egress traffic to "
"addresses outside of the cluster to a certain subset of trusted IP addresses "
"using ``toCIDR``, ``toCIDRSet`` and ``toFQDN`` rules. See :ref:"
"`policy_examples` for more details about how to write network policies that "
"restrict egress traffic to certain endpoints."
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:11
msgid "IPsec Transparent Encryption"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:13
msgid ""
"This guide explains how to configure Cilium to use IPsec based transparent "
"encryption using Kubernetes secrets to distribute the IPsec keys. After this "
"configuration is complete all traffic between Cilium-managed endpoints, as well "
"as Cilium-managed host traffic, will be encrypted using IPsec. This guide uses "
"Kubernetes secrets to distribute keys. Alternatively, keys may be manually "
"distributed, but that is not shown here."
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:20
#: ../../gettingstarted/encryption-wireguard.rst:24
msgid ""
"Packets are not encrypted when they are destined to the same node from which "
"they were sent. This behavior is intended. Encryption would provide no benefits "
"in that case, given that the raw traffic can be observed on the node anyway."
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:25
msgid "Generate & Import the PSK"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:27
msgid ""
"First, create a Kubernetes secret for the IPsec configuration to be stored. The "
"example below demonstrates generation of the necessary IPsec configuration "
"which will be distributed as a Kubernetes secret called ``cilium-ipsec-keys``. "
"A Kubernetes secret should consist of one key-value pair where the key is the "
"name of the file to be mounted as a volume in cilium-agent pods, and the value "
"is an IPSec configuration in the following format::"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:38
msgid ""
"``Secret`` resources need to be deployed in the same namespace as Cilium! In "
"our example, we use ``kube-system``."
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:41
msgid ""
"In the example below, GMC-128-AES is used. However, any of the algorithms "
"supported by Linux may be used. To generate the secret, you may use the "
"following command:"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:50
msgid ""
"The secret can be seen with ``kubectl -n kube-system get secrets`` and will be "
"listed as ``cilium-ipsec-keys``."
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:60
msgid "Enable Encryption in Cilium"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:66
#: ../../gettingstarted/encryption-wireguard.rst:66
msgid ""
"If you are deploying Cilium with the Cilium CLI, pass the following options:"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:75
#: ../../gettingstarted/encryption-wireguard.rst:75
msgid ""
"If you are deploying Cilium with Helm by following :ref:`k8s_install_helm`, "
"pass the following options:"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:86
msgid ""
"``encryption.enabled`` enables encryption of the traffic between Cilium-managed "
"pods and ``encryption.nodeEncryption`` controls whether host traffic is "
"encrypted. ``encryption.type`` specifies the encryption method and can be "
"omitted as it defaults to ``ipsec``."
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:93
msgid ""
"When using Cilium in any direct routing configuration, ensure that the native "
"routing CIDR is set properly. This is done using ``--ipv4-native-routing-"
"cidr=CIDR`` with the CLI or ``--set ipv4NativeRoutingCIDR=CIDR`` with Helm."
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:98
msgid ""
"At this point the Cilium managed nodes will be using IPsec for all traffic. For "
"further information on Cilium's transparent encryption, see :ref:"
"`ebpf_datapath`."
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:102
msgid "Encryption interface"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:104
msgid ""
"An additional argument can be used to identify the network-facing interface. If "
"direct routing is used and no interface is specified, the default route link is "
"chosen by inspecting the routing tables. This will work in many cases, but "
"depending on routing rules, users may need to specify the encryption interface "
"as follows:"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:127
msgid "Node-to-node encryption (beta)"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:129
msgid "In order to enable node-to-node encryption, add:"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:148
msgid ""
"Node-to-node encryption is a beta feature. Please provide feedback and file a "
"GitHub issue if you experience any problems."
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:151
msgid ""
"Node-to-node encryption is tested and supported with direct routing modes. "
"Using with encapsulation/tunneling is not currently tested or supported."
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:154
msgid "Support with tunneling mode is tracked with :gh-issue:`13663`."
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:157
#: ../../gettingstarted/encryption-wireguard.rst:91
#: ../../gettingstarted/kubeproxy-free.rst:141
msgid "Validate the Setup"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:159
#: ../../gettingstarted/encryption-wireguard.rst:93
msgid ""
"Run a ``bash`` shell in one of the Cilium pods with ``kubectl -n kube-system "
"exec -ti ds/cilium -- bash`` and execute the following commands:"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:163
#: ../../gettingstarted/encryption-wireguard.rst:106
msgid "Install tcpdump"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:170
msgid ""
"Check that traffic is encrypted. In the example below, this can be verified by "
"the fact that packets carry the IP Encapsulating Security Payload (ESP). In the "
"example below, ``eth0`` is the interface used for pod-to-pod communication. "
"Replace this interface with ``cilium_vxlan`` if tunneling is enabled."
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:189
msgid "Key Rotation"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:191
msgid "To replace cilium-ipsec-keys secret with a new key:"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:200
msgid ""
"Then restart Cilium agents to transition to the new key with ``kubectl delete "
"pod -n kube-system -l k8s-app=cilium``. During transition the new and old keys "
"will be in use. The Cilium agent keeps per endpoint data on which key is used "
"by each endpoint and will use the correct key if either side has not yet been "
"updated. In this way encryption will work as new keys are rolled out."
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:207
msgid ""
"The ``KEYID`` environment variable in the above example stores the current key "
"ID used by Cilium. The key variable is a uint8 with value between 0-16 and "
"should be monotonically increasing every re-key with a rollover from 16 to 0. "
"The Cilium agent will default to ``KEYID`` of zero if its not specified in the "
"secret."
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:216
msgid ""
"If the ``cilium`` Pods fail to start after enabling encryption, double-check if "
"the IPSec ``Secret`` and Cilium are deployed in the same namespace together."
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:219
msgid "Make sure that the Cilium pods have kvstore connectivity:"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:227
msgid ""
"Check for ``level=warning`` and ``level=error`` messages in the Cilium log files"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:229
msgid ""
"If there is a warning message similar to ``Device eth0 does not exist``, use "
"``--set encryption.ipsec.interface=ethX`` to set the encryption interface."
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:233
msgid "Run a ``bash`` in a Cilium Pod and validate the following:"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:235
msgid "Routing rules matching on fwmark:"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:244
msgid "Content of routing tables"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:252
msgid ""
"In case of IPAM ENI mode, check if routing rules exist for the the IP address "
"of ``cilium_host`` interface.."
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:268
msgid "XFRM policy:"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:289
msgid "XFRM stats with state:"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:291
msgid "Check if the packets count increases as you send traffic."
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:293
msgid "Following is the output from the source node."
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:316
msgid "Following is the output from the destination node."
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:339
msgid "BPF program to decrypt traffic:"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:341
msgid ""
"Check if the BPF program to decrypt traffic is attached to all network facing "
"interfaces, or matching the configuration of ``--encrypt-interface`` (if "
"specified)."
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:351
msgid "Disabling Encryption"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:353
msgid ""
"To disable the encryption, regenerate the YAML with the option ``encryption."
"enabled=false``"
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:359
msgid ""
"Transparent encryption is not currently supported when chaining Cilium on top "
"of other CNI plugins. For more information, see :gh-issue:`15596`."
msgstr ""

#: ../../gettingstarted/encryption-ipsec.rst:361
msgid ":ref:`HostPolicies` are not currently supported with IPsec encryption."
msgstr ""

#: ../../gettingstarted/encryption-wireguard.rst:11
msgid "WireGuard Transparent Encryption"
msgstr ""

#: ../../gettingstarted/encryption-wireguard.rst:13
msgid ""
"This guide explains how to configure Cilium with transparent encryption of "
"traffic between Cilium-managed endpoints using `WireGuard® <https://www."
"wireguard.com/>`_."
msgstr ""

#: ../../gettingstarted/encryption-wireguard.rst:16
msgid ""
"When WireGuard is enabled in Cilium, the agent running on each cluster node "
"will establish a secure WireGuard tunnel between it and all other known nodes "
"in the cluster. Each node automatically creates its own encryption key-pair and "
"distributes its public key via the ``io.cilium.network.wg-pub-key`` annotation "
"in the Kubernetes ``CiliumNode`` custom resource object. Each node's public key "
"is then used by other nodes to decrypt and encrypt traffic from and to Cilium-"
"managed endpoints running on that node."
msgstr ""

#: ../../gettingstarted/encryption-wireguard.rst:28
msgid ""
"The WireGuard tunnel endpoint is exposed on UDP port ``51871`` on each node. If "
"you run Cilium in an environment that requires firewall rules to enable "
"connectivity, you will have to ensure that all Cilium cluster nodes can reach "
"each other via that port."
msgstr ""

#: ../../gettingstarted/encryption-wireguard.rst:35
msgid ""
"When running in the tunneling mode (i.e. with VXLAN or Geneve), pod to pod "
"traffic will be sent only over the WireGuard tunnel which means that the "
"packets will bypass the other tunnel, and thus they will be encapsulated only "
"once."
msgstr ""

#: ../../gettingstarted/encryption-wireguard.rst:41
msgid "Enable WireGuard in Cilium"
msgstr ""

#: ../../gettingstarted/encryption-wireguard.rst:43
msgid ""
"Before you enable WireGuard in Cilium, please ensure that the Linux "
"distribution running on your cluster nodes has support for WireGuard in kernel "
"mode (i.e. ``CONFIG_WIREGUARD=m`` on Linux 5.6 and newer, or via the out-of-"
"tree WireGuard module on older kernels). See `WireGuard Installation <https://"
"www.wireguard.com/install/>`_ for details on how to install the kernel module "
"on your Linux distribution."
msgstr ""

#: ../../gettingstarted/encryption-wireguard.rst:50
msgid ""
"If your kernel or distribution does not support WireGuard, Cilium agent can be "
"configured to fall back on the user-space implementation via the ``--enable-"
"wireguard-userspace-fallback`` flag. When this flag is enabled and Cilium "
"detects that the kernel has no native support for WireGuard, it will fallback "
"on the ``wireguard-go`` user-space implementation of WireGuard. When running "
"the user-space implementation, encryption and decryption of packets is "
"performed by the ``cilium-agent`` process. As a consequence, connectivity "
"between Cilium-managed endpoints will be unavailable whenever the ``cilium-"
"agent`` process is restarted, such as during upgrades or configuration changes. "
"Running WireGuard in user-space mode is therefore not recommended for "
"production workloads that require high availability."
msgstr ""

#: ../../gettingstarted/encryption-wireguard.rst:86
msgid ""
"WireGuard may also be enabled manually by setting setting the ``enable-"
"wireguard: true`` option in the Cilium ``ConfigMap`` and restarting each Cilium "
"agent instance."
msgstr ""

#: ../../gettingstarted/encryption-wireguard.rst:97
msgid ""
"Check that WireGuard has been enabled (number of peers should correspond to a "
"number of nodes subtracted by one):"
msgstr ""

#: ../../gettingstarted/encryption-wireguard.rst:113
msgid "Check that traffic is sent via the ``cilium_wg0`` tunnel device:"
msgstr ""

#: ../../gettingstarted/encryption-wireguard.rst:137
msgid ""
"When troubleshooting dropped or unencrypted packets between pods, the following "
"commands can be helpful:"
msgstr ""

#: ../../gettingstarted/encryption-wireguard.rst:193
msgid ""
"For pod to pod packets to be successfully encrypted and decrypted, the "
"following must hold:"
msgstr ""

#: ../../gettingstarted/encryption-wireguard.rst:196
msgid ""
"WireGuard public key of a remote node in the ``peers[*].public-key`` section "
"matches the actual public key of the remote node (``public-key`` retrieved via "
"the same command on the remote node)."
msgstr ""

#: ../../gettingstarted/encryption-wireguard.rst:199
msgid ""
"``peers[*].allowed-ips`` should contain a list of pod IP addresses running on "
"the remote."
msgstr ""

#: ../../gettingstarted/encryption-wireguard.rst:203
#: ../../gettingstarted/index.rst:85 ../../gettingstarted/kind.rst:116
msgid "Cluster Mesh"
msgstr ""

#: ../../gettingstarted/encryption-wireguard.rst:205
msgid ""
"WireGuard enabled Cilium clusters can be connected via :ref:`Cluster Mesh`. The "
"``clustermesh-apiserver`` will forward the necessary WireGuard public keys "
"automatically to remote clusters. In such a setup, it is important to note that "
"all participating clusters must have WireGuard encryption enabled, i.e. mixed "
"mode is currently not supported. In addition, UDP traffic between nodes of "
"different clusters on port ``51871`` must be allowed."
msgstr ""

#: ../../gettingstarted/encryption-wireguard.rst:216
msgid ""
"WireGuard support in Cilium currently lacks the following features, which may "
"be resolved in upcoming Cilium releases:"
msgstr ""

#: ../../gettingstarted/encryption-wireguard.rst:219
msgid ""
"Host-level encryption. Only traffic between two Cilium-managed endpoints (i.e. "
"pod-to-pod traffic) is encrypted. Traffic between two nodes and traffic between "
"a Cilium-managed pod and a remote node currently won't be encrypted."
msgstr ""

#: ../../gettingstarted/encryption-wireguard.rst:223
msgid "L7 policy enforcement and visibility"
msgstr ""

#: ../../gettingstarted/encryption-wireguard.rst:224
msgid "eBPF-based host routing"
msgstr ""

#: ../../gettingstarted/encryption-wireguard.rst:226
msgid "The current status of these limitations is tracked in :gh-issue:`15462`."
msgstr ""

#: ../../gettingstarted/encryption-wireguard.rst:229
msgid "Legal"
msgstr ""

#: ../../gettingstarted/encryption-wireguard.rst:231
msgid "\"WireGuard\" is a registered trademark of Jason A. Donenfeld."
msgstr ""

#: ../../gettingstarted/external-workloads.rst:13
msgid "Setting up Support for External Workloads (beta)"
msgstr ""

#: ../../gettingstarted/external-workloads.rst:15
msgid ""
"This is a step-by-step guide on how to add external workloads (such as VMs) in "
"to your Kubernetes cluster and to enforce security policies to restrict access."
msgstr ""

#: ../../gettingstarted/external-workloads.rst:24
msgid ""
"Cilium must be configured to use Kubernetes for identity allocation "
"(``identityAllocationMode`` set to ``crd``). This is the default for new "
"installations."
msgstr ""

#: ../../gettingstarted/external-workloads.rst:28
msgid ""
"External workloads must run a recent enough kernel (>= 4.17) for k8s service "
"access from the external host to work, see :ref:`host-services` for details."
msgstr ""

#: ../../gettingstarted/external-workloads.rst:32
msgid ""
"External workloads must have Docker 20.10 or newer installed on the system (a "
"version which supports ``--cgroupns`` CLI option)."
msgstr ""

#: ../../gettingstarted/external-workloads.rst:35
msgid ""
"External workloads must have IP connectivity with the nodes in your cluster. "
"This requirement is typically met by running your VMs in the same cloud "
"provider virtual network (e.g., GCP VPC) as your k8s cluster, or establishing "
"peering or VPN tunnels between the networks of the nodes of your cluster and "
"your external workloads. Note that this precludes any VMs running behind NATs."
msgstr ""

#: ../../gettingstarted/external-workloads.rst:42
msgid ""
"All external workloads must have a unique IP address assigned them. Node IPs of "
"such nodes and your clusters must not conflict with each other."
msgstr ""

#: ../../gettingstarted/external-workloads.rst:46
msgid ""
"The network between the external workloads and your cluster must allow the node-"
"cluster communication. The exact ports are documented in the :ref:"
"`firewall_requirements` section."
msgstr ""

#: ../../gettingstarted/external-workloads.rst:50
msgid ""
"This guide assumes your external workload manages domain name resolution "
"service by a stand-alone ``/etc/resolv.conf``, or via systemd (e.g., Ubuntu)."
msgstr ""

#: ../../gettingstarted/external-workloads.rst:54
msgid ""
"So far this functionality is only tested with the vxlan tunneling datapath mode "
"(default for most installations)."
msgstr ""

#: ../../gettingstarted/external-workloads.rst:60
msgid ""
"Transparent encryption of traffic to/from external workloads is currently not "
"supported."
msgstr ""

#: ../../gettingstarted/external-workloads.rst:64
msgid "Prepare your cluster"
msgstr ""

#: ../../gettingstarted/external-workloads.rst:67
msgid "Enable support for external workloads"
msgstr ""

#: ../../gettingstarted/external-workloads.rst:69
msgid ""
"Your cluster must be configured with support for external workloads enabled. "
"This can be done with the cilium CLI tool by issuing ``cilium clustermesh "
"enable`` after ``cilium install``:"
msgstr ""

#: ../../gettingstarted/external-workloads.rst:78
msgid ""
"Config option ``tunnel=vxlan`` overrides any default that could otherwise be "
"auto-detected for your k8s cluster. This is currently a requirement for "
"external workload support."
msgstr ""

#: ../../gettingstarted/external-workloads.rst:84
msgid ""
"If this fails indicating that ``--service-type`` needs to be given, add ``--"
"service-type NodePort`` to the second command above, i.e. ``cilium clustermesh "
"enable --service-type NodePort``. This will allow you to go through this guide, "
"but be warned that NodePort service type makes your installation very fragile, "
"it will become non-functional if the node through which the service is accessed "
"is removed from the cluster or if it otherwise becomes unreachable."
msgstr ""

#: ../../gettingstarted/external-workloads.rst:93
msgid ""
"This will add a deployment for ``clustermesh-apiserver`` into your cluster, as "
"well as the related cluster resources, such as TLS secrets. ``clustermesh-"
"apiserver`` service is exposed to the external workloads. If your are on GKE, "
"EKS, or AKS, this is done by default using the internal ``LoadBalancer`` "
"service type. Override the auto-detection with an explicit ``--service-type "
"LoadBalancer`` to use an external LoadBalancer service type that uses an IP "
"that is accessible from outside of the cluster."
msgstr ""

#: ../../gettingstarted/external-workloads.rst:104
msgid ""
"Use the ``--help`` option after any of the ``cilium clustermesh`` commands to "
"see a short synopsis of available command options."
msgstr ""

#: ../../gettingstarted/external-workloads.rst:108
msgid "Tell your cluster about external workloads"
msgstr ""

#: ../../gettingstarted/external-workloads.rst:110
msgid ""
"To allow an external workload to join your cluster, the cluster must be "
"informed about each such workload. This is done by creating a "
"``CiliumExternalWorkload`` (CEW) resource for each external workload. CEW "
"resource specifies the name and identity labels (including namespace) for the "
"workload. The name must be the hostname of the external workload, as returned "
"by the ``hostname`` command run in the external workload. In this example this "
"is ``runtime``. For now you must also allocate a small IP CIDR that must be "
"unique to each workload. For example, for a VM named ``runtime`` that is to "
"join the ``default`` namespace (``vm`` is an alias for the ``external-"
"workload`` subcommand):"
msgstr ""

#: ../../gettingstarted/external-workloads.rst:126
msgid ""
"``-n`` is an alias for ``--namespace`` and can be left out when the value is "
"``default``. The namespace value will be set as an identity label. The CEW "
"resource itself is not namespaced."
msgstr ""

#: ../../gettingstarted/external-workloads.rst:130
msgid "To see the list of existing CEW resources, run:"
msgstr ""

#: ../../gettingstarted/external-workloads.rst:136
msgid ""
"Note that CEW resources are not namespaced, so this command shows the status of "
"all CEW resources regardless of the namespace label that was used when creating "
"them. ``--namespace`` option for the status command controls the namespace of "
"Cilium deployment in your cluster and usually needs to be left as the default "
"``kube-system``."
msgstr ""

#: ../../gettingstarted/external-workloads.rst:142
msgid ""
"At this point the ``IP:`` in the status for ``runtime`` is ``N/A`` to inform "
"that the VM has not yet joined the cluster."
msgstr ""

#: ../../gettingstarted/external-workloads.rst:146
msgid "Install and configure Cilium on external workloads"
msgstr ""

#: ../../gettingstarted/external-workloads.rst:148
msgid ""
"Run the external workload install command on your k8s cluster. This extracts "
"the TLS certificates and other access information from the cluster installation "
"and writes out an installation script to be used in the external workloads to "
"install Cilium and connect it to your k8s cluster:"
msgstr ""

#: ../../gettingstarted/external-workloads.rst:158
msgid ""
"Note that the created script embeds the IP address for the ``clustermesh-"
"apiserver`` service. If service type ``LoadBalancer`` can not be used, this IP "
"address will be the one of the first node in your k8s cluster (for ``NodePort`` "
"service type). If this node is removed from the cluster the above step for "
"creating the installation script must be repeated and all the external "
"workloads reinstalled. ``LoadBalancer`` is not affected by a node removal."
msgstr ""

#: ../../gettingstarted/external-workloads.rst:166
msgid ""
"Log in to the external workload. First make sure the hostname matches the name "
"used in the CiliumExternalWorkload resource:"
msgstr ""

#: ../../gettingstarted/external-workloads.rst:173
msgid ""
"Next, copy ``install-external-workload.sh`` created above to the external "
"workload. Then run the installation script:"
msgstr ""

#: ../../gettingstarted/external-workloads.rst:180
msgid ""
"This command launches the Cilium agent in a docker container named ``cilium`` "
"and copies the ``cilium`` node CLI to your host. This needs ``sudo`` "
"permissions, so you may be asked for a password. Note that this ``cilium`` "
"command is not the same as the ``cilium`` CLI used to manage Cilium "
"installation on a k8s cluster."
msgstr ""

#: ../../gettingstarted/external-workloads.rst:186
msgid ""
"This command waits until the node has been connected to the cluster and the "
"cluster services are available. Then it re-configures ``/etc/resolv.conf`` with "
"the IP address of the ``kube-dns`` service."
msgstr ""

#: ../../gettingstarted/external-workloads.rst:192
msgid ""
"If your external workload node has multiple IP addresses you may need to tell "
"Cilium agent which IP to use. To this end add ``HOST_IP=<ip-address>`` to the "
"beginning of the command line above."
msgstr ""

#: ../../gettingstarted/external-workloads.rst:198
msgid "Verify basic connectivity"
msgstr ""

#: ../../gettingstarted/external-workloads.rst:200
msgid ""
"Next you can check the status of the Cilium agent in your external workload:"
msgstr ""

#: ../../gettingstarted/external-workloads.rst:206
msgid "You should see something like:"
msgstr ""

#: ../../gettingstarted/external-workloads.rst:214
msgid "Check that cluster DNS works:"
msgstr ""

#: ../../gettingstarted/external-workloads.rst:221
msgid "Inspecting status changes in the cluster"
msgstr ""

#: ../../gettingstarted/external-workloads.rst:223
msgid ""
"The following command in your cluster should show the external workload IPs and "
"their Cilium security IDs:"
msgstr ""

#: ../../gettingstarted/external-workloads.rst:229
msgid "External workloads should also be visible as Cilium Endpoints:"
msgstr ""

#: ../../gettingstarted/external-workloads.rst:236
msgid "Apply Cilium Network Policy to enforce traffic from external workloads"
msgstr ""

#: ../../gettingstarted/external-workloads.rst:238
msgid ""
"From the external workload, ping the backend IP of ``clustermesh-apiserver`` "
"service to verify connectivity:"
msgstr ""

#: ../../gettingstarted/external-workloads.rst:244
msgid ""
"The ping should keep running also when the following CCNP is applied in your "
"cluster:"
msgstr ""

#: ../../gettingstarted/external-workloads.rst:266
msgid ""
"The ping should stop if you delete these lines from the policy (e.g., ``kubectl "
"edit ccnp test-ccnp``):"
msgstr ""

#: ../../gettingstarted/external-workloads.rst:274
msgid "The ping should continue if you delete the policy:"
msgstr ""

#: ../../gettingstarted/external-workloads.rst:283
msgid ""
"You can remove the Cilium installation from your external workload by running "
"the installation script with the ``uninstall`` argument:"
msgstr ""

#: ../../gettingstarted/external-workloads.rst:292
msgid "Conclusion"
msgstr ""

#: ../../gettingstarted/external-workloads.rst:294
msgid ""
"With the above we have enabled policy-based communication between external "
"workloads and pods in your Kubernetes cluster. We have also established service "
"load-balancing from external workloads to your cluster backends, and configured "
"domain name lookup in the external workload to be served by kube-dns of your "
"cluster."
msgstr ""

#: ../../gettingstarted/grafana.rst:11
msgid "Running Prometheus & Grafana"
msgstr ""

#: ../../gettingstarted/grafana.rst:14
msgid "Install Prometheus & Grafana"
msgstr ""

#: ../../gettingstarted/grafana.rst:16
msgid ""
"This is an example deployment that includes Prometheus and Grafana in a single "
"deployment."
msgstr ""

#: ../../gettingstarted/grafana.rst:19
msgid "The default installation contains:"
msgstr ""

#: ../../gettingstarted/grafana.rst:21
msgid "**Grafana**: A visualization dashboard with Cilium Dashboard pre-loaded."
msgstr ""

#: ../../gettingstarted/grafana.rst:22
msgid "**Prometheus**: a time series database and monitoring system."
msgstr ""

#: ../../gettingstarted/grafana.rst:41
msgid ""
"This example deployment of Prometheus and Grafana will automatically scrape the "
"Cilium and Hubble metrics. See the :ref:`metrics` configuration guide on how to "
"configure a custom Prometheus instance."
msgstr ""

#: ../../gettingstarted/grafana.rst:46
msgid "Deploy Cilium and Hubble with metrics enabled"
msgstr ""

#: ../../gettingstarted/grafana.rst:48
msgid ""
"*Cilium*, *Hubble*, and *Cilium Operator* do not expose metrics by default. "
"Enabling metrics for these services will open ports ``9090``, ``9091``, and "
"``6942`` respectively on all nodes of your cluster where these components are "
"running."
msgstr ""

#: ../../gettingstarted/grafana.rst:53
msgid ""
"The metrics for Cilium, Hubble, and Cilium Operator can all be enabled "
"independently of each other with the following Helm values:"
msgstr ""

#: ../../gettingstarted/grafana.rst:56
msgid "``prometheus.enabled=true``: Enables metrics for ``cilium-agent``."
msgstr ""

#: ../../gettingstarted/grafana.rst:57
msgid ""
"``operator.prometheus.enabled=true``: Enables metrics for ``cilium-operator``."
msgstr ""

#: ../../gettingstarted/grafana.rst:58
msgid ""
"``hubble.metrics.enabled``: Enables the provided list of Hubble metrics. For "
"Hubble metrics to work, Hubble itself needs to be enabled with ``hubble."
"enabled=true``. See :ref:`Hubble exported metrics<hubble_exported_metrics>` for "
"the list of available Hubble metrics."
msgstr ""

#: ../../gettingstarted/grafana.rst:64
msgid "Refer to :ref:`metrics` for more details about the individual metrics."
msgstr ""

#: ../../gettingstarted/grafana.rst:68
msgid "Deploy Cilium via Helm as follows to enable all metrics:"
msgstr ""

#: ../../gettingstarted/grafana.rst:81
msgid ""
"You can combine the above Helm options with any of the other installation "
"guides."
msgstr ""

#: ../../gettingstarted/grafana.rst:86
msgid "How to access Grafana"
msgstr ""

#: ../../gettingstarted/grafana.rst:88 ../../gettingstarted/grafana.rst:99
msgid "Expose the port on your local machine"
msgstr ""

#: ../../gettingstarted/grafana.rst:94
msgid "Access it via your browser: http://localhost:3000"
msgstr ""

#: ../../gettingstarted/grafana.rst:97
msgid "How to access Prometheus"
msgstr ""

#: ../../gettingstarted/grafana.rst:105
msgid "Access it via your browser: http://localhost:9090"
msgstr ""

#: ../../gettingstarted/grafana.rst:108
msgid "Examples"
msgstr ""

#: ../../gettingstarted/grafana.rst:116
msgid "Network"
msgstr ""

#: ../../gettingstarted/grafana.rst:121
msgid "Policy"
msgstr ""

#: ../../gettingstarted/grafana.rst:127
msgid "Endpoints"
msgstr ""

#: ../../gettingstarted/grafana.rst:132
msgid "Controllers"
msgstr ""

#: ../../gettingstarted/grafana.rst:137
msgid "Kubernetes"
msgstr ""

#: ../../gettingstarted/grafana.rst:142
msgid "Hubble General Processing"
msgstr ""

#: ../../gettingstarted/grafana.rst:147
msgid "Hubble Networking"
msgstr ""

#: ../../gettingstarted/grafana.rst:150
msgid ""
"The ``port-distribution`` metric is disabled by default. Refer to :ref:"
"`metrics` for more details about the individual metrics."
msgstr ""

#: ../../gettingstarted/grafana.rst:158
msgid "Hubble DNS"
msgstr ""

#: ../../gettingstarted/grafana.rst:163
msgid "Hubble HTTP"
msgstr ""

#: ../../gettingstarted/grafana.rst:168
msgid "Hubble Network Policy"
msgstr ""

#: ../../gettingstarted/grpc.rst:9
msgid "How to secure gRPC"
msgstr ""

#: ../../gettingstarted/grpc.rst:11
msgid ""
"This document serves as an introduction to using Cilium to enforce gRPC-aware "
"security policies.  It is a detailed walk-through of getting a single-node "
"Cilium environment running on your machine. It is designed to take 15-30 "
"minutes."
msgstr ""

#: ../../gettingstarted/grpc.rst:18
msgid ""
"It is important for this demo that ``kube-dns`` is working correctly. To know "
"the status of ``kube-dns`` you can run the following command:"
msgstr ""

#: ../../gettingstarted/grpc.rst:32
msgid ""
"Now that we have Cilium deployed and ``kube-dns`` operating correctly we can "
"deploy our demo gRPC application.  Since our first demo of Cilium + HTTP-aware "
"security policies was Star Wars-themed, we decided to do the same for gRPC. "
"While the `HTTP-aware Cilium  Star Wars demo <https://cilium.io/blog/2017/5/4/"
"demo-may-the-force-be-with-you/>`_ showed how the Galactic Empire used HTTP-"
"aware security policies to protect the Death Star from the Rebel Alliance, this "
"gRPC demo shows how the lack of gRPC-aware security policies allowed Leia, "
"Chewbacca, Lando, C-3PO, and R2-D2 to escape from Cloud City, which had been "
"overtaken by empire forces."
msgstr ""

#: ../../gettingstarted/grpc.rst:40
msgid ""
"`gRPC <https://grpc.io/>`_ is a high-performance RPC framework built on top of "
"the `protobuf <https://developers.google.com/protocol-buffers/>`_ serialization/"
"deserialization library popularized by Google.  There are gRPC bindings for "
"many programming languages, and the efficiency of the protobuf parsing as well "
"as advantages from leveraging HTTP 2 as a transport make it a popular RPC "
"framework for those building new microservices from scratch."
msgstr ""

#: ../../gettingstarted/grpc.rst:46
msgid ""
"For those unfamiliar with the details of the movie, Leia and the other rebels "
"are fleeing storm troopers and trying to reach the space port platform where "
"the Millennium Falcon is parked, so they can fly out of Cloud City. However, "
"the door to the platform is closed, and the access code has been changed. "
"However, R2-D2 is able to access the Cloud City computer system via a public "
"terminal, and disable this security, opening the door and letting the Rebels "
"reach the Millennium Falcon just in time to escape."
msgstr ""

#: ../../gettingstarted/grpc.rst:55
msgid ""
"In our example, Cloud City's internal computer system is built as a set of gRPC-"
"based microservices (who knew that gRPC was actually invented a long time ago, "
"in a galaxy far, far away?)."
msgstr ""

#: ../../gettingstarted/grpc.rst:59
msgid ""
"With gRPC, each service is defined using a language independent protocol buffer "
"definition. Here is the definition for the system used to manage doors within "
"Cloud City:"
msgstr ""

#: ../../gettingstarted/grpc.rst:86
msgid ""
"To keep the setup small, we will just launch two pods to represent this setup:"
msgstr ""

#: ../../gettingstarted/grpc.rst:88
msgid ""
"**cc-door-mgr**: A single pod running the gRPC door manager service with label "
"``app=cc-door-mgr``."
msgstr ""

#: ../../gettingstarted/grpc.rst:89
msgid ""
"**terminal-87**: One of the public network access terminals scattered across "
"Cloud City. R2-D2 plugs into terminal-87 as the rebels are desperately trying "
"to escape. This terminal uses the gRPC client code to communicate with the door "
"management services with label ``app=public-terminal``."
msgstr ""

#: ../../gettingstarted/grpc.rst:94
msgid ""
"The file ``cc-door-app.yaml`` contains a Kubernetes Deployment for the door "
"manager service, a Kubernetes Pod representing ``terminal-87``, and a "
"Kubernetes Service for the door manager services. To deploy this example app, "
"run:"
msgstr ""

#: ../../gettingstarted/grpc.rst:122
msgid "Test Access Between gRPC Client and Server"
msgstr ""

#: ../../gettingstarted/grpc.rst:124
msgid ""
"First, let's confirm that the public terminal can properly act as a client to "
"the door service.  We can test this by running a Python gRPC client for the "
"door service that exists in the *terminal-87* container."
msgstr ""

#: ../../gettingstarted/grpc.rst:128
msgid ""
"We'll invoke the 'cc_door_client' with the name of the gRPC method to call, and "
"any parameters (in this case, the door-id):"
msgstr ""

#: ../../gettingstarted/grpc.rst:139
msgid ""
"Exposing this information to public terminals seems quite useful, as it helps "
"travelers new to Cloud City identify and locate different doors. But recall "
"that the door service also exposes several other methods, including "
"``SetAccessCode``. If access to the door manager service is protected only "
"using traditional IP and port-based firewalling, the TCP port of the service "
"(50051 in this example) will be wide open to allow legitimate calls like "
"``GetName`` and ``GetLocation``, which also leave more sensitive calls like "
"``SetAccessCode`` exposed as well. It is this mismatch between the course "
"granularity of traditional firewalls and the fine-grained nature of gRPC calls "
"that R2-D2 exploited to override the security and help the rebels escape."
msgstr ""

#: ../../gettingstarted/grpc.rst:149
msgid "To see this, run:"
msgstr ""

#: ../../gettingstarted/grpc.rst:158
msgid "Securing Access to a gRPC Service with Cilium"
msgstr ""

#: ../../gettingstarted/grpc.rst:160
msgid ""
"Once the legitimate owners of Cloud City recover the city from the empire, how "
"can they use Cilium to plug this key security hole and block requests to "
"``SetAccessCode`` and ``GetStatus`` while still allowing ``GetName``, "
"``GetLocation``, and ``RequestMaintenance``?"
msgstr ""

#: ../../gettingstarted/grpc.rst:166
msgid ""
"Since gRPC build on top of HTTP, this can be achieved easily by understanding "
"how a gRPC call is mapped to an HTTP URL, and then applying a Cilium HTTP-aware "
"filter to allow public terminals to only invoke a subset of all the total gRPC "
"methods available on the door service."
msgstr ""

#: ../../gettingstarted/grpc.rst:171
msgid ""
"Each gRPC method is mapped to an HTTP POST call to a URL of the form ``/"
"cloudcity.DoorManager/<method-name>``."
msgstr ""

#: ../../gettingstarted/grpc.rst:174
msgid ""
"As a result, the following *CiliumNetworkPolicy* rule limits access of pods "
"with label ``app=public-terminal`` to only invoke ``GetName``, ``GetLocation``, "
"and ``RequestMaintenance`` on the door service, identified by label ``app=cc-"
"door-mgr``:"
msgstr ""

#: ../../gettingstarted/grpc.rst:182
msgid ""
"A *CiliumNetworkPolicy* contains a list of rules that define allowed requests, "
"meaning that requests that do not match any rules (e.g., ``SetAccessCode``) are "
"denied as invalid."
msgstr ""

#: ../../gettingstarted/grpc.rst:185
msgid ""
"The above rule applies to inbound (i.e., \"ingress\") connections to ``cc-door-"
"mgr pods`` (as indicated by ``app: cc-door-mgr`` in the \"endpointSelector\" "
"section). The rule will apply to connections from pods with label ``app: public-"
"terminal`` as indicated by the \"fromEndpoints\" section. The rule explicitly "
"matches gRPC connections destined to TCP 50051, and white-lists specifically "
"the permitted URLs."
msgstr ""

#: ../../gettingstarted/grpc.rst:192
msgid ""
"Apply this gRPC-aware network security policy using ``kubectl`` in the main "
"window:"
msgstr ""

#: ../../gettingstarted/grpc.rst:198
msgid ""
"After this security policy is in place, access to the innocuous calls like "
"``GetLocation`` still works as intended:"
msgstr ""

#: ../../gettingstarted/grpc.rst:207
msgid "However, if we then again try to invoke ``SetAccessCode``, it is denied:"
msgstr ""

#: ../../gettingstarted/grpc.rst:225
msgid ""
"This is now blocked, thanks to the Cilium network policy. And notice that "
"unlike a traditional firewall which would just drop packets in a way "
"indistinguishable from a network failure, because Cilium operates at the API-"
"layer, it can explicitly reply with an custom HTTP 403 Unauthorized error, "
"indicating that the request was intentionally denied for security reasons."
msgstr ""

#: ../../gettingstarted/grpc.rst:231
msgid ""
"Thank goodness that the empire IT staff hadn't had time to deploy Cilium on "
"Cloud City's internal network prior to the escape attempt, or things might have "
"turned out quite differently for Leia and the other Rebels!"
msgstr ""

#: ../../gettingstarted/grpc.rst:236
msgid "Clean-Up"
msgstr ""

#: ../../gettingstarted/grpc.rst:238
msgid ""
"You have now installed Cilium, deployed a demo app, and tested L7 gRPC-aware "
"network security policies. To clean-up, run:"
msgstr ""

#: ../../gettingstarted/gsg_sw_demo.rst:4
msgid ""
"Now that we have Cilium deployed and ``kube-dns`` operating correctly we can "
"deploy our demo application."
msgstr ""

#: ../../gettingstarted/gsg_sw_demo.rst:6
msgid ""
"In our Star Wars-inspired example, there are three microservices applications: "
"*deathstar*, *tiefighter*, and *xwing*. The *deathstar* runs an HTTP webservice "
"on port 80, which is exposed as a `Kubernetes Service <https://kubernetes.io/"
"docs/concepts/services-networking/service/>`_ to load-balance requests to "
"*deathstar* across two pod replicas. The *deathstar* service provides landing "
"services to the empire's spaceships so that they can request a landing port. "
"The *tiefighter* pod represents a landing-request client service on a typical "
"empire ship and *xwing* represents a similar service on an alliance ship. They "
"exist so that we can test different security policies for access control to "
"*deathstar* landing services."
msgstr ""

#: ../../gettingstarted/gsg_sw_demo.rst:8
msgid "**Application Topology for Cilium and Kubernetes**"
msgstr ""

#: ../../gettingstarted/gsg_sw_demo.rst:13
msgid ""
"The file ``http-sw-app.yaml`` contains a `Kubernetes Deployment <https://"
"kubernetes.io/docs/concepts/workloads/controllers/deployment/>`_ for each of "
"the three services. Each deployment is identified using the Kubernetes labels "
"(``org=empire, class=deathstar``), (``org=empire, class=tiefighter``), and "
"(``org=alliance, class=xwing``). It also includes a deathstar-service, which "
"load-balances traffic to all pods with label (``org=empire, class=deathstar``)."
msgstr ""

#: ../../gettingstarted/gsg_sw_demo.rst:27
msgid ""
"Kubernetes will deploy the pods and service in the background.  Running "
"``kubectl get pods,svc`` will inform you about the progress of the operation. "
"Each pod will go through several states until it reaches ``Running`` at which "
"point the pod is ready."
msgstr ""

#: ../../gettingstarted/gsg_sw_demo.rst:45
msgid ""
"Each pod will be represented in Cilium as an :ref:`endpoint`. We can invoke the "
"``cilium`` tool inside the Cilium pod to list them:"
msgstr ""

#: ../../gettingstarted/gsg_sw_demo.rst:89
msgid ""
"Both ingress and egress policy enforcement is still disabled on all of these "
"pods because no network policy has been imported yet which select any of the "
"pods."
msgstr ""

#: ../../gettingstarted/host-firewall.rst:11
msgid "Host Firewall"
msgstr ""

#: ../../gettingstarted/host-firewall.rst:13
msgid ""
"This document serves as an introduction to Cilium's host firewall, to enforce "
"security policies for Kubernetes nodes."
msgstr ""

#: ../../gettingstarted/host-firewall.rst:17
msgid "Enable the Host Firewall in Cilium"
msgstr ""

#: ../../gettingstarted/host-firewall.rst:30
msgid ""
"The ``devices`` flag refers to the network devices Cilium is configured on such "
"as ``eth0``. Omitting this option leads Cilium to auto-detect what interfaces "
"the host firewall applies to."
msgstr ""

#: ../../gettingstarted/host-firewall.rst:34
msgid ""
"At this point, the Cilium-managed nodes are ready to enforce network policies."
msgstr ""

#: ../../gettingstarted/host-firewall.rst:38
msgid "Attach a Label to the Node"
msgstr ""

#: ../../gettingstarted/host-firewall.rst:40
msgid ""
"In this guide, we will apply host policies only to nodes with the label ``node-"
"access=ssh``. We thus first need to attach that label to a node in the cluster."
msgstr ""

#: ../../gettingstarted/host-firewall.rst:52
msgid "Enable Policy Audit Mode for the Host Endpoint"
msgstr ""

#: ../../gettingstarted/host-firewall.rst:54
msgid ""
"`HostPolicies` enforce access control over connectivity to and from nodes. "
"Particular care must be taken to ensure that when host policies are imported, "
"Cilium does not block access to the nodes or break the cluster's normal "
"behavior (for example by blocking communication with ``kube-apiserver``)."
msgstr ""

#: ../../gettingstarted/host-firewall.rst:59
msgid ""
"To avoid such issues, we can switch the host firewall in audit mode, to "
"validate the impact of host policies before enforcing them. When Policy Audit "
"Mode is enabled, no network policy is enforced so this setting is *not "
"recommended for production deployment*."
msgstr ""

#: ../../gettingstarted/host-firewall.rst:76
msgid "Apply a Host Network Policy"
msgstr ""

#: ../../gettingstarted/host-firewall.rst:78
msgid ""
"`HostPolicies` match on node labels using a :ref:`NodeSelector` to identify the "
"nodes to which the policy applies. The following policy applies to all nodes. "
"It allows communications from outside the cluster only on port TCP/22. All "
"communications from the cluster to the hosts are allowed."
msgstr ""

#: ../../gettingstarted/host-firewall.rst:83
msgid ""
"Host policies don't apply to communications between pods or between pods and "
"the outside of the cluster, except if those pods are host-networking pods."
msgstr ""

#: ../../gettingstarted/host-firewall.rst:88
#: ../../gettingstarted/policy-creation.rst:66
msgid "To apply this policy, run:"
msgstr ""

#: ../../gettingstarted/host-firewall.rst:95
msgid ""
"The host is represented as a special endpoint, with label ``reserved:host``, in "
"the output of command ``cilium endpoint list``. You can therefore inspect the "
"status of the policy using that command."
msgstr ""

#: ../../gettingstarted/host-firewall.rst:114
msgid "Adjust the Host Policy to Your Environment"
msgstr ""

#: ../../gettingstarted/host-firewall.rst:116
msgid ""
"As long as the host endpoint is running in audit mode, communications "
"disallowed by the policy won't be dropped. They will however be reported by "
"``cilium monitor`` as ``action audit``. The audit mode thus allows you to "
"adjust the host policy to your environment, to avoid unexpected connection "
"breakages."
msgstr ""

#: ../../gettingstarted/host-firewall.rst:129
msgid ""
"For details on how to derive the network policies from the output of ``cilium "
"monitor``, please refer to `observe_policy_verdicts` and "
"`create_network_policy` in the `policy_verdicts` guide."
msgstr ""

#: ../../gettingstarted/host-firewall.rst:133
msgid ""
"In particular, `Entities based` rules are convenient for example to allow "
"communication to entire classes of destinations, such as all remotes nodes "
"(``remote-node``) or the entire cluster (``cluster``)."
msgstr ""

#: ../../gettingstarted/host-firewall.rst:139
msgid ""
"Make sure that none of the communications required to access the cluster or for "
"the cluster to work properly are denied. They should appear as ``action allow``."
msgstr ""

#: ../../gettingstarted/host-firewall.rst:146
#: ../../gettingstarted/policy-creation.rst:169
msgid "Disable Policy Audit Mode"
msgstr ""

#: ../../gettingstarted/host-firewall.rst:148
msgid ""
"Once you are confident all required communication to the host from outside the "
"cluster are allowed, you can disable policy audit mode to enforce the host "
"policy."
msgstr ""

#: ../../gettingstarted/host-firewall.rst:157
msgid "Ingress host policies should now appear as enforced:"
msgstr ""

#: ../../gettingstarted/host-firewall.rst:173
msgid ""
"Communications not explicitly allowed by the host policy will now be dropped:"
msgstr ""

#: ../../gettingstarted/host-services.rst:11
msgid "Host-Reachable Services"
msgstr ""

#: ../../gettingstarted/host-services.rst:13
msgid ""
"This guide explains how to configure Cilium to enable services to be reached "
"from the host namespace in addition to pod namespaces."
msgstr ""

#: ../../gettingstarted/host-services.rst:18
msgid ""
"Host-reachable services for TCP and UDP requires a v4.19.57, v5.1.16, v5.2.0 or "
"more recent Linux kernel. Note that v5.0.y kernels do not have the fix required "
"to run host-reachable services with UDP since at this point in time the v5.0.y "
"stable kernel is end-of-life (EOL) and not maintained anymore. For only "
"enabling TCP-based host-reachable services a v4.17.0 or newer kernel is "
"required. The most optimal kernel with the full feature set is v5.8."
msgstr ""

#: ../../gettingstarted/host-services.rst:35
msgid ""
"If you can't run 4.19.57 but have 4.17.0 available you can restrict protocol "
"support to TCP only:"
msgstr ""

#: ../../gettingstarted/host-services.rst:45
msgid ""
"Host-reachable services act transparent to Cilium's lower layer datapath in "
"that upon connect system call (TCP, connected UDP) or sendmsg as well as "
"recvmsg (UDP) the destination IP is checked for an existing service IP and one "
"of the service backends is selected as a target, meaning, while the application "
"is assuming its connection to the service address, the corresponding kernel's "
"socket is actually connected to the backend address and therefore no additional "
"lower layer NAT is required."
msgstr ""

#: ../../gettingstarted/host-services.rst:53
msgid "Verify that it has come up correctly:"
msgstr ""

#: ../../gettingstarted/host-services.rst:64
msgid ""
"The kernel eBPF cgroup hooks operate at connect(2), sendmsg(2) and recvmsg(2) "
"system call layers for connecting the application to one of the service "
"backends. In the v5.8 Linux kernel, a getpeername(2) hook for eBPF has been "
"added in order to also reverse translate the connected sock addresses for "
"application's getpeername(2) calls in Cilium. For kernels older than v5.8 such "
"reverse translation is not taking place for this system call. For the vast "
"majority of applications not having this translation at getpeername(2) does not "
"cause any issues. There is one known case for libceph where its monitor might "
"return an error since expected peer address mismatches."
msgstr ""

#: ../../gettingstarted/http.rst:11
msgid "Identity-Aware and HTTP-Aware Policy Enforcement"
msgstr ""

#: ../../gettingstarted/http.rst:17
msgid "Check Current Access"
msgstr ""

#: ../../gettingstarted/http.rst:18
msgid ""
"From the perspective of the *deathstar* service, only the ships with label "
"``org=empire`` are allowed to connect and request landing. Since we have no "
"rules enforced, both *xwing* and *tiefighter* will be able to request landing. "
"To test this, use the commands below."
msgstr ""

#: ../../gettingstarted/http.rst:28
msgid "Apply an L3/L4 Policy"
msgstr ""

#: ../../gettingstarted/http.rst:30
msgid ""
"When using Cilium, endpoint IP addresses are irrelevant when defining security "
"policies. Instead, you can use the labels assigned to the pods to define "
"security policies. The policies will be applied to the right pods based on the "
"labels irrespective of where or when it is running within the cluster."
msgstr ""

#: ../../gettingstarted/http.rst:34
msgid ""
"We'll start with the basic policy restricting deathstar landing requests to "
"only the ships that have label (``org=empire``). This will not allow any ships "
"that don't have the ``org=empire`` label to even connect with the *deathstar* "
"service. This is a simple policy that filters only on IP protocol (network "
"layer 3) and TCP protocol (network layer 4), so it is often referred to as an "
"L3/L4 network security policy."
msgstr ""

#: ../../gettingstarted/http.rst:37
msgid ""
"Note: Cilium performs stateful *connection tracking*, meaning that if policy "
"allows the frontend to reach backend, it will automatically allow all required "
"reply packets that are part of backend replying to frontend within the context "
"of the same TCP/UDP connection."
msgstr ""

#: ../../gettingstarted/http.rst:42
msgid "**L4 Policy with Cilium and Kubernetes**"
msgstr ""

#: ../../gettingstarted/http.rst:47
msgid "We can achieve that with the following CiliumNetworkPolicy:"
msgstr ""

#: ../../gettingstarted/http.rst:51
msgid ""
"CiliumNetworkPolicies match on pod labels using an \"endpointSelector\" to "
"identify the sources and destinations to which the policy applies. The above "
"policy whitelists traffic sent from any pods with label (``org=empire``) to "
"*deathstar* pods with label (``org=empire, class=deathstar``) on TCP port 80."
msgstr ""

#: ../../gettingstarted/http.rst:54 ../../gettingstarted/policy-creation.rst:142
msgid "To apply this L3/L4 policy, run:"
msgstr ""

#: ../../gettingstarted/http.rst:62 ../../gettingstarted/policy-creation.rst:198
msgid ""
"Now if we run the landing requests again, only the *tiefighter* pods with the "
"label ``org=empire`` will succeed. The *xwing* pods will be blocked!"
msgstr ""

#: ../../gettingstarted/http.rst:69 ../../gettingstarted/policy-creation.rst:206
msgid ""
"This works as expected. Now the same request run from an *xwing* pod will fail:"
msgstr ""

#: ../../gettingstarted/http.rst:75 ../../gettingstarted/policy-creation.rst:212
msgid ""
"This request will hang, so press Control-C to kill the curl request, or wait "
"for it to time out."
msgstr ""

#: ../../gettingstarted/http.rst:78
msgid "Inspecting the Policy"
msgstr ""

#: ../../gettingstarted/http.rst:80
msgid ""
"If we run ``cilium endpoint list`` again we will see that the pods with the "
"label ``org=empire`` and ``class=deathstar`` now have ingress policy "
"enforcement enabled as per the policy above."
msgstr ""

#: ../../gettingstarted/http.rst:119
msgid "You can also inspect the policy details via ``kubectl``"
msgstr ""

#: ../../gettingstarted/http.rst:175
msgid "Apply and Test HTTP-aware L7 Policy"
msgstr ""

#: ../../gettingstarted/http.rst:177
msgid ""
"In the simple scenario above, it was sufficient to either give *tiefighter* / "
"*xwing* full access to *deathstar's* API or no access at all. But to provide "
"the strongest security (i.e., enforce least-privilege isolation) between "
"microservices, each service that calls *deathstar's* API should be limited to "
"making only the set of HTTP requests it requires for legitimate operation."
msgstr ""

#: ../../gettingstarted/http.rst:184
msgid ""
"For example, consider that the *deathstar* service exposes some maintenance "
"APIs which should not be called by random empire ships. To see this run:"
msgstr ""

#: ../../gettingstarted/http.rst:200
msgid ""
"While this is an illustrative example, unauthorized access such as above can "
"have adverse security repercussions."
msgstr ""

#: ../../gettingstarted/http.rst:202
msgid "**L7 Policy with Cilium and Kubernetes**"
msgstr ""

#: ../../gettingstarted/http.rst:207
msgid ""
"Cilium is capable of enforcing HTTP-layer (i.e., L7) policies to limit what "
"URLs the *tiefighter* is allowed to reach.  Here is an example policy file that "
"extends our original policy by limiting *tiefighter* to making only a POST /v1/"
"request-landing API call, but disallowing all other calls (including PUT /v1/"
"exhaust-port)."
msgstr ""

#: ../../gettingstarted/http.rst:214
msgid ""
"Update the existing rule to apply L7-aware policy to protect *deathstar* using:"
msgstr ""

#: ../../gettingstarted/http.rst:222
msgid ""
"We can now re-run the same test as above, but we will see a different outcome:"
msgstr ""

#: ../../gettingstarted/http.rst:230
msgid "and"
msgstr ""

#: ../../gettingstarted/http.rst:237
msgid ""
"As this rule builds on the identity-aware rule, traffic from pods without the "
"label ``org=empire`` will continue to be dropped causing the connection to time "
"out:"
msgstr ""

#: ../../gettingstarted/http.rst:245
msgid ""
"As you can see, with Cilium L7 security policies, we are able to permit "
"*tiefighter* to access only the required API resources on *deathstar*, thereby "
"implementing a \"least privilege\" security approach for communication between "
"microservices. Note that ``path`` matches the exact url, if for example you "
"want to allow anything under /v1/, you need to use a regular expression:"
msgstr ""

#: ../../gettingstarted/http.rst:255
msgid "You can observe the L7 policy via ``kubectl``:"
msgstr ""

#: ../../gettingstarted/http.rst:312
msgid "and ``cilium`` CLI:"
msgstr ""

#: ../../gettingstarted/http.rst:382
msgid ""
"It is also possible to monitor the HTTP requests live by using ``cilium "
"monitor``:"
msgstr ""

#: ../../gettingstarted/http.rst:390
msgid ""
"The above output demonstrates a successful response to a POST request followed "
"by a PUT request that is denied by the L7 policy."
msgstr ""

#: ../../gettingstarted/http.rst:392
msgid ""
"We hope you enjoyed the tutorial.  Feel free to play more with the setup, read "
"the rest of the documentation, and reach out to us on the `Cilium Slack channel "
"<https://cilium.herokuapp.com>`_ with any questions!"
msgstr ""

#: ../../gettingstarted/hubble.rst:12
msgid "Service Map & Hubble UI"
msgstr ""

#: ../../gettingstarted/hubble.rst:14
msgid ""
"This tutorial guides you through enabling the Hubble UI to access the graphical "
"service map."
msgstr ""

#: ../../gettingstarted/hubble.rst:21 ../../gettingstarted/hubble_cli.rst:22
msgid ""
"This guide assumes that Cilium has been correctly installed in your Kubernetes "
"cluster and that Hubble has been enabled. Please see :ref:`k8s_quick_install` "
"and :ref:`hubble_setup` for more information. If unsure, run ``cilium status`` "
"and validate that Cilium and Hubble are up and running."
msgstr ""

#: ../../gettingstarted/hubble.rst:28
msgid "Enable the Hubble UI"
msgstr ""

#: ../../gettingstarted/hubble.rst:30
msgid ""
"If you have not done so already, enable the Hubble UI by running the following "
"command:"
msgstr ""

#: ../../gettingstarted/hubble.rst:55
msgid "Helm (Standalone install)"
msgstr ""

#: ../../gettingstarted/hubble.rst:57
msgid ""
"Clusters sometimes come with Cilium, Hubble, and Hubble relay already "
"installed. When this is the case you can still use Helm to install only Hubble "
"UI on top of the pre-installed components."
msgstr ""

#: ../../gettingstarted/hubble.rst:60
msgid ""
"You will need to set ``hubble.ui.standalone.enabled`` to ``true`` and "
"optionally provide a volume to mount Hubble UI client certificates if TLS is "
"enabled on Hubble Relay server side."
msgstr ""

#: ../../gettingstarted/hubble.rst:63
msgid ""
"Below is an example deploying Hubble UI as standalone, with client certificates "
"mounted from a ``my-hubble-ui-client-certs`` secret:"
msgstr ""

#: ../../gettingstarted/hubble.rst:105
msgid ""
"Please note that Hubble UI expects the certificate files to be available under "
"the following paths:"
msgstr ""

#: ../../gettingstarted/hubble.rst:116
msgid "Keep this in mind when providing the volume containing the certificate."
msgstr ""

#: ../../gettingstarted/hubble.rst:120
msgid "Open the Hubble UI"
msgstr ""

#: ../../gettingstarted/hubble.rst:122
msgid ""
"Open the Hubble UI in your browser by running ``cilium hubble ui``. It will "
"automatically set up a port forward to the hubble-ui service in your Kubernetes "
"cluster and make it available on a local port on your machine."
msgstr ""

#: ../../gettingstarted/hubble.rst:134
msgid ""
"The above command will block and continue running while the port forward is "
"active. You can interrupt the command to abort the port forward and re-run the "
"command to make the UI accessible again."
msgstr ""

#: ../../gettingstarted/hubble.rst:138
msgid ""
"If your browser has not automatically opened the UI, open the page http://"
"localhost:12000 in your browser. You should see a screen with an invitation to "
"select a namespace, use the namespace selector dropdown on the left top corner "
"to select a namespace:"
msgstr ""

#: ../../gettingstarted/hubble.rst:145
msgid ""
"In this example, we are deploying the Star Wars demo from the :ref:`gs_http` "
"guide. However you can apply the same techniques to observe application "
"connectivity dependencies in your own namespace, and clusters for application "
"of any type."
msgstr ""

#: ../../gettingstarted/hubble.rst:150
msgid ""
"Once the deployment is ready, issue a request from both spaceships to emulate "
"some traffic."
msgstr ""

#: ../../gettingstarted/hubble.rst:160
msgid ""
"These requests will then be displayed in the UI as service dependencies between "
"the different pods:"
msgstr ""

#: ../../gettingstarted/hubble.rst:165
msgid ""
"In the bottom of the interface, you may also inspect each recent Hubble flow "
"event in your current namespace individually."
msgstr ""

#: ../../gettingstarted/hubble.rst:169
msgid "Inspecting a wide variety of network traffic"
msgstr ""

#: ../../gettingstarted/hubble.rst:171
msgid ""
"In order to generate some network traffic, run the connectivity test in a loop:"
msgstr ""

#: ../../gettingstarted/hubble.rst:177
msgid ""
"To see the traffic in Hubble, open http://localhost:12000/cilium-test in your "
"browser."
msgstr ""

#: ../../gettingstarted/hubble-install.rst:5
#: ../../gettingstarted/hubble-install.rst:17
#: ../../gettingstarted/hubble-install.rst:29
msgid "Download the latest hubble release:"
msgstr ""

#: ../../gettingstarted/hubble-install.rst:15
#: ../../gettingstarted/k8s-install-kops.rst:49
msgid "MacOS"
msgstr ""

#: ../../gettingstarted/hubble-install.rst:27
msgid "Windows"
msgstr ""

#: ../../gettingstarted/hubble-install.rst:42
msgid ""
"and move the ``hubble.exe`` CLI to a directory listed in the ``%PATH%`` "
"environment variable after extracting it from the tarball."
msgstr ""

#: ../../gettingstarted/hubble_cli.rst:11
msgid "Inspecting Network Flows with the CLI"
msgstr ""

#: ../../gettingstarted/hubble_cli.rst:13
msgid ""
"This guide walks you through using the Hubble CLI to inspect network flows and "
"gain visibility into what is happening on the network level."
msgstr ""

#: ../../gettingstarted/hubble_cli.rst:30
msgid ""
"This guide uses examples based on the Demo App. If you would like to run them, "
"deploy the Demo App first. Please refer to :ref:`gs_http` for more details."
msgstr ""

#: ../../gettingstarted/hubble_cli.rst:34
msgid "Inspecting the cluster's network traffic with Hubble Relay"
msgstr ""

#: ../../gettingstarted/hubble_cli.rst:36
msgid ""
"Let's issue some requests to emulate some traffic again. This first request is "
"allowed by the policy."
msgstr ""

#: ../../gettingstarted/hubble_cli.rst:44
msgid "This next request is accessing an HTTP endpoint which is denied by policy."
msgstr ""

#: ../../gettingstarted/hubble_cli.rst:51
msgid ""
"Finally, this last request will hang because the ``xwing`` pod does not have "
"the ``org=empire`` label required by policy. Press Control-C to kill the curl "
"request, or wait for it to time out."
msgstr ""

#: ../../gettingstarted/hubble_cli.rst:60
msgid ""
"Let's now inspect this traffic using the CLI. The command below filters all "
"traffic on the application layer (L7, HTTP) to the ``deathstar`` pod:"
msgstr ""

#: ../../gettingstarted/hubble_cli.rst:72
msgid ""
"The following command shows all traffic to the ``deathstar`` pod that has been "
"dropped:"
msgstr ""

#: ../../gettingstarted/hubble_cli.rst:83
msgid ""
"Feel free to further inspect the traffic. To get help for the ``observe`` "
"command, use ``hubble help observe``."
msgstr ""

#: ../../gettingstarted/hubble_setup.rst:11
msgid "Setting up Hubble Observability"
msgstr ""

#: ../../gettingstarted/hubble_setup.rst:13
msgid ""
"Hubble is the observability layer of Cilium and can be used to obtain cluster-"
"wide visibility into the network and security layer of your Kubernetes cluster. "
"For more information about Hubble and its components, see the :ref:"
"`concepts_observability` section."
msgstr ""

#: ../../gettingstarted/hubble_setup.rst:26
msgid "Enable Hubble in Cilium"
msgstr ""

#: ../../gettingstarted/hubble_setup.rst:32
msgid ""
"In order to enable Hubble, run the command ``cilium hubble enable`` as shown "
"below:"
msgstr ""

#: ../../gettingstarted/hubble_setup.rst:57
msgid ""
"Enabling Hubble requires the TCP port 4244 to be open on all nodes running "
"Cilium. This is required for Relay to operate correctly."
msgstr ""

#: ../../gettingstarted/hubble_setup.rst:60
msgid "Run ``cilium status`` to validate that Hubble is enabled and running:"
msgstr ""

#: ../../gettingstarted/hubble_setup.rst:84
msgid ""
"If you installed Cilium via ``helm install``, you may enable Hubble Relay and "
"UI with the following command:"
msgstr ""

#: ../../gettingstarted/hubble_setup.rst:96
msgid "Install the Hubble Client"
msgstr ""

#: ../../gettingstarted/hubble_setup.rst:98
msgid ""
"In order to access the observability data collected by Hubble, install the "
"Hubble CLI:"
msgstr ""

#: ../../gettingstarted/hubble_setup.rst:104
msgid "Validate Hubble API Access"
msgstr ""

#: ../../gettingstarted/hubble_setup.rst:106
msgid ""
"In order to access the Hubble API, create a port forward to the Hubble service "
"from your local machine. This will allow you to connect the Hubble client to "
"the local port ``4245`` and access the Hubble Relay service in your Kubernetes "
"cluster. For more information on this method, see `Use Port Forwarding to "
"Access Application in a Cluster <https://kubernetes.io/docs/tasks/access-"
"application-cluster/port-forward-access-application-cluster/>`_."
msgstr ""

#: ../../gettingstarted/hubble_setup.rst:117
msgid ""
"Now you can validate that you can access the Hubble API via the installed CLI:"
msgstr ""

#: ../../gettingstarted/hubble_setup.rst:127
msgid "You can also query the flow API and look for flows:"
msgstr ""

#: ../../gettingstarted/hubble_setup.rst:135
msgid ""
"If you port forward to a port other than ``4245``, make sure to use the ``--"
"server`` flag or ``HUBBLE_SERVER`` environment variable to set the Hubble "
"server address (default: ``localhost:4245``). For more information, check out "
"Hubble CLI's help message by running ``hubble help status`` or ``hubble help "
"observe`` as well as ``hubble config`` for  configuring Hubble CLI."
msgstr ""

#: ../../gettingstarted/index.rst:10
msgid "Getting Started Guides"
msgstr ""

#: ../../gettingstarted/index.rst:12
msgid ""
"The following is a list of guides that help you get started with Cilium. The "
"guides cover the installation and then dive into more detailed topics such as "
"securing clusters, connecting multiple clusters, monitoring, and "
"troubleshooting. If you are new to Cilium it is recommended to read the :ref:"
"`intro` section first to learn about the basic concepts and motivation."
msgstr ""

#: ../../gettingstarted/index.rst:21
msgid "Installation"
msgstr ""

#: ../../gettingstarted/index.rst:33
msgid "Observability"
msgstr ""

#: ../../gettingstarted/index.rst:44
msgid "Network Policy Security Tutorials"
msgstr ""

#: ../../gettingstarted/index.rst:63
msgid "Advanced Networking"
msgstr ""

#: ../../gettingstarted/index.rst:97
msgid "Operations"
msgstr ""

#: ../../gettingstarted/index.rst:106
msgid "Istio"
msgstr ""

#: ../../gettingstarted/ipam.rst:11
msgid "Configuring IPAM modes"
msgstr ""

#: ../../gettingstarted/ipam-cluster-pool.rst:3
msgid ""
"WARNING: You are looking at unreleased Cilium documentation. Please use the "
"official rendered version released here: http://docs.cilium.io"
msgstr ""

#: ../../gettingstarted/ipam-cluster-pool.rst:11
msgid "CRD-backed by Cilium cluster-pool IPAM"
msgstr ""

#: ../../gettingstarted/ipam-cluster-pool.rst:13
msgid ""
"This is a quick tutorial walking through how to enable CRD-backed by Cilium "
"cluster-pool IPAM. The purpose of this tutorial is to show how components are "
"configured and resources interact with each other to enable users to automate "
"or extend on their own."
msgstr ""

#: ../../gettingstarted/ipam-cluster-pool.rst:18
msgid "For more details, see the section :ref:`ipam_crd_cluster_pool`"
msgstr ""

#: ../../gettingstarted/ipam-cluster-pool.rst:21
msgid "Enable Cluster-pool IPAM mode"
msgstr ""

#: ../../gettingstarted/ipam-cluster-pool.rst:23
msgid ""
"Setup Cilium for Kubernetes using helm with the options: ``--set ipam."
"mode=cluster-pool``."
msgstr ""

#: ../../gettingstarted/ipam-cluster-pool.rst:25
msgid ""
"Depending if you are using IPv4 and / or IPv6, you might want to adjust the "
"``podCIDR`` allocated for your cluster's pods with the options:"
msgstr ""

#: ../../gettingstarted/ipam-cluster-pool.rst:28
msgid "``--set ipam.operator.clusterPoolIPv4PodCIDRList=<IPv4CIDR>``"
msgstr ""

#: ../../gettingstarted/ipam-cluster-pool.rst:29
msgid "``--set ipam.operator.clusterPoolIPv6PodCIDRList=<IPv6CIDR>``"
msgstr ""

#: ../../gettingstarted/ipam-cluster-pool.rst:31
msgid ""
"To adjust the CIDR size that should be allocated for each node you can use the "
"following options:"
msgstr ""

#: ../../gettingstarted/ipam-cluster-pool.rst:34
msgid "``--set ipam.operator.clusterPoolIPv4MaskSize=<IPv4MaskSize>``"
msgstr ""

#: ../../gettingstarted/ipam-cluster-pool.rst:35
msgid "``--set ipam.operator.clusterPoolIPv6MaskSize=<IPv6MaskSize>``"
msgstr ""

#: ../../gettingstarted/ipam-cluster-pool.rst:37
msgid ""
"Deploy Cilium and Cilium-Operator. Cilium will automatically wait until the "
"``podCIDR`` is allocated for its node by Cilium Operator."
msgstr ""

#: ../../gettingstarted/ipam-cluster-pool.rst:41
msgid "Validate installation"
msgstr ""

#: ../../gettingstarted/ipam-cluster-pool.rst:43
#: ../../gettingstarted/ipam-crd.rst:59
msgid "Validate that Cilium has started up correctly"
msgstr ""

#: ../../gettingstarted/ipam-cluster-pool.rst:55
msgid "Validate the ``spec.ipam.podCIDRs`` section:"
msgstr ""

#: ../../gettingstarted/ipam-crd.rst:11
msgid "CRD-backed IPAM"
msgstr ""

#: ../../gettingstarted/ipam-crd.rst:13
msgid ""
"This is a quick tutorial walking through how to enable CRD-backed IPAM. The "
"purpose of this tutorial is to show how components are configured and resources "
"interact with each other to enable users to automate or extend on their own."
msgstr ""

#: ../../gettingstarted/ipam-crd.rst:17
msgid "For more details, see the section :ref:`concepts_ipam_crd`"
msgstr ""

#: ../../gettingstarted/ipam-crd.rst:20
msgid "Enable CRD IPAM mode"
msgstr ""

#: ../../gettingstarted/ipam-crd.rst:22
msgid "Setup Cilium for Kubernetes using any of the available guides."
msgstr ""

#: ../../gettingstarted/ipam-crd.rst:23
msgid ""
"Run Cilium with the ``--ipam=crd`` option or set ``ipam: crd`` in the ``cilium-"
"config`` ConfigMap."
msgstr ""

#: ../../gettingstarted/ipam-crd.rst:25
msgid ""
"Restart Cilium. Cilium will automatically register the CRD if not available "
"already"
msgstr ""

#: ../../gettingstarted/ipam-crd.rst:31
msgid "Validate that the CRD has been registered:"
msgstr ""

#: ../../gettingstarted/ipam-crd.rst:41
msgid "Create a CiliumNode CR"
msgstr ""

#: ../../gettingstarted/ipam-crd.rst:43
msgid ""
"Import the following custom resource to make IPs available in the Cilium agent."
msgstr ""

#: ../../gettingstarted/ipam-crd.rst:71
msgid "Validate the ``status.IPAM.used`` section:"
msgstr ""

#: ../../gettingstarted/ipam-crd.rst:98
msgid ""
"At the moment only single IP addresses are allowed. CIDR's are not supported."
msgstr ""

#: ../../gettingstarted/istio.rst:11
msgid "Getting Started Using Istio"
msgstr ""

#: ../../gettingstarted/istio.rst:13
msgid ""
"This document serves as an introduction to using Cilium Istio integration to "
"enforce security policies in Kubernetes micro-services managed with Istio.  It "
"is a detailed walk-through of getting a single-node Cilium + Istio environment "
"running on your machine."
msgstr ""

#: ../../gettingstarted/istio.rst:18
msgid ""
"Cilium's Istio integration allows Cilium to enforce HTTP L7 network policies "
"for mTLS protected traffic within the Istio sidecar proxies. Note that Istio "
"can also be deployed without Cilium integration by running a standard version "
"of ``istioctl``.  In that case Cilium will enforce HTTP L7 policies outside of "
"the Istio sidecar proxy, but that will only work if mTLS is not used."
msgstr ""

#: ../../gettingstarted/istio.rst:29
msgid ""
"If running on minikube, make sure it is using a VM, as Istio ingress gateway "
"may not be reachable from your host otherwise. You may also need to up the "
"memory and CPUs available to the minikube VM from the defaults and/or the "
"instructions provided here for the other GSGs. 5 GB and 4 CPUs should be enough "
"for this GSG (``--vm=true --memory=5120 --cpus=4``)."
msgstr ""

#: ../../gettingstarted/istio.rst:38
msgid ""
"If Cilium is deployed with the kube-proxy replacement, you need to set ``bpf-lb-"
"sock-hostns-only: true`` in the deployment yaml directly or via ``hostServices."
"hostNamespaceOnly`` option with Helm. Without this option, when Cilium does "
"service resolution via socket load balancing, Istio sidecar will be bypassed, "
"resulting in loss of Istio features including encryption and telemetry."
msgstr ""

#: ../../gettingstarted/istio.rst:46
msgid "Step 2: Install cilium-istioctl"
msgstr ""

#: ../../gettingstarted/istio.rst:50
msgid "Make sure that Cilium is running in your cluster before proceeding."
msgstr ""

#: ../../gettingstarted/istio.rst:52
msgid ""
"Download the `cilium enhanced istioctl version 1.10.4 <https://github.com/"
"cilium/istio/releases/tag/1.10.4>`_:"
msgstr ""

#: ../../gettingstarted/istio.rst:55
msgid "Linux (amd64)"
msgstr ""

#: ../../gettingstarted/istio.rst:61
msgid "Linux (arm64)"
msgstr ""

#: ../../gettingstarted/istio.rst:67
msgid "OSX"
msgstr ""

#: ../../gettingstarted/istio.rst:73
msgid "OSX (Apple Silicon)"
msgstr ""

#: ../../gettingstarted/istio.rst:81
msgid ""
"Cilium integration, as presented in this Getting Started Guide, has been tested "
"with Kubernetes releases 1.17, 1.18, 1.19, 1.20, 1.21, 1.22, 1.23 and 1.24. "
"This Istio release does not work with Kubernetes 1.16 or older."
msgstr ""

#: ../../gettingstarted/istio.rst:85
msgid "Deploy the default Istio configuration profile onto Kubernetes:"
msgstr ""

#: ../../gettingstarted/istio.rst:91
msgid ""
"Add a namespace label to instruct Istio to automatically inject Envoy sidecar "
"proxies when you deploy your application later:"
msgstr ""

#: ../../gettingstarted/istio.rst:98
msgid "Step 3: Deploy the Bookinfo Application V1"
msgstr ""

#: ../../gettingstarted/istio.rst:100
msgid ""
"Now that we have Cilium and Istio deployed, we can deploy version ``v1`` of the "
"services of the `Istio Bookinfo sample application <https://istio.io/docs/"
"examples/bookinfo/>`_."
msgstr ""

#: ../../gettingstarted/istio.rst:104
msgid ""
"While the upstream `Istio Bookinfo Application example for Kubernetes <https://"
"istio.io/docs/examples/bookinfo/#if-you-are-running-on-kubernetes>`_ deploys "
"multiple versions of the Bookinfo application at the same time, here we first "
"deploy only the version 1."
msgstr ""

#: ../../gettingstarted/istio.rst:109
msgid "The BookInfo application is broken into four separate microservices:"
msgstr ""

#: ../../gettingstarted/istio.rst:111
msgid ""
"*productpage*. The productpage microservice calls the details and reviews "
"microservices to populate the page."
msgstr ""

#: ../../gettingstarted/istio.rst:113
msgid "*details*. The details microservice contains book information."
msgstr ""

#: ../../gettingstarted/istio.rst:114
msgid ""
"*reviews*. The reviews microservice contains book reviews. It also calls the "
"ratings microservice."
msgstr ""

#: ../../gettingstarted/istio.rst:116
msgid ""
"*ratings*. The ratings microservice contains book ranking information that "
"accompanies a book review."
msgstr ""

#: ../../gettingstarted/istio.rst:119
msgid ""
"In this demo, each specific version of each microservice is deployed into "
"Kubernetes using separate YAML files which define:"
msgstr ""

#: ../../gettingstarted/istio.rst:122
msgid "A Kubernetes Service."
msgstr ""

#: ../../gettingstarted/istio.rst:123
msgid ""
"A Kubernetes Deployment specifying the microservice's pods, specific to each "
"service version."
msgstr ""

#: ../../gettingstarted/istio.rst:125
msgid ""
"A Cilium Network Policy limiting the traffic to the microservice, specific to "
"each service version."
msgstr ""

#: ../../gettingstarted/istio.rst:132
msgid "To deploy the application with manual sidecar injection, run:"
msgstr ""

#: ../../gettingstarted/istio.rst:139 ../../gettingstarted/istio.rst:218
#: ../../gettingstarted/istio.rst:414
msgid ""
"Check the progress of the deployment (every service should have an "
"``AVAILABLE`` count of ``1``):"
msgstr ""

#: ../../gettingstarted/istio.rst:152
msgid ""
"Notes for Istio debugging: - Set istio proxy to debug or trace mode: kubectl "
"exec -it $(kubectl get pod -l app=productpage -o jsonpath='{.items[0].metadata."
"name}') -c istio-proxy -- curl -XPOST http://127.0.0.1:15000/logging?"
"level=debug - Check that bpf is mounted: kubectl exec -it $(kubectl get pod -l "
"app=productpage -o jsonpath='{.items[0].metadata.name}') -c istio-proxy -- ls -"
"la /sys/fs/bpf/tc/globals - Upgrade cilium to a local image (after setting "
"image pull policy to never): export CILIUM_TAG=my-test-tag docker save cilium/"
"cilium:${CILIUM_TAG} -o cilium-${CILIUM_TAG}.tar scp -i $(minikube ssh-key) "
"cilium-${CILIUM_TAG}.tar docker@$(minikube ip):. minikube ssh \"docker image "
"load -i cilium-${CILIUM_TAG}.tar\" kubectl -n kube-system set image daemonset/"
"cilium cilium-agent=docker.io/cilium/cilium:${CILIUM_TAG}"
msgstr ""

#: ../../gettingstarted/istio.rst:164
msgid "Create an Istio ingress gateway for the productpage service:"
msgstr ""

#: ../../gettingstarted/istio.rst:170
msgid "To obtain the URL to the frontend productpage service, run:"
msgstr ""

#: ../../gettingstarted/istio.rst:178
msgid ""
"Open that URL in your web browser and check that the application has been "
"successfully deployed.  It may take several seconds before all services become "
"accessible in the Istio service mesh, so you may have have to reload the page."
msgstr ""

#: ../../gettingstarted/istio.rst:184
msgid "Step 4: Canary and Deploy the Reviews Service V2"
msgstr ""

#: ../../gettingstarted/istio.rst:186
msgid ""
"We will now deploy version ``v2`` of the ``reviews`` service.  In addition to "
"providing reviews from readers, ``reviews v2`` queries a new ``ratings`` "
"service for book ratings, and displays each rating as 1 to 5 black stars."
msgstr ""

#: ../../gettingstarted/istio.rst:191
msgid ""
"As a precaution, we will use Istio's service routing feature to canary the "
"``v2`` deployment to prevent breaking the end-to-end application completely if "
"it is faulty."
msgstr ""

#: ../../gettingstarted/istio.rst:195
#, python-format
msgid ""
"Before deploying ``v2``, to prevent any traffic from being routed to it for "
"now, we will create this Istio route rules to route 100% of the ``reviews`` "
"traffic to ``v1``:"
msgstr ""

#: ../../gettingstarted/istio.rst:205 ../../gettingstarted/istio.rst:270
#: ../../gettingstarted/istio.rst:294
msgid "Apply this route rule:"
msgstr ""

#: ../../gettingstarted/istio.rst:211
msgid "Deploy the ``ratings v1`` and ``reviews v2`` services:"
msgstr ""

#: ../../gettingstarted/istio.rst:231
msgid ""
"Check in your web browser that no stars are appearing in the Book Reviews, even "
"after refreshing the page several times.  This indicates that all reviews are "
"retrieved from ``reviews v1`` and none from ``reviews v2``."
msgstr ""

#: ../../gettingstarted/istio.rst:240
msgid ""
"The ``ratings-v1`` CiliumNetworkPolicy explicitly whitelists access to the "
"``ratings`` API only from ``productpage`` and ``reviews v2``:"
msgstr ""

#: ../../gettingstarted/istio.rst:245
msgid ""
"Check that ``reviews v1`` may not be able to access the ``ratings`` service, "
"even if it were compromised or suffered from a bug, by running ``curl`` from "
"within the pod:"
msgstr ""

#: ../../gettingstarted/istio.rst:251
msgid ""
"All traffic from ``reviews v1`` to ``ratings`` is blocked, so the connection "
"attempt fails after the connection timeout."
msgstr ""

#: ../../gettingstarted/istio.rst:261
#, python-format
msgid ""
"Update the Istio route rule to send 50% of ``reviews`` traffic to ``v1`` and "
"50% to ``v2``:"
msgstr ""

#: ../../gettingstarted/istio.rst:276
#, python-format
msgid ""
"Check in your web browser that stars are appearing in the Book Reviews roughly "
"50% of the time.  This may require refreshing the page for a few seconds to "
"observe.  Queries to ``reviews v2`` result in reviews containing ratings "
"displayed as black stars:"
msgstr ""

#: ../../gettingstarted/istio.rst:285
#, python-format
msgid ""
"Finally, update the route rule to send 100% of ``reviews`` traffic to ``v2``:"
msgstr ""

#: ../../gettingstarted/istio.rst:300
msgid ""
"Refresh the product page in your web browser several times to verify that stars "
"are now appearing in the Book Reviews on every page refresh.  All the reviews "
"are now retrieved from ``reviews v2`` and none from ``reviews v1``."
msgstr ""

#: ../../gettingstarted/istio.rst:306
msgid "Step 5: Deploy the Product Page Service V2"
msgstr ""

#: ../../gettingstarted/istio.rst:308
msgid ""
"We will now deploy version ``v2`` of the ``productpage`` service, which brings "
"two changes:"
msgstr ""

#: ../../gettingstarted/istio.rst:311
msgid ""
"It is deployed with a more restrictive CiliumNetworkPolicy, which restricts "
"access to a subset of the HTTP URLs, at Layer-7."
msgstr ""

#: ../../gettingstarted/istio.rst:313
msgid "It implements a new authentication audit log into Kafka."
msgstr ""

#: ../../gettingstarted/istio.rst:319
msgid ""
"The policy for ``v1`` currently allows read access to the full HTTP REST API, "
"under the ``/api/v1`` HTTP URI path:"
msgstr ""

#: ../../gettingstarted/istio.rst:322
msgid "``/api/v1/products``: Returns the list of books and their details."
msgstr ""

#: ../../gettingstarted/istio.rst:323
msgid "``/api/v1/products/<id>``: Returns details about a specific book."
msgstr ""

#: ../../gettingstarted/istio.rst:324
msgid "``/api/v1/products/<id>/reviews``: Returns reviews for a specific book."
msgstr ""

#: ../../gettingstarted/istio.rst:326
msgid "``/api/v1/products/<id>/ratings``: Returns ratings for a specific book."
msgstr ""

#: ../../gettingstarted/istio.rst:329
msgid ""
"Check that the full REST API is currently accessible in ``v1`` and returns "
"valid JSON data:"
msgstr ""

#: ../../gettingstarted/istio.rst:337 ../../gettingstarted/istio.rst:435
msgid "The output will be similar to this::"
msgstr ""

#: ../../gettingstarted/istio.rst:347
msgid ""
"We realized that the REST API to get the book reviews and ratings was meant "
"only for consumption by other internal services, and will be blocked from "
"external clients using the updated Layer-7 CiliumNetworkPolicy in ``productpage "
"v2``, i.e. only the ``/api/v1/products`` and ``/api/v1/products/<id>`` HTTP "
"URLs will be whitelisted:"
msgstr ""

#: ../../gettingstarted/istio.rst:356
msgid ""
"Because ``productpage v2`` sends messages into Kafka, we must first deploy a "
"Kafka broker:"
msgstr ""

#: ../../gettingstarted/istio.rst:367
msgid ""
"Wait until the ``kafka-v1-0`` pod is ready, i.e. until it has a ``READY`` count "
"of ``1/1``:"
msgstr ""

#: ../../gettingstarted/istio.rst:376
msgid ""
"Create the ``authaudit`` Kafka topic, which will be used by ``productpage v2``:"
msgstr ""

#: ../../gettingstarted/istio.rst:383
msgid "We are now ready to deploy ``productpage v2``."
msgstr ""

#: ../../gettingstarted/istio.rst:385
msgid ""
"Create the ``productpage v2`` service and its updated CiliumNetworkPolicy and "
"delete ``productpage v1``:"
msgstr ""

#: ../../gettingstarted/istio.rst:396
msgid ""
"``productpage v2`` implements an authorization audit logging.  On every user "
"login or logout, it produces into Kafka topic ``authaudit`` a JSON-formatted "
"message which contains the following information:"
msgstr ""

#: ../../gettingstarted/istio.rst:400
msgid "event: ``login`` or ``logout``"
msgstr ""

#: ../../gettingstarted/istio.rst:401
msgid "username"
msgstr ""

#: ../../gettingstarted/istio.rst:402
msgid "client IP address"
msgstr ""

#: ../../gettingstarted/istio.rst:403
msgid "timestamp"
msgstr ""

#: ../../gettingstarted/istio.rst:405
msgid ""
"To observe the Kafka messages sent by ``productpage``, we will run an "
"additional ``authaudit-logger`` service.  This service fetches and prints out "
"all messages from the ``authaudit`` Kafka topic.  Start this service:"
msgstr ""

#: ../../gettingstarted/istio.rst:428
msgid ""
"Check that the product REST API is still accessible, and that Cilium now denies "
"at Layer-7 any access to the reviews and ratings REST API:"
msgstr ""

#: ../../gettingstarted/istio.rst:446
msgid ""
"This demonstrated that requests to the ``/api/v1/products/<id>/reviews`` and ``/"
"api/v1/products/<id>/ratings`` URIs now result in Cilium returning ``HTTP 403 "
"Forbidden`` HTTP responses."
msgstr ""

#: ../../gettingstarted/istio.rst:451
msgid ""
"Every login and logout on the product page will result in a line in this "
"service's log. Note that you need to log in/out using the ``sign in``/``sign "
"out`` element on the bookinfo web page. When you do, you can observe these kind "
"of audit logs:"
msgstr ""

#: ../../gettingstarted/istio.rst:469
msgid ""
"As you can see, the user-identifiable information sent by ``productpage`` in "
"every Kafka message is sensitive, so access to this Kafka topic must be "
"protected using Cilium.  The CiliumNetworkPolicy configured on the Kafka broker "
"enforces that:"
msgstr ""

#: ../../gettingstarted/istio.rst:474
msgid ""
"only ``productpage v2`` is allowed to produce messages into the ``authaudit`` "
"topic;"
msgstr ""

#: ../../gettingstarted/istio.rst:476
msgid "only ``authaudit-logger`` can fetch messages from this topic;"
msgstr ""

#: ../../gettingstarted/istio.rst:477
msgid "no service can access any other topic."
msgstr ""

#: ../../gettingstarted/istio.rst:481
msgid ""
"Check that Cilium prevents the ``authaudit-logger`` service from writing into "
"the ``authaudit`` topic (enter a message followed by ENTER, e.g. ``test "
"message``)"
msgstr ""

#: ../../gettingstarted/istio.rst:487
msgid "Note that the error message may take a short time to appear."
msgstr ""

#: ../../gettingstarted/istio.rst:491
msgid "You can terminate the command with a single ``<CTRL>-d``."
msgstr ""

#: ../../gettingstarted/istio.rst:500
msgid ""
"This demonstrated that Cilium sent a response with an authorization error for "
"any ``Produce`` request from this service."
msgstr ""

#: ../../gettingstarted/istio.rst:503
msgid ""
"Create another topic named ``credit-card-payments``, meant to transmit highly-"
"sensitive credit card payment requests:"
msgstr ""

#: ../../gettingstarted/istio.rst:510
msgid ""
"Check that Cilium prevents the ``authaudit-logger`` service from fetching "
"messages from this topic:"
msgstr ""

#: ../../gettingstarted/istio.rst:521
msgid ""
"This demonstrated that Cilium sent a response with an authorization error for "
"any ``Fetch`` request from this service for any topic other than ``authaudit``."
msgstr ""

#: ../../gettingstarted/istio.rst:527
msgid "At present, the above command may also result in an error message."
msgstr ""

#: ../../gettingstarted/istio.rst:530 ../../gettingstarted/memcached.rst:310
msgid "Step 6: Clean Up"
msgstr ""

#: ../../gettingstarted/istio.rst:532
msgid ""
"You have now installed Cilium and Istio, deployed a demo app, and tested both "
"Cilium's L3-L7 network security policies and Istio's service route rules.  To "
"clean up, run:"
msgstr ""

#: ../../gettingstarted/istio.rst:540
msgid "After this, you can re-run the tutorial from Step 0."
msgstr ""

#: ../../gettingstarted/k3s.rst:11
msgid "Getting Started Using K3s"
msgstr ""

#: ../../gettingstarted/k3s.rst:13
msgid ""
"This guide walks you through installation of Cilium on `K3s <https://k3s.io/"
">`_, a highly available, certified Kubernetes distribution designed for "
"production workloads in unattended, resource-constrained, remote locations or "
"inside IoT appliances."
msgstr ""

#: ../../gettingstarted/k3s.rst:18
msgid "Cilium is presently supported on amd64 and arm64 architectures."
msgstr ""

#: ../../gettingstarted/k3s.rst:21
msgid "Install a Master Node"
msgstr ""

#: ../../gettingstarted/k3s.rst:23
msgid ""
"The first step is to install a K3s master node making sure to disable support "
"for the default CNI plugin and the built-in network policy enforcer:"
msgstr ""

#: ../../gettingstarted/k3s.rst:31
msgid "Install Agent Nodes (Optional)"
msgstr ""

#: ../../gettingstarted/k3s.rst:33
msgid ""
"K3s can run in standalone mode or as a cluster making it a great choice for "
"local testing with multi-node data paths. Agent nodes are joined to the master "
"node using a node-token which can be found on the master node at ``/var/lib/"
"rancher/k3s/server/node-token``."
msgstr ""

#: ../../gettingstarted/k3s.rst:38
msgid ""
"Install K3s on agent nodes and join them to the master node making sure to "
"replace the variables with values from your environment:"
msgstr ""

#: ../../gettingstarted/k3s.rst:45 ../../gettingstarted/k8s-install-default.rst:23
msgid ""
"Should you encounter any issues during the installation, please refer to the :"
"ref:`troubleshooting_k8s` section and / or seek help on the :term:`Slack "
"channel`."
msgstr ""

#: ../../gettingstarted/k3s.rst:48
msgid ""
"Please consult the Kubernetes :ref:`k8s_requirements` for information on  how "
"you need to configure your Kubernetes cluster to operate with Cilium."
msgstr ""

#: ../../gettingstarted/k3s.rst:52
msgid "Configure Cluster Access"
msgstr ""

#: ../../gettingstarted/k3s.rst:54 ../../gettingstarted/requirements-k3s.rst:22
msgid ""
"For the Cilium CLI to access the cluster in successive steps you will need to "
"use the ``kubeconfig`` file stored at ``/etc/rancher/k3s/k3s.yaml`` by setting "
"the ``KUBECONFIG`` environment variable:"
msgstr ""

#: ../../gettingstarted/k3s.rst:63
#: ../../gettingstarted/k8s-install-default.rst:231
#: ../../gettingstarted/k8s-install-helm.rst:19 ../../gettingstarted/kind.rst:35
#: ../../gettingstarted/rancher-desktop.rst:22
msgid "Install Cilium"
msgstr ""

#: ../../gettingstarted/k3s.rst:67 ../../gettingstarted/k8s-install-rke.rst:62
#: ../../gettingstarted/rancher-desktop.rst:26
msgid "Install Cilium by running:"
msgstr ""

#: ../../gettingstarted/k8s-install-advanced.rst:12
msgid "Advanced Installation"
msgstr ""

#: ../../gettingstarted/k8s-install-advanced.rst:16
msgid ""
"The following guides cover advanced use cases. They cover detailed aspects of "
"certain platforms. For the standard installation path, see :ref:"
"`k8s_install_standard`."
msgstr ""

#: ../../gettingstarted/k8s-install-advanced.rst:34
msgid "External Installers"
msgstr ""

#: ../../gettingstarted/k8s-install-default.rst:13
msgid "Quick Installation"
msgstr "快速安装"

#: ../../gettingstarted/k8s-install-default.rst:15
msgid ""
"This guide will walk you through the quick default installation. It will "
"automatically detect and use the best configuration possible for the Kubernetes "
"distribution you are using. All state is stored using Kubernetes custom "
"resource definitions (CRDs)."
msgstr ""
"本指南将引导您完成快速完成默认配置的安装。 它将自动检测并使用您正在使用的 "
"Kubernetes 发行版的最佳配置，并且所有状态都使用 Kubernetes CRD 进行存储。"

#: ../../gettingstarted/k8s-install-default.rst:19
msgid ""
"This is the best installation method for most use cases.  For large "
"environments (> 500 nodes) or if you want to run specific datapath modes, refer "
"to the :ref:`k8s_install_advanced` guide."
msgstr ""
"以下是大多数时候的最佳安装方法。 对于大型规模集群（> 500 个节点）或者如果您想运"
"行特定的数据路径模式，请参阅:ref:`k8s_install_advanced` 指南。"

#: ../../gettingstarted/k8s-install-default.rst:29
msgid "Create the Cluster"
msgstr "创建集群"

#: ../../gettingstarted/k8s-install-default.rst:31
msgid ""
"If you don't have a Kubernetes Cluster yet, you can use the instructions below "
"to create a Kubernetes cluster locally or using a managed Kubernetes service:"
msgstr ""
"如果您还没有 Kubernetes 集群，您可以使用以下说明在本地或使用托管 Kubernetes 服务"
"创建 Kubernetes 集群："

#: ../../gettingstarted/k8s-install-default.rst:36
#: ../../gettingstarted/k8s-install-default.rst:255
#: ../../gettingstarted/k8s-install-helm.rst:51
msgid "GKE"
msgstr "GKE"

#: ../../gettingstarted/k8s-install-default.rst:38
msgid ""
"The following commands create a Kubernetes cluster using `Google Kubernetes "
"Engine <https://cloud.google.com/kubernetes-engine>`_.  See `Installing Google "
"Cloud SDK <https://cloud.google.com/sdk/install>`_ for instructions on how to "
"install ``gcloud`` and prepare your account."
msgstr ""
"以下命令使用 `Google Kubernetes Engine <https://cloud.google.com/kubernetes-"
"engine>`_ 创建 Kubernetes 集群。 请参阅`安装 Google Cloud SDK <https://cloud."
"google.com/sdk/install>`_ 以获取有关如何安装 ``gcloud`` 并准备您的帐户。"

#: ../../gettingstarted/k8s-install-default.rst:57
#: ../../gettingstarted/k8s-install-default.rst:187
#: ../../gettingstarted/kubeproxy-free.rst:674
msgid ""
"Please make sure to read and understand the documentation page on :ref:`taint "
"effects and unmanaged pods<taint_effects>`."
msgstr ""
"请确保阅读并理解 :ref:`taint effects 和 unmanaged pods<taint_effects>` 的文档页"
"面。"

#: ../../gettingstarted/k8s-install-default.rst:59
#: ../../gettingstarted/k8s-install-default.rst:267
#: ../../gettingstarted/k8s-install-helm.rst:84
msgid "AKS"
msgstr "AKS"

#: ../../gettingstarted/k8s-install-default.rst:61
msgid ""
"The following commands create a Kubernetes cluster using `Azure Kubernetes "
"Service <https://docs.microsoft.com/en-us/azure/aks/>`_. See `Azure Cloud CLI "
"<https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-"
"latest>`_ for instructions on how to install ``az`` and prepare your account."
msgstr ""
"以下命令使用`Azure Kubernetes Service <https://docs.microsoft.com/en-us/azure/"
"aks/>`_创建一个 Kubernetes 集群。 有关如何安装 `az` 的说明，请参阅`Azure Cloud "
"CLI <https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-"
"cli-latest>`_ ` 并准备好您的帐户。"

#: ../../gettingstarted/k8s-install-default.rst:67
msgid ""
"For more details about why node pools must be set up in this way on AKS, see "
"the note below the commands."
msgstr ""
"有关为什么必须在 AKS 上以这种方式设置节点池的更多详细信息，请参阅命令下方的注"
"释。"

#: ../../gettingstarted/k8s-install-default.rst:121
msgid ""
"Do NOT specify the ``--network-policy`` flag when creating the cluster, as this "
"will cause the Azure CNI plugin to install unwanted iptables rules."
msgstr ""
"创建集群时不要指定 —network-policy 标志，因为这会导致 Azure CNI 插件安装不需要"
"的 iptables 规则。"

#: ../../gettingstarted/k8s-install-default.rst:127
msgid ""
"`Node pools <https://aka.ms/aks/nodepools>`_ should be tainted with ``node."
"cilium.io/agent-not-ready=true:NoExecute`` to ensure that applications pods "
"will only be scheduled/executed once Cilium is ready to manage them. However, "
"there are other options. Please make sure to read and understand the "
"documentation page on :ref:`taint effects and unmanaged pods<taint_effects>`."
msgstr ""
"`Node pools <https://aka.ms/aks/nodepools>`_ 应该通过 `node.cilium.io/agent-not-"
"ready=true:NoExecute` 打上污点，以确保应用程序 pod 只会被Cilium调度并执行。 但是"
"您还有其他选择。 请确保阅读并理解 :ref:`taint effects 和 unmanaged "
"pods<taint_effects>` 的文档页面。"

#: ../../gettingstarted/k8s-install-default.rst:133
msgid "Additionally on AKS:"
msgstr "更多关于AKS的信息:"

#: ../../gettingstarted/k8s-install-default.rst:135
msgid ""
"It is not possible to assign taints to the initial node pool at this time, cf. "
"`Azure/AKS#1402 <https://github.com/Azure/AKS/issues/1402>`_."
msgstr ""
"目前无法在初始节点池打上污点，参见。 `Azure/AKS#1402 <https://github.com/Azure/"
"AKS/issues/1402>`_。"

#: ../../gettingstarted/k8s-install-default.rst:138
msgid ""
"It is not possible to assign custom node taints such as ``node.cilium.io/agent-"
"not-ready=true:NoExecute`` to system node pools, cf. `Azure/AKS#2578 <https://"
"github.com/Azure/AKS/issues/2578>`_."
msgstr ""
"无法将自定义污点并（例如“node.cilium.io/agent-not-ready=true:NoExecute”）分配给"
"系统节点池，参见。 `Azure/AKS#2578 <https://github.com/Azure/AKS/"
"issues/2578>`_。"

#: ../../gettingstarted/k8s-install-default.rst:141
msgid ""
"In order to have Cilium properly manage application pods on AKS with these "
"limitations, the operations above:"
msgstr ""
"为了使 Cilium 在具有这些限制的情况下正确管理 AKS 上的应用程序 pod，上述操作："

#: ../../gettingstarted/k8s-install-default.rst:144
msgid ""
"Replace the initial node pool with a new system node pool tainted with "
"``CriticalAddonsOnly=true:NoSchedule``, preventing application pods from being "
"scheduled on it."
msgstr ""
"用带有“CriticalAddonsOnly=true:NoSchedule”的新系统节点池替换初始节点池，防止应用"
"程序 pod 在其上调度。"

#: ../../gettingstarted/k8s-install-default.rst:148
msgid ""
"Create a secondary user node pool tainted with ``node.cilium.io/agent-not-"
"ready=true:NoExecute``, preventing application pods from being scheduled/"
"executed on it until Cilium is ready to manage them."
msgstr ""
"创建一个打上“node.cilium.io/agent-not-ready=true:NoExecute”污点的辅助用户节点"
"池，防止应用程序 pod 在Cilium 准备好管理pod前被调度/执行。"

#: ../../gettingstarted/k8s-install-default.rst:152
#: ../../gettingstarted/k8s-install-default.rst:279
#: ../../gettingstarted/k8s-install-helm.rst:137
msgid "EKS"
msgstr ""

#: ../../gettingstarted/k8s-install-default.rst:154
msgid ""
"The following commands create a Kubernetes cluster with ``eksctl`` using "
"`Amazon Elastic Kubernetes Service <https://aws.amazon.com/eks/>`_.  See "
"`eksctl Installation <https://github.com/weaveworks/eksctl>`_ for instructions "
"on how to install ``eksctl`` and prepare your account."
msgstr ""
"以下命令使用 `Amazon Elastic Kubernetes Service <https://aws.amazon.com/eks/>`_ "
"创建一个带有 ``eksctl`` 的 Kubernetes 集群。 请参阅 `eksctl 安装 <https://"
"github.com/weaveworks/eksctl>`_ 以获取有关如何安装 ``eksctl`` 和准备您的帐户。"

#: ../../gettingstarted/k8s-install-default.rst:189
msgid "kind"
msgstr "kind"

#: ../../gettingstarted/k8s-install-default.rst:191
#: ../../gettingstarted/kind-install-deps.rst:10
msgid ""
"Install ``kind`` >= v0.7.0 per kind documentation: `Installation and Usage "
"<https://kind.sigs.k8s.io/#installation-and-usage>`_"
msgstr ""
"安装 ``kind`` >= v0.7.0 per kind 文档：`Installation and Usage <https://kind."
"sigs.k8s.io/#installation-and-usage>`_"

#: ../../gettingstarted/k8s-install-default.rst:199
msgid "minikube"
msgstr "minikube"

#: ../../gettingstarted/k8s-install-default.rst:201
msgid ""
"Install minikube >= v1.12 as per minikube documentation: `Install Minikube "
"<https://kubernetes.io/docs/tasks/tools/install-minikube/>`_. The following "
"command will bring up a single node minikube cluster prepared for installing "
"cilium."
msgstr ""
"根据 minikube 文档安装 minikube >= v1.12：`Install Minikube <https://kubernetes."
"io/docs/tasks/tools/install-minikube/>`_。 以下命令将打开一个准备安装 cilium 的"
"单节点 minikube 集群。"

#: ../../gettingstarted/k8s-install-default.rst:211
msgid ""
"From minikube v1.12.1+, cilium networking plugin can be enabled directly with "
"``--cni=cilium`` parameter in ``minikube start`` command. However, this may not "
"install the latest version of cilium."
msgstr ""
"从 minikube v1.12.1+ 起，cilium 网络插件可以直接在 ``minikube start`` 命令中使用"
"``—cni=cilium`` 参数启用。 但是，这可能不会安装最新版本的 cilium。"

#: ../../gettingstarted/k8s-install-default.rst:215
#: ../../gettingstarted/k8s-install-helm.rst:230
msgid "Rancher Desktop"
msgstr "Rancher Desktop"

#: ../../gettingstarted/k8s-install-default.rst:217
msgid ""
"Install Rancher Desktop >= v1.1.0 as per Rancher Desktop documentation: "
"`Install Rancher Desktop <https://docs.rancherdesktop.io/getting-started/"
"installation>`_."
msgstr ""
"根据 Rancher Desktop 文档安装 Rancher Desktop >= v1.1.0：`安装 Rancher Desktop "
"<https://docs.rancherdesktop.io/getting-started/installation>`_。"

#: ../../gettingstarted/k8s-install-default.rst:220
msgid ""
"Next you need to configure Rancher Desktop so to disable the builtin CNI so you "
"can install Cilium."
msgstr "接下来，您需要配置 Rancher Desktop 以禁用内置 CNI以便可以安装 Cilium。"

#: ../../gettingstarted/rancher-desktop-configure.rst:1
msgid ""
"Configuring Rancher Desktop is done using a YAML configuration file. This step "
"is necessary in order to disable the default CNI and replace it with Cilium."
msgstr ""
"使用 YAML 配置文件配置 Rancher Desktop。 此步骤对于禁用默认 CNI 并将其替换为 "
"Cilium 是必需的。"

#: ../../gettingstarted/rancher-desktop-configure.rst:5
msgid ""
"Next you need to start Rancher Desktop with ``containerd`` and create a :"
"download:`override.yaml <rancher-desktop-override.yaml>`:"
msgstr ""
"接下来，您需要使用 ``containerd`` 启动 Rancher Desktop 并创建一个 :download:"
"`override.yaml <rancher-desktop-override.yaml> `："

#: ../../gettingstarted/rancher-desktop-configure.rst:10
msgid ""
"After the file is created move it into your Rancher Desktop's ``lima/_config`` "
"directory:"
msgstr "创建文件后，将其移动到 Rancher Desktop 的 ``lima/_config`` 目录中："

#: ../../gettingstarted/rancher-desktop-configure.rst:25
msgid ""
"Finally, open the Rancher Desktop UI and go to Kubernetes Settings panel and "
"click \"Reset Kubernetes\"."
msgstr ""
"最后，打开 Rancher 桌面 UI 并转到 Kubernetes 设置面板并单击“重置 Kubernetes”。"

#: ../../gettingstarted/rancher-desktop-configure.rst:27
msgid ""
"After a few minutes Rancher Desktop will start back up prepared for installing "
"Cilium."
msgstr "几分钟后 Rancher Desktop 将开始备份，并为安装 Cilium 做好准备。"

#: ../../gettingstarted/k8s-install-default.rst:233
msgid ""
"You can install Cilium on any Kubernetes cluster. Pick one of the options below:"
msgstr "您可以在任何 Kubernetes 集群上安装 Cilium。 选择以下方式之一进行安装："

#: ../../gettingstarted/k8s-install-default.rst:239
msgid ""
"These are the generic instructions on how to install Cilium into any Kubernetes "
"cluster. The installer will attempt to automatically pick the best "
"configuration options for you. Please see the other tabs for distribution/"
"platform specific instructions which also list the ideal default configuration "
"for particular platforms."
msgstr ""
"这些是关于如何将 Cilium 安装到任何 Kubernetes 集群中的通用方式。 安装程序将尝试"
"自动为您选择最佳配置选项。 请参阅其他选项卡以了解不同方式/平台特定说明，其中还列"
"出了特定平台的理想默认配置。"

#: ../../gettingstarted/requirements-aks.rst:17
#: ../../gettingstarted/requirements-eks.rst:19
#: ../../gettingstarted/requirements-generic.rst:1
#: ../../gettingstarted/requirements-gke.rst:12
#: ../../gettingstarted/requirements-k3s.rst:12
#: ../../gettingstarted/requirements-openshift.rst:12
#: ../../gettingstarted/requirements-rke.rst:18
msgid "**Requirements:**"
msgstr "**要求:**"

#: ../../gettingstarted/requirements-generic.rst:3
msgid ""
"Kubernetes must be configured to use CNI (see `Network Plugin Requirements "
"<https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/"
"network-plugins/#network-plugin-requirements>`_)"
msgstr ""
"Kubernetes 必须配置为使用 CNI（请参阅`网络插件要求 <https://kubernetes.io/docs/"
"concepts/extend-kubernetes/compute-storage-net/network-plugins/#network-plugin-"
"requirements>`_）"

#: ../../gettingstarted/requirements-generic.rst:4
msgid "Linux kernel >= 4.9.17"
msgstr "Linux kernel >= 4.9.17"

#: ../../gettingstarted/requirements-generic.rst:8
msgid "See :ref:`admin_system_reqs` for more details on the system requirements."
msgstr "有关系统要求的更多详细信息，请参阅 :ref:`admin_system_reqs`。"

#: ../../gettingstarted/k8s-install-default.rst:247
msgid "**Install Cilium**"
msgstr "**安装 Cilium**"

#: ../../gettingstarted/k8s-install-default.rst:249
msgid ""
"Install Cilium into the Kubernetes cluster pointed to by your current kubectl "
"context:"
msgstr "将 Cilium 安装到当前 kubectl context 指向的 Kubernetes 集群中："

#: ../../gettingstarted/requirements-gke.rst:1
msgid ""
"To install Cilium on `Google Kubernetes Engine (GKE) <https://cloud.google.com/"
"kubernetes-engine>`_, perform the following steps:"
msgstr ""
"要在 `Google Kubernetes Engine (GKE) <https://cloud.google.com/kubernetes-"
"engine>`_ 上安装 Cilium，请执行以下步骤："

#: ../../gettingstarted/k8s-install-helm.rst:32
#: ../../gettingstarted/requirements-aks.rst:4
#: ../../gettingstarted/requirements-eks.rst:4
#: ../../gettingstarted/requirements-gke.rst:4
#: ../../gettingstarted/requirements-k3s.rst:4
#: ../../gettingstarted/requirements-openshift.rst:4
#: ../../gettingstarted/requirements-rke.rst:10
msgid "**Default Configuration:**"
msgstr "**默认配置：**"

#: ../../gettingstarted/k8s-install-helm.rst:35
#: ../../gettingstarted/requirements-aks.rst:7
#: ../../gettingstarted/requirements-eks.rst:7
#: ../../gettingstarted/requirements-gke.rst:7
#: ../../gettingstarted/requirements-k3s.rst:7
#: ../../gettingstarted/requirements-openshift.rst:7
#: ../../gettingstarted/requirements-rke.rst:13
msgid "Datapath"
msgstr "Datapath"

#: ../../gettingstarted/k8s-install-helm.rst:35
#: ../../gettingstarted/requirements-aks.rst:7
#: ../../gettingstarted/requirements-eks.rst:7
#: ../../gettingstarted/requirements-gke.rst:7
#: ../../gettingstarted/requirements-k3s.rst:7
#: ../../gettingstarted/requirements-openshift.rst:7
#: ../../gettingstarted/requirements-rke.rst:13
msgid "IPAM"
msgstr "IPAM"

#: ../../gettingstarted/k8s-install-helm.rst:35
#: ../../gettingstarted/requirements-aks.rst:7
#: ../../gettingstarted/requirements-eks.rst:7
#: ../../gettingstarted/requirements-gke.rst:7
#: ../../gettingstarted/requirements-k3s.rst:7
#: ../../gettingstarted/requirements-openshift.rst:7
#: ../../gettingstarted/requirements-rke.rst:13
msgid "Datastore"
msgstr "Datastore"

#: ../../gettingstarted/requirements-aks.rst:9
#: ../../gettingstarted/requirements-gke.rst:9
msgid "Direct Routing"
msgstr "Direct Routing"

#: ../../gettingstarted/requirements-gke.rst:9
msgid "Kubernetes PodCIDR"
msgstr "Kubernetes PodCIDR"

#: ../../gettingstarted/k8s-install-helm.rst:37
#: ../../gettingstarted/requirements-aks.rst:9
#: ../../gettingstarted/requirements-eks.rst:9
#: ../../gettingstarted/requirements-gke.rst:9
#: ../../gettingstarted/requirements-k3s.rst:9
#: ../../gettingstarted/requirements-openshift.rst:9
#: ../../gettingstarted/requirements-rke.rst:15
msgid "Kubernetes CRD"
msgstr "Kubernetes CRD"

#: ../../gettingstarted/requirements-gke.rst:14
msgid ""
"The cluster should be created with the taint ``node.cilium.io/agent-not-"
"ready=true:NoExecute`` using ``--node-taints`` option. However, there are other "
"options. Please make sure to read and understand the documentation page on :ref:"
"`taint effects and unmanaged pods<taint_effects>`."
msgstr ""
"最好使用 ``—node-taints`` 选项使用 taint``node.cilium.io/agent-not-ready=true:"
"NoExecute`` 创建集群。 但是还有其他选择。 请确保阅读并理解 :ref:`taint effects "
"和 unmanaged pods<taint_effects>` 的文档页面。"

#: ../../gettingstarted/k8s-install-default.rst:259
#: ../../gettingstarted/k8s-install-default.rst:271
#: ../../gettingstarted/k8s-install-default.rst:283
#: ../../gettingstarted/k8s-install-default.rst:296
#: ../../gettingstarted/k8s-install-default.rst:306
#: ../../gettingstarted/k8s-install-default.rst:318
#: ../../gettingstarted/k8s-install-helm.rst:42
#: ../../gettingstarted/k8s-install-helm.rst:55
#: ../../gettingstarted/k8s-install-helm.rst:118
#: ../../gettingstarted/k8s-install-helm.rst:150
#: ../../gettingstarted/k8s-install-helm.rst:200
#: ../../gettingstarted/k8s-install-helm.rst:210
#: ../../gettingstarted/k8s-install-helm.rst:223
#: ../../gettingstarted/k8s-install-helm.rst:239
msgid "**Install Cilium:**"
msgstr "**安装 Cilium:**"

#: ../../gettingstarted/k8s-install-default.rst:261
msgid "Install Cilium into the GKE cluster:"
msgstr "在 GKE 集群中安装 Cilium:"

#: ../../gettingstarted/requirements-aks.rst:1
msgid ""
"To install Cilium on `Azure Kubernetes Service (AKS) <https://docs.microsoft."
"com/en-us/azure/aks/>`_, perform the following steps:"
msgstr ""
"要在 `Azure Kubernetes Service (AKS) <https://docs.microsoft.com/en-us/azure/"
"aks/>`_ 上安装 Cilium，请执行以下步骤："

#: ../../gettingstarted/requirements-aks.rst:9
msgid "Azure IPAM"
msgstr "Azure IPAM"

#: ../../gettingstarted/requirements-aks.rst:14
msgid ""
"If you want to chain Cilium on top of the Azure CNI, refer to the guide :ref:"
"`chaining_azure`."
msgstr "如果您想在 Azure CNI 上加入 Cilium，请参阅指南:ref:`chaining_azure`。"

#: ../../gettingstarted/requirements-aks.rst:19
msgid ""
"The AKS cluster must be created with ``--network-plugin azure`` for "
"compatibility with Cilium. The Azure network plugin will be replaced with "
"Cilium by the installer."
msgstr ""
"为了与 Cilium 兼容，必须使用 —network-plugin azure 创建 AKS 集群。 Azure 网络插"
"件将被安装程序替换为 Cilium。"

#: ../../gettingstarted/requirements-aks.rst:23
msgid ""
"Node pools must be properly tainted to ensure applications pods are properly "
"managed by Cilium:"
msgstr "节点池必须被正确的打上污点，以确保应用程序 pod 由 Cilium 管理："

#: ../../gettingstarted/requirements-aks.rst:26
msgid ""
"User node pools should be tainted with ``node.cilium.io/agent-not-ready=true:"
"NoExecute`` to ensure application pods will only be scheduled/executed once "
"Cilium is ready to manage them. However, there are other options. Please make "
"sure to read and understand the documentation page on :ref:`taint effects and "
"unmanaged pods<taint_effects>`."
msgstr ""
"用户节点池应使用“node.cilium.io/agent-not-ready=true:NoExecute”打上污点，以确保"
"只有在 Cilium 准备好管理应用程序 pod 时才会调度/执行它们。 但这不是唯一的选择，"
"请确保阅读并理解 :ref:`taint effects 和 unmanaged pods<taint_effects>` 的文档页"
"面。"

#: ../../gettingstarted/requirements-aks.rst:31
msgid ""
"System node pools must be tainted with ``CriticalAddonsOnly=true:NoSchedule``, "
"preventing application pods from being scheduled on them. This is necessary "
"because it is not possible to assign custom node taints such as ``node.cilium."
"io/agent-not-ready=true:NoExecute`` to system node pools, cf. `Azure/AKS#2578 "
"<https://github.com/Azure/AKS/issues/2578>`_."
msgstr ""
"系统节点池必须打上“CriticalAddonsOnly=true:NoSchedule”作为污点，防止应用程序 "
"pod 被调度在它们上面。 这是必要的，因为无法将自定义节点污点（例如“node.cilium."
"io/agent-not-ready=true:NoExecute”）分配给系统节点池，参见。 `Azure/AKS#2578 "
"<https://github.com/Azure/AKS/issues/2578>`_。"

#: ../../gettingstarted/requirements-aks.rst:36
msgid ""
"The initial node pool must be replaced with a new system node pool since it is "
"not possible to assign taints to the initial node pool at this time, cf. `Azure/"
"AKS#1402 <https://github.com/Azure/AKS/issues/1402>`_."
msgstr ""
"因为此时无法给初始节点池打上污点，初始节点池必须替换为新的系统节点池。参见。 "
"`Azure/AKS#1402 <https://github.com/Azure/AKS/issues/1402>`_。"

#: ../../gettingstarted/requirements-aks.rst:40
#: ../../gettingstarted/requirements-eks.rst:50
msgid "**Limitations:**"
msgstr "**限制:**"

#: ../../gettingstarted/requirements-aks.rst:42
msgid ""
"All VMs and VM scale sets used in a cluster must belong to the same resource "
"group."
msgstr "群集中使用的所有 VM 和 VM 弹性组必须属于同一资源组。"

#: ../../gettingstarted/k8s-install-default.rst:273
msgid "Install Cilium into the AKS cluster:"
msgstr "在 AKS 集群上安装 Cilium:"

#: ../../gettingstarted/requirements-eks.rst:1
msgid ""
"To install Cilium on `Amazon Elastic Kubernetes Service (EKS) <https://docs.aws."
"amazon.com/eks/latest/userguide/getting-started.html>`_, perform the following "
"steps:"
msgstr ""
"要在 `Amazon Elastic Kubernetes Service (EKS) <https://docs.aws.amazon.com/eks/"
"latest/userguide/getting-started.html>`_ 上安装 Cilium，请执行以下步骤："

#: ../../gettingstarted/requirements-eks.rst:9
msgid "Direct Routing (ENI)"
msgstr "Direct Routing (ENI)"

#: ../../gettingstarted/requirements-eks.rst:9
msgid "AWS ENI"
msgstr "AWS ENI"

#: ../../gettingstarted/requirements-eks.rst:12
msgid "For more information on AWS ENI mode, see :ref:`ipam_eni`."
msgstr "有关 AWS ENI 模式的更多信息，请参阅:ref:`ipam_eni`。"

#: ../../gettingstarted/requirements-eks.rst:16
msgid ""
"If you want to chain Cilium on top of the AWS CNI, refer to the guide :ref:"
"`chaining_aws_cni`."
msgstr "如果您想在 AWS CNI 中添加 Cilium，请参阅指南:ref:`chaining_aws_cni`。"

#: ../../gettingstarted/requirements-eks.rst:21
msgid ""
"The `EKS Managed Nodegroups <https://eksctl.io/usage/eks-managed-nodes>`_ must "
"be properly tainted to ensure applications pods are properly managed by Cilium:"
msgstr ""
"`EKS Managed Nodegroups <https://eksctl.io/usage/eks-managed-nodes>`_ 必须打上污"
"点，以确保应用程序 pod 由 Cilium 正确管理："

#: ../../gettingstarted/requirements-eks.rst:25
msgid ""
"``managedNodeGroups`` should be tainted with ``node.cilium.io/agent-not-"
"ready=true:NoExecute`` to ensure application pods will only be scheduled once "
"Cilium is ready to manage them. However, there are other options. Please make "
"sure to read and understand the documentation page on :ref:`taint effects and "
"unmanaged pods<taint_effects>`."
msgstr ""
"``managedNodeGroups`` 应该被``node.cilium.io/agent-not-ready=true:NoExecute`` 打"
"上污点，以确保只有在 Cilium 准备好管理它们时才会调度应用程序 pod。 其它方式请阅"
"读并理解 :ref:`taint effects 和 unmanaged pods<taint_effects>` 的文档页面。"

#: ../../gettingstarted/requirements-eks.rst:31
msgid ""
"Below is an example on how to use `ClusterConfig <https://eksctl.io/usage/"
"creating-and-managing-clusters/#using-config-files>`_ file to create the "
"cluster:"
msgstr ""
"下面是一个关于如何使用 `ClusterConfig <https://eksctl.io/usage/creating-and-"
"managing-clusters/#using-config-files>`_ 文件创建集群的示例："

#: ../../gettingstarted/requirements-eks.rst:52
msgid ""
"The AWS ENI integration of Cilium is currently only enabled for IPv4. If you "
"want to use IPv6, use a datapath/IPAM mode other than ENI."
msgstr ""
"Cilium 的 AWS ENI 集成目前仅支持 IPv4。 如果要使用 IPv6，请使用 ENI 以外的数据路"
"径/IPAM 模式。"

#: ../../gettingstarted/k8s-install-default.rst:285
msgid "Install Cilium into the EKS cluster."
msgstr "在 EKS 集群上安装 Cilium。"

#: ../../gettingstarted/requirements-openshift.rst:1
msgid ""
"To install Cilium on `OpenShift <https://www.openshift.com/>`_, perform the "
"following steps:"
msgstr ""
"要在 `OpenShift <https://www.openshift.com/>`_ 上安装 Cilium，请执行以下步骤："

#: ../../gettingstarted/k8s-install-helm.rst:37
#: ../../gettingstarted/requirements-k3s.rst:9
#: ../../gettingstarted/requirements-openshift.rst:9
#: ../../gettingstarted/requirements-rke.rst:15
msgid "Encapsulation"
msgstr "封装"

#: ../../gettingstarted/k8s-install-helm.rst:37
#: ../../gettingstarted/requirements-k3s.rst:9
#: ../../gettingstarted/requirements-openshift.rst:9
#: ../../gettingstarted/requirements-rke.rst:15
msgid "Cluster Pool"
msgstr "集群池"

#: ../../gettingstarted/requirements-openshift.rst:14
msgid "OpenShift 4.x"
msgstr "OpenShift 4.x"

#: ../../gettingstarted/k8s-install-default.rst:298
#: ../../gettingstarted/k8s-install-helm.rst:202
msgid ""
"Cilium is a `Certified OpenShift CNI Plugin <https://access.redhat.com/"
"articles/5436171>`_ and is best installed when an OpenShift cluster is created "
"using the OpenShift installer. Please refer to :ref:`k8s_install_openshift_okd` "
"for more information."
msgstr ""
"Cilium 是一个`Certified OpenShift CNI Plugin <https://access.redhat.com/"
"articles/5436171>`_，最好在使用 OpenShift 安装程序并创建 OpenShift 集群时安装。 "
"更多信息请参考:ref:`k8s_install_openshift_okd`。"

#: ../../gettingstarted/k8s-install-default.rst:302
#: ../../gettingstarted/k8s-install-helm.rst:206
msgid "RKE"
msgstr "RKE"

#: ../../gettingstarted/requirements-rke.rst:1
msgid ""
"To install Cilium on `Rancher Kubernetes Engine (RKE) <https://rancher.com/docs/"
"rke/latest/en/>`_, perform the following steps:"
msgstr ""
"要在 `Rancher Kubernetes Engine (RKE) <https://rancher.com/docs/rke/latest/en/"
">`_ 上安装 Cilium，请执行以下步骤："

#: ../../gettingstarted/requirements-rke.rst:6
msgid ""
"If you are using RKE2, Cilium has been directly integrated. Please see `Using "
"Cilium <https://docs.rke2.io/install/network_options/#using-cilium-or-calico-"
"instead-of-canal>`_ in the RKE2 documentation. You can use either method."
msgstr ""
"如果你使用的是 RKE2，Cilium 已经直接集成了。 请参阅 RKE2 文档中的`使用 Cilium "
"<https://docs.rke2.io/install/network_options/#using-cilium-or-calico-instead-"
"of-canal>`_。"

#: ../../gettingstarted/requirements-rke.rst:20
msgid ""
"Follow the `RKE Installation Guide <https://rancher.com/docs/rke/latest/en/"
"installation/>`_ with the below change:"
msgstr ""
"按照 `RKE 安装指南 <https://rancher.com/docs/rke/latest/en/installation/>`_ 进行"
"以下步骤："

#: ../../gettingstarted/requirements-rke.rst:23
msgid "From:"
msgstr "从："

#: ../../gettingstarted/k8s-install-rke.rst:35
#: ../../gettingstarted/requirements-rke.rst:32
msgid "To:"
msgstr "到："

#: ../../gettingstarted/k8s-install-default.rst:308
msgid "Install Cilium into your newly created RKE cluster:"
msgstr "将 Cilium 安装到新创建的 RKE 集群中："

#: ../../gettingstarted/k8s-install-default.rst:314
#: ../../gettingstarted/k8s-install-helm.rst:219
msgid "k3s"
msgstr "k3s"

#: ../../gettingstarted/requirements-k3s.rst:1
msgid ""
"To install Cilium on `k3s <https://rancher.com/docs/k3s/latest/en/quick-start/"
">`_, perform the following steps:"
msgstr ""
"要在 `k3s <https://rancher.com/docs/k3s/latest/en/quick-start/>`_ 上安装 "
"Cilium，请执行以下步骤："

#: ../../gettingstarted/requirements-k3s.rst:14
msgid ""
"Install your k3s cluster as you normally would but making sure to disable "
"support for the default CNI plugin and the built-in network policy enforcer so "
"you can install Cilium on top:"
msgstr ""
"整个安装过程除了需要确保对默认 CNI 插件和内置网络策略执行器的禁用外并和往常没有"
"什么区别，之后您可以在集群的基础上安装 Cilium："

#: ../../gettingstarted/k8s-install-default.rst:320
msgid "Install Cilium into your newly created Kubernetes cluster:"
msgstr "将 Cilium 安装到新创建的 Kubernetes 集群中："

#: ../../gettingstarted/k8s-install-default.rst:327
msgid ""
"If the installation fails for some reason, run ``cilium status`` to retrieve "
"the overall status of the Cilium deployment and inspect the logs of whatever "
"pods are failing to be deployed."
msgstr ""
"如果由于某种原因安装失败，请运行“cilium status”来检索 Cilium 部署的整体状态，并"
"检查任何未能部署的 pod 的日志。"

#: ../../gettingstarted/k8s-install-default.rst:333
msgid "You may be seeing ``cilium install`` print something like this:"
msgstr "你可能会看到 cilium install 打印如下内容："

#: ../../gettingstarted/k8s-install-default.rst:344
msgid ""
"This indicates that your cluster was already running some pods before Cilium "
"was deployed and the installer has automatically restarted them to ensure all "
"pods get networking provided by Cilium."
msgstr ""
"这表明您的集群在部署 Cilium 之前已经运行了一些 Pod，并且安装程序已自动重新启动它"
"们以确保所有 Pod 都获得 Cilium 提供的网络。"

#: ../../gettingstarted/k8s-install-external-etcd.rst:12
msgid "Installation with external etcd"
msgstr "使用外部 etcd 安装 Cilium"

#: ../../gettingstarted/k8s-install-external-etcd.rst:14
msgid ""
"This guide walks you through the steps required to set up Cilium on Kubernetes "
"using an external etcd. Use of an external etcd provides better performance and "
"is suitable for larger environments."
msgstr ""
"本指南将引导您完成使用外部 etcd 在 Kubernetes 上设置 Cilium 所需的步骤。 使用外"
"部 etcd 可提供更好的性能，并且适用于更大的集群规模。"

#: ../../gettingstarted/k8s-install-external-etcd.rst:18
msgid ""
"Should you encounter any issues during the installation, please refer to the :"
"ref:`troubleshooting_k8s` section and / or seek help on :ref:`slack`."
msgstr ""
"如果您在安装过程中遇到任何问题，请参阅:ref:`troubleshooting_k8s` 部分和/或在:"
"ref:`slack` 上寻求帮助。"

#: ../../gettingstarted/k8s-install-external-etcd.rst:22
msgid "When do I need to use a kvstore?"
msgstr "我什么时候需要使用 kvstore？"

#: ../../gettingstarted/k8s-install-external-etcd.rst:24
msgid ""
"Unlike the section :ref:`k8s_quick_install`, this guide explains how to "
"configure Cilium to use an external kvstore such as etcd. If you are unsure "
"whether you need to use a kvstore at all, the following is a list of reasons "
"when to use a kvstore:"
msgstr ""
"与 :ref:`k8s_quick_install` 部分不同，本指南解释了如何配置 Cilium 以使用例如 "
"etcd 这样的外部 kvstore。 如果您不确定是否需要使用 kvstore，可以参照建议使用 "
"kvstore 的原因列表："

#: ../../gettingstarted/k8s-install-external-etcd.rst:29
msgid ""
"If you are running in an environment where you observe a high overhead in state "
"propagation caused by Kubernetes events."
msgstr "集群中由 Kubernetes Evnet 状态传播开销很高。"

#: ../../gettingstarted/k8s-install-external-etcd.rst:31
msgid ""
"If you do not want Cilium to store state in Kubernetes custom resources (CRDs)."
msgstr "如果您不希望 Cilium 将状态存储在 Kubernetes 自定义资源 (CRD) 中。"

#: ../../gettingstarted/k8s-install-external-etcd.rst:33
msgid ""
"If you run a cluster with more pods and more nodes than the ones tested in the :"
"ref:`scalability_guide`."
msgstr "如果您运行的集群具有比 :ref:`scalability_guide` 中更多的 pod 和节点。"

#: ../../gettingstarted/requirements_intro.rst:2
msgid "Requirements"
msgstr "要求"

#: ../../gettingstarted/requirements_intro.rst:4
msgid "Make sure your Kubernetes environment is meeting the requirements:"
msgstr "确保您的 Kubernetes 环境满足要求："

#: ../../gettingstarted/requirements_intro.rst:6
msgid "Kubernetes >= 1.16"
msgstr "Kubernetes >= 1.16"

#: ../../gettingstarted/requirements_intro.rst:7
msgid "Linux kernel >= 4.9"
msgstr "Linux kernel >= 4.9"

#: ../../gettingstarted/requirements_intro.rst:8
msgid "Kubernetes in CNI mode"
msgstr "CNI 模式下的 Kubernetes"

#: ../../gettingstarted/requirements_intro.rst:9
msgid "Mounted eBPF filesystem mounted on all worker nodes"
msgstr "将 eBPF 文件系统挂载在所有工作节点上"

#: ../../gettingstarted/requirements_intro.rst:10
msgid ""
"Recommended: Enable PodCIDR allocation (``--allocate-node-cidrs``) in the "
"``kube-controller-manager`` (recommended)"
msgstr ""
"推荐：在 ``kube-controller-manager`` 中启用 PodCIDR 分配（``--allocate-node-"
"cidrs``）（推荐）"

#: ../../gettingstarted/requirements_intro.rst:12
msgid ""
"Refer to the section :ref:`k8s_requirements` for detailed instruction on how to "
"prepare your Kubernetes environment."
msgstr ""
"有关如何准备 Kubernetes 环境的详细说明，请参阅 :ref:`k8s_requirements` 部分。"

#: ../../gettingstarted/k8s-install-external-etcd.rst:40
msgid "You will also need an external etcd version 3.1.0 or higher."
msgstr "外部 etcd 的版本需要 3.1.0 或更高版本。"

#: ../../gettingstarted/k8s-install-external-etcd.rst:43
msgid "Configure Cilium"
msgstr "配置 Cilium"

#: ../../gettingstarted/k8s-install-external-etcd.rst:45
msgid ""
"When using an external kvstore, the address of the external kvstore needs to be "
"configured in the ConfigMap. Download the base YAML and configure it with :term:"
"`Helm`:"
msgstr ""
"使用外部kvstore时，需要在ConfigMap中配置外部kvstore的地址。 下载基本 YAML 并使"
"用 Helm 进行配置："

#: ../../gettingstarted/k8s-install-external-etcd.rst:62
msgid ""
"If you do not want Cilium to store state in Kubernetes custom resources (CRDs), "
"consider setting ``identityAllocationMode``::"
msgstr ""
"如果您不希望 Cilium 将状态存储在 Kubernetes 自定义资源 (CRD) 中，请考虑设置 "
"``identityAllocationMode``::"

#: ../../gettingstarted/k8s-install-external-etcd.rst:69
msgid "Optional: Configure the SSL certificates"
msgstr "可选：配置 SSL 证书"

#: ../../gettingstarted/k8s-install-external-etcd.rst:71
msgid ""
"Create a Kubernetes secret with the root certificate authority, and client-side "
"key and certificate of etcd:"
msgstr "使用根证书颁发机构以及 etcd 的客户端密钥和证书创建 Kubernetes secret："

#: ../../gettingstarted/k8s-install-external-etcd.rst:81
msgid ""
"Adjust the helm template generation to enable SSL for etcd and use https "
"instead of http for the etcd endpoint URLs:"
msgstr ""
"调整 helm 模板为 etcd 启用 SSL，并为配置 etcd endpoint URL 为 https 而不是 "
"http："

#: ../../gettingstarted/k8s-install-helm.rst:11
msgid "Installation using Helm"
msgstr "通过 Helm 安装"

#: ../../gettingstarted/k8s-install-helm.rst:13
msgid ""
"This guide will show you how to install Cilium using `Helm <https://helm.sh/"
">`_. This involves a couple of additional steps compared to the :ref:"
"`k8s_quick_install` and requires you to manually select the best datapath and "
"IPAM mode for your particular environment."
msgstr ""
"本指南将向您展示如何使用 `Helm <https://helm.sh/>`_ 安装 Cilium。 与:ref:"
"`k8s_quick_install` 相比，这涉及几个额外的步骤，并且需要您为特定环境手动选择最"
"佳 datapath 和 IPAM 模式。"

#: ../../gettingstarted/k8s-install-helm.rst:27
msgid ""
"These are the generic instructions on how to install Cilium into any Kubernetes "
"cluster using the default configuration options below. Please see the other "
"tabs for distribution/platform specific instructions which also list the ideal "
"default configuration for particular platforms."
msgstr ""
"以下是关于如何使用以下默认配置选项将 Cilium 安装到任何 Kubernetes 集群上的指"
"南。 请参阅其他选项卡以了解不同发行版或平台的不同之处，其中还列出了特定平台的最"
"佳默认配置。"

#: ../../gettingstarted/k8s-install-helm.rst:57
msgid "Extract the Cluster CIDR to enable native-routing:"
msgstr "使用 CIDR 以启用本机路由："

#: ../../gettingstarted/k8s-install-helm.rst:78
msgid ""
"The NodeInit DaemonSet is required to prepare the GKE nodes as nodes are added "
"to the cluster. The NodeInit DaemonSet will perform the following actions:"
msgstr ""
"通过 NodeInit DaemonSet 将 GKE 节点添加到集群时， NodeInit DaemonSet 将执行以下"
"操作："

#: ../../gettingstarted/k8s-install-helm.rst:81
msgid "Reconfigure kubelet to run in CNI mode"
msgstr "在 CNI 模式下重新配置 kubelet"

#: ../../gettingstarted/k8s-install-helm.rst:82
msgid "Mount the eBPF filesystem"
msgstr "挂载 eBPF 文件系统"

#: ../../gettingstarted/k8s-install-helm.rst:88
msgid "**Create a Service Principal:**"
msgstr "**创建服务主体：**"

#: ../../gettingstarted/k8s-install-helm.rst:90
msgid ""
"In order to allow cilium-operator to interact with the Azure API, a Service "
"Principal with ``Contributor`` privileges over the AKS cluster is required "
"(see :ref:`Azure IPAM required privileges <ipam_azure_required_privileges>` for "
"more details). It is recommended to create a dedicated Service Principal for "
"each Cilium installation with minimal privileges over the AKS node resource "
"group:"
msgstr ""
"为了允许 cilium-operator 与 Azure API 交互，需要对 AKS 群集具有“Contributor”权限"
"的服务主体（有关更多详细信息，请参阅:ref:`Azure IPAM 所需特权 "
"<ipam_azure_required_privileges>`）。 建议为每个 Cilium 安装创建一个专用的服务主"
"体，对 AKS 节点资源组具有最低权限："

#: ../../gettingstarted/k8s-install-helm.rst:108
msgid ""
"The ``AZURE_NODE_RESOURCE_GROUP`` node resource group is *not* the resource "
"group of the AKS cluster. A single resource group may hold multiple AKS "
"clusters, but each AKS cluster regroups all resources in an automatically "
"managed secondary resource group. See `Why are two resource groups created with "
"AKS? <https://docs.microsoft.com/en-us/azure/aks/faq#why-are-two-resource-"
"groups-created-with-aks>`__ for more details."
msgstr ""
"``AZURE_NODE_RESOURCE_GROUP`` 节点资源组*not* AKS 群集的资源组。 单个资源组可能"
"包含多个 AKS 群集，但每个 AKS 群集会重新组合自动管理的辅助资源组中的所有资源。 "
"请参阅`为什么使用 AKS 创建两个资源组？ <https://docs.microsoft.com/en-us/azure/"
"aks/faq#why-are-two-resource-groups-created-with-aks>`__ 了解更多详情。"

#: ../../gettingstarted/k8s-install-helm.rst:115
msgid ""
"This ensures the Service Principal only has privileges over the AKS cluster "
"itself and not any other resources within the resource group."
msgstr ""
"这可确保服务主体仅对 AKS 群集本身具有特权，而对资源组中的任何其他资源没有特权。"

#: ../../gettingstarted/k8s-install-helm.rst:141
msgid "**Delete VPC CNI (``aws-node`` DaemonSet)**"
msgstr "**删除 VPC CNI (``aws-node`` DaemonSet)**"

#: ../../gettingstarted/k8s-install-helm.rst:143
msgid ""
"Cilium will manage ENIs instead of VPC CNI, so the ``aws-node`` DaemonSet has "
"to be deleted to prevent conflict behavior."
msgstr ""
"Cilium 将管理 ENI 而不是 VPC CNI，因此必须删除``aws-node`` DaemonSet 以防止冲突"
"行为。"

#: ../../gettingstarted/k8s-install-helm.rst:165
msgid ""
"This helm command sets ``eni.enabled=true`` and ``tunnel=disabled``, meaning "
"that Cilium will allocate a fully-routable AWS ENI IP address for each pod, "
"similar to the behavior of the `Amazon VPC CNI plugin <https://docs.aws.amazon."
"com/eks/latest/userguide/pod-networking.html>`_."
msgstr ""
"这个 helm 命令设置 ``eni.enabled=true`` 和 ``tunnel=disabled``，这意味着 Cilium "
"将为每个 pod 分配一个完全可路由的 AWS ENI IP 地址，类似于`Amazon VPC CNI 插件的"
"行为 <https://docs.aws.amazon.com/eks/latest/userguide/pod-networking.html>`_。"

#: ../../gettingstarted/k8s-install-helm.rst:170
msgid "This mode depends on a set of :ref:`ec2privileges` from the EC2 API."
msgstr "此模式依赖于 EC2 API 中的 :ref:`ec2privileges`。"

#: ../../gettingstarted/k8s-install-helm.rst:172
msgid ""
"Cilium can alternatively run in EKS using an overlay mode that gives pods non-"
"VPC-routable IPs.  This allows running more pods per Kubernetes worker node "
"than the ENI limit, but means that pod connectivity to resources outside the "
"cluster (e.g., VMs in the VPC or AWS managed services) is masqueraded (i.e., "
"SNAT) by Cilium to use the VPC IP address of the Kubernetes worker node. To set "
"up Cilium overlay mode, follow the steps below:"
msgstr ""
"Cilium 也可以使用覆盖模式在 EKS 中运行，该模式为 pod 提供非 VPC 可路由 IP。 这允"
"许每个 Kubernetes 工作程序节点运行比 ENI 限制更多的 pod，但这意味着与集群外部资"
"源（例如，VPC 或 AWS 托管服务中的 VM）的 pod 连接被 Cilium 伪装（即 SNAT）以使"
"用 VPC IP Kubernetes 工作节点的地址。 要设置 Cilium 覆盖模式，请按照以下步骤操"
"作："

#: ../../gettingstarted/k8s-install-helm.rst:180
msgid ""
"Excluding the lines for ``eni.enabled=true``, ``ipam.mode=eni`` and "
"``tunnel=disabled`` from the helm command will configure Cilium to use overlay "
"routing mode (which is the helm default)."
msgstr ""
"从 helm 命令中排除 ``eni.enabled=true``、``ipam.mode=eni`` 和 "
"``tunnel=disabled`` 的行将配置 Cilium 使用覆盖路由模式（这是 helm 默认行为） ."

#: ../../gettingstarted/k8s-install-helm.rst:183
msgid "Flush iptables rules added by VPC CNI"
msgstr "清空iptables并通过VPC CNI 添加新的 iptables 规则"

#: ../../gettingstarted/k8s-install-helm.rst:192
msgid ""
"Some Linux distributions use a different interface naming convention. If you "
"use masquerading with the option ``egressMasqueradeInterfaces=eth0``, remember "
"to replace ``eth0`` with the proper interface name."
msgstr ""
"一些 Linux 发行版使用不同的接口命名约定。 如果您使用带有选"
"项“egressMasqueradeInterfaces=eth0”的配置，请记住用正确的接口名称替换“eth0”。"

#: ../../gettingstarted/k8s-install-helm.rst:212
#: ../../gettingstarted/k8s-install-rke.rst:49
msgid "Install Cilium via ``helm install``:"
msgstr "通过 ``helm install`` 安装 Cilium："

#: ../../gettingstarted/k8s-install-helm.rst:232
msgid "**Configure Rancher Desktop:**"
msgstr "**配置 Rancher Desktop:**"

#: ../../gettingstarted/k8s-install-helm.rst:234
msgid ""
"To install Cilium on `Rancher Desktop <https://rancherdesktop.io>`_, perform "
"the following steps:"
msgstr ""
"执行以下步骤在 `Rancher Desktop <https://rancherdesktop.io>`_ 上安装 Cilium："

#: ../../gettingstarted/k8s-install-kops.rst:12
msgid "Installation using Kops"
msgstr "通过 Kops 安装"

#: ../../gettingstarted/k8s-install-kops.rst:14
msgid ""
"As of kops 1.9 release, Cilium can be plugged into kops-deployed clusters as "
"the CNI plugin. This guide provides steps to create a Kubernetes cluster on AWS "
"using kops and Cilium as the CNI plugin. Note, the kops deployment will "
"automate several deployment features in AWS by default, including AutoScaling, "
"Volumes, VPCs, etc."
msgstr ""
"从 kops 1.9 版本开始，Cilium 可以作为 CNI 插件插入到 kops 部署的集群中。 本指南"
"提供了使用 kops 和 Cilium 作为 CNI 插件在 AWS 上创建 Kubernetes 集群的步骤。 请"
"注意，默认情况下，kops 部署将自动执行 AWS 中的多项部署功能，包括 AutoScaling、"
"Volumes、VPC 等。"

#: ../../gettingstarted/k8s-install-kops.rst:20
msgid ""
"Kops offers several out-of-the-box configurations of Cilium including :ref:"
"`kubeproxy-free`, :ref:`ipam_eni`, and dedicated etcd cluster for Cilium. This "
"guide will just go through a basic setup."
msgstr ""
"Kops 提供了几个开箱即用的 Cilium 配置，包括:ref:`kubeproxy-free`、:ref:"
"`ipam_eni` 和用于 Cilium 的专用 etcd 集群。 本指南将仅介绍基本设置。"

#: ../../gettingstarted/k8s-install-kops.rst:27
msgid "`aws cli <https://aws.amazon.com/cli/>`_"
msgstr "`aws cli <https://aws.amazon.com/cli/>`_"

#: ../../gettingstarted/k8s-install-kops.rst:28
msgid "`kubectl <https://kubernetes.io/docs/tasks/tools/install-kubectl/>`_"
msgstr "`kubectl <https://kubernetes.io/docs/tasks/tools/install-kubectl/>`_"

#: ../../gettingstarted/k8s-install-kops.rst:29
msgid ""
"aws account with permissions: * AmazonEC2FullAccess * AmazonRoute53FullAccess * "
"AmazonS3FullAccess * IAMFullAccess * AmazonVPCFullAccess"
msgstr ""
"具有权限的 aws 账户：* AmazonEC2FullAccess * AmazonRoute53FullAccess * "
"AmazonS3FullAccess * IAMFullAccess * AmazonVPCFullAccess"

#: ../../gettingstarted/k8s-install-kops.rst:38
msgid "Installing kops"
msgstr "安装kops"

#: ../../gettingstarted/k8s-install-kops.rst:57
msgid "Setting up IAM Group and User"
msgstr "设置 IAM 和用户"

#: ../../gettingstarted/k8s-install-kops.rst:59
msgid ""
"Assuming you have all the prerequisites, run the following commands to create "
"the kops user and group:"
msgstr "假设您满足以上所有条件，请运行以下命令来创建 kops 用户和组："

#: ../../gettingstarted/k8s-install-kops.rst:76
msgid ""
"kops requires the creation of a dedicated S3 bucket in order to store the state "
"and representation of the cluster. You will need to change the bucket name and "
"provide your unique bucket name (for example a reverse of FQDN added with short "
"description of the cluster). Also make sure to use the region where you will be "
"deploying the cluster."
msgstr ""
"kops 需要创建一个专用的 S3 存储桶来存储集群的状态和表示。 您需要更改存储桶名称并"
"提供您唯一的存储桶名称（例如，添加了集群简短描述的 FQDN 反向）。 还要确保使用您"
"将部署集群的区域。"

#: ../../gettingstarted/k8s-install-kops.rst:87
msgid ""
"The above steps are sufficient for getting a working cluster installed. Please "
"consult `kops aws documentation <https://kops.sigs.k8s.io/getting_started/"
"install/>`_ for more detailed setup instructions."
msgstr ""
"上述步骤足以准备好工作集群。 请参阅 `kops aws 文档 <https://kops.sigs.k8s.io/"
"getting_started/install/>`_ 了解更详细的设置说明。"

#: ../../gettingstarted/k8s-install-kops.rst:94
msgid "Cilium Prerequisites"
msgstr "运行Cilium的前提条件"

#: ../../gettingstarted/k8s-install-kops.rst:96
msgid ""
"Ensure the :ref:`admin_system_reqs` are met, particularly the Linux kernel and "
"key-value store versions."
msgstr ""
"确保满足 :ref:`admin_system_reqs`，尤其是 Linux 内核和key-value store的版本。"

#: ../../gettingstarted/k8s-install-kops.rst:99
msgid ""
"The default AMI satisfies the minimum kernel version required by Cilium, which "
"is what we will use in this guide."
msgstr "默认的 AMI 满足 Cilium 要求的最低内核版本，这是我们将在本指南中使用的。"

#: ../../gettingstarted/k8s-install-kops.rst:104
msgid "Creating a Cluster"
msgstr "创建集群"

#: ../../gettingstarted/k8s-install-kops.rst:106
msgid ""
"Note that you will need to specify the ``--master-zones`` and ``--zones`` for "
"creating the master and worker nodes. The number of master zones should be * "
"odd (1, 3, ...) for HA. For simplicity, you can just use 1 region."
msgstr ""
"请注意，您需要指定 —master-zones 和 —zones 来创建主节点和工作节点。 对于 HA，主"
"区域的数量应该是 * 奇数 (1, 3, …)。 为简单起见，您可以只使用 1 个区域。"

#: ../../gettingstarted/k8s-install-kops.rst:109
msgid ""
"To keep things simple when following this guide, we will use a gossip-based "
"cluster. This means you do not have to create a hosted zone upfront.  cluster "
"``NAME`` variable must end with ``k8s.local`` to use the gossip  protocol. If "
"creating multiple clusters using the same kops user, then make the cluster name "
"unique by adding a prefix such as ``com-company-emailid-``."
msgstr ""
"为了在遵循本指南时保持简单，我们将使用基于 gossip 的集群。 这意味着您不必预先创"
"建托管区域。 cluster ``NAME`` 变量必须以 ``k8s.local`` 结尾才能使用 gossip 协"
"议。 如果使用同一个 kops 用户创建多个集群，则通过添加诸如 com-company-emailid- "
"之类的前缀使集群名称唯一。"

#: ../../gettingstarted/k8s-install-kops.rst:122
msgid "You may be prompted to create a ssh public-private key pair."
msgstr "系统可能会提示您创建 ssh 公-私密钥对。"

#: ../../gettingstarted/k8s-install-kops.rst:129
msgid "(Please see :ref:`appendix_kops`)"
msgstr "（请参阅:ref:`appendix_kops`）"

#: ../../gettingstarted/k8s-install-kops.rst:137
msgid "Deleting a Cluster"
msgstr "删除集群"

#: ../../gettingstarted/k8s-install-kops.rst:139
msgid ""
"To undo the dependencies and other deployment features in AWS from the kops "
"cluster creation, use kops to destroy a cluster *immediately* with the "
"parameter ``--yes``:"
msgstr ""
"要从 kops 集群创建中撤消 AWS 中的依赖项和其他部署功能，请使用 kops 使用参数“—"
"yes”*立即*销毁集群："

#: ../../gettingstarted/k8s-install-kops.rst:149
msgid "Further reading on using Cilium with Kops"
msgstr "进一步阅读将 Cilium 与 Kops 结合使用"

#: ../../gettingstarted/k8s-install-kops.rst:150
msgid ""
"See the `kops networking documentation <https://kops.sigs.k8s.io/networking/"
"cilium/>`_ for more information on the configuration options kops offers."
msgstr ""
"有关 kops 提供的配置选项的更多信息，请参阅`kops 网络文档 <https://kops.sigs.k8s."
"io/networking/cilium/>`_。"

#: ../../gettingstarted/k8s-install-kops.rst:152
msgid ""
"See the `kops cluster spec documentation <https://pkg.go.dev/k8s.io/kops/pkg/"
"apis/kops?tab=doc#CiliumNetworkingSpec>`_ for a comprehensive list of all the "
"options"
msgstr ""
"有关所有选项的完整列表，请参阅 `kops 集群规范文档 <https://pkg.go.dev/k8s.io/"
"kops/pkg/apis/kops?tab=doc#CiliumNetworkingSpec>`_"

#: ../../gettingstarted/k8s-install-kops.rst:156
msgid "Appendix: Details of kops flags used in cluster creation"
msgstr "附录：集群创建中使用的 kops flags的详细信息"

#: ../../gettingstarted/k8s-install-kops.rst:158
msgid ""
"The following section explains all the flags used in create cluster command."
msgstr "以下部分解释了 create cluster 命令中使用的所有flags。"

#: ../../gettingstarted/k8s-install-kops.rst:160
msgid ""
"``--state=${KOPS_STATE_STORE}`` : KOPS uses an S3 bucket to store the state of "
"your cluster and representation of your cluster"
msgstr ""
"``—state=${KOPS_STATE_STORE}`` ：KOPS 使用 S3 存储桶来存储集群的状态和集群的表示"

#: ../../gettingstarted/k8s-install-kops.rst:161
msgid "``--node-count 3`` : No. of worker nodes in the kubernetes cluster."
msgstr "``—node-count 3`` ：kubernetes 集群中工作节点的数量。"

#: ../../gettingstarted/k8s-install-kops.rst:162
msgid ""
"``--topology private`` : Cluster will be created with private topology, what "
"that means is all masters/nodes will be launched in a private subnet in the VPC"
msgstr ""
"``—topology private`` ：将使用私有拓扑创建集群，这意味着所有主节点/节点都将在 "
"VPC 的私有子网中启动"

#: ../../gettingstarted/k8s-install-kops.rst:163
msgid ""
"``--master-zones eu-west-1a,eu-west-1b,eu-west-1c`` : The 3 zones ensure the HA "
"of master nodes, each belonging in a different Availability zones."
msgstr ""
"``—master-zones eu-west-1a,eu-west-1b,eu-west-1c`` : 3 个区域确保主节点的 HA，每"
"个区域都属于不同的可用区域。"

#: ../../gettingstarted/k8s-install-kops.rst:164
msgid ""
"``--zones eu-west-1a,eu-west-1b,eu-west-1c`` : Zones where the worker nodes "
"will be deployed"
msgstr "``—zones eu-west-1a,eu-west-1b,eu-west-1c`` ：将部署工作节点的区域"

#: ../../gettingstarted/k8s-install-kops.rst:165
msgid ""
"``--networking cilium`` : Networking CNI plugin to be used - cilium. You can "
"also use ``cilium-etcd``, which will use a dedicated etcd cluster as key/value "
"store instead of CRDs."
msgstr ""
"``—networking cilium`` ：要使用的网络 CNI 插件 - cilium。 您还可以使用 cilium-"
"etcd ，它将使用专用的 etcd 集群作为key-value store而不是 CRD。"

#: ../../gettingstarted/k8s-install-kops.rst:166
msgid ""
"``--cloud-labels \"Team=Dev,Owner=Admin\"`` :  Labels for your cluster that "
"will be applied to your instances"
msgstr "``—cloud-labels “Team=Dev,Owner=Admin”`` ：将应用于您的实例的集群标签"

#: ../../gettingstarted/k8s-install-kops.rst:167
msgid ""
"``${NAME}`` : Name of the cluster. Make sure the name ends with k8s.local for a "
"gossip based cluster"
msgstr ""
"``${NAME}`` ：集群的名称。 对于基于 gossip 的集群，确保名称以 k8s.local 结尾"

#: ../../gettingstarted/k8s-install-kubeadm.rst:9
msgid "Installation using kubeadm"
msgstr "通过kubeadm安装"

#: ../../gettingstarted/k8s-install-kubeadm.rst:11
msgid ""
"This guide describes deploying Cilium on a Kubernetes cluster created with "
"``kubeadm``."
msgstr ""

#: ../../gettingstarted/k8s-install-kubeadm.rst:14
msgid ""
"For installing ``kubeadm`` on your system, please refer to `the official "
"kubeadm documentation <https://kubernetes.io/docs/setup/production-environment/"
"tools/kubeadm/create-cluster-kubeadm/>`_ The official documentation also "
"describes additional options of kubeadm which are not mentioned here."
msgstr ""

#: ../../gettingstarted/k8s-install-kubeadm.rst:20
msgid ""
"If you are interested in using Cilium's kube-proxy replacement, please follow "
"the :ref:`kubeproxy-free` guide and skip this one."
msgstr ""

#: ../../gettingstarted/k8s-install-kubeadm.rst:24
msgid "Create the cluster"
msgstr ""

#: ../../gettingstarted/k8s-install-kubeadm.rst:26
msgid "Initialize the control plane via executing on it:"
msgstr ""

#: ../../gettingstarted/k8s-install-kubeadm.rst:33
msgid ""
"If you want to use Cilium's kube-proxy replacement, kubeadm needs to skip the "
"kube-proxy deployment phase, so it has to be executed with the ``--skip-"
"phases=addon/kube-proxy`` option:"
msgstr ""

#: ../../gettingstarted/k8s-install-kubeadm.rst:41
msgid "For more information please refer to the :ref:`kubeproxy-free` guide."
msgstr ""

#: ../../gettingstarted/k8s-install-kubeadm.rst:43
#: ../../gettingstarted/kubeproxy-free.rst:43
msgid ""
"Afterwards, join worker nodes by specifying the control-plane node IP address "
"and the token returned by ``kubeadm init``:"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:11
msgid "Installation using Kubespray"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:13
msgid ""
"The guide is to use Kubespray for creating an AWS Kubernetes cluster running "
"Cilium as the CNI. The guide uses:"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:16
msgid "Kubespray v2.6.0"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:17
msgid ""
"Latest `Cilium released version`_ (instructions for using the version are "
"mentioned below)"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:19
msgid ""
"Please consult `Kubespray Prerequisites <https://github.com/kubernetes-sigs/"
"kubespray#requirements>`__ and Cilium :ref:`admin_system_reqs`."
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:24
msgid "Installing Kubespray"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:30
msgid "Install dependencies from ``requirements.txt``"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:39
msgid "Infrastructure Provisioning"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:41
msgid "We will use Terraform for provisioning AWS infrastructure."
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:44
msgid "Configure AWS credentials"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:46
msgid "Export the variables for your AWS credentials"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:56
msgid "Configure Terraform Variables"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:58
msgid ""
"We will start by specifying the infrastructure needed for the Kubernetes "
"cluster."
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:65
msgid ""
"Open the file and change any defaults particularly, the number of master, etcd, "
"and worker nodes. You can change the master and etcd number to 1 for "
"deployments that don't need high availability. By default, this tutorial will "
"create:"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:69
msgid "VPC with 2 public and private subnets"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:70
msgid "Bastion Hosts and NAT Gateways in the Public Subnet"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:71
msgid "Three of each (masters, etcd, and worker nodes) in the Private Subnet"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:72
msgid ""
"AWS ELB in the Public Subnet for accessing the Kubernetes API from the internet"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:74
msgid "Terraform scripts using ``CoreOS`` as base image."
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:76
msgid "Example ``terraform.tfvars`` file:"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:111
msgid "Apply the configuration"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:113
msgid "``terraform init`` to initialize the following modules"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:115
msgid "``module.aws-vpc``"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:116
msgid "``module.aws-elb``"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:117
msgid "``module.aws-iam``"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:123
msgid "Once initialized , execute:"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:129
msgid ""
"This will generate a file, ``aws_kubespray_plan``, depicting an execution plan "
"of the infrastructure that will be created on AWS. To apply, execute:"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:137
msgid ""
"Terraform automatically creates an Ansible Inventory file at ``inventory/"
"hosts``."
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:140
msgid "Installing Kubernetes cluster with Cilium as CNI"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:142
msgid ""
"Kubespray uses Ansible as its substrate for provisioning and orchestration. "
"Once the infrastructure is created, you can run the Ansible playbook to install "
"Kubernetes and all the required dependencies. Execute the below command in the "
"kubespray clone repo, providing the correct path of the AWS EC2 ssh private key "
"in ``ansible_ssh_private_key_file=<path to EC2 SSH private key file>``"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:144
msgid ""
"We recommend using the `latest released Cilium version`_ by passing the "
"variable when running the ``ansible-playbook`` command. For example, you could "
"add the following flag to the command below: ``-e cilium_version=v1.11.0``."
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:153
msgid ""
"If you are interested in configuring your Kubernetes cluster setup, you should "
"consider copying the sample inventory. Then, you can edit the variables in the "
"relevant file in the ``group_vars`` directory."
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:163
msgid "Validate Cluster"
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:165
msgid ""
"To check if cluster is created successfully, ssh into the bastion host with the "
"user ``core``."
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:173
msgid ""
"Execute the commands below from the bastion host. If ``kubectl`` isn't "
"installed on the bastion host, you can login to the master node to test the "
"below commands. You may need to copy the private key to the bastion host to "
"access the master node."
msgstr ""

#: ../../gettingstarted/k8s-install-kubespray.rst:178
msgid "Delete Cluster"
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:11
msgid "Installation on OpenShift OKD"
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:14
msgid "OpenShift Requirements"
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:16
msgid ""
"Choose preferred cloud provider. This guide was tested in AWS, Azure, and GCP "
"from a Linux host."
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:19
msgid ""
"Read `OpenShift documentation <https://docs.okd.io/latest/welcome/index.html>`_ "
"to find out about provider-specific prerequisites."
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:21
msgid ""
"`Get OpenShift Installer <https://github.com/openshift/okd#getting-started>`_."
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:25
msgid ""
"It is highly recommended to read the OpenShift documentation, unless you have "
"installed OpenShift in the past. Here are a few notes that you may find useful."
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:29
msgid ""
"With the AWS provider ``openshift-install`` will not work properly when MFA "
"credentials are stored in ``~/.aws/credentials``, traditional credentials are "
"required."
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:31
msgid ""
"With the Azure provider ``openshift-install`` will prompt for credentials and "
"store them in ``~/.azure/osServicePrincipal.json``, it doesn't simply pickup "
"``az login`` credentials. It's recommended to setup a dedicated service "
"principal and use it."
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:35
msgid ""
"With the GCP provider ``openshift-install`` will only work with a service "
"account key, which has to be set using ``GOOGLE_CREDENTIALS`` environment "
"variable (e.g. ``GOOGLE_CREDENTIALS=service-account.json``). Follow `Openshift "
"Installer documentation <https://github.com/openshift/installer/blob/master/"
"docs/user/gcp/iam.md>`_ to assign required roles to your service account."
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:42
msgid "Create an OpenShift OKD Cluster"
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:44
msgid "First, set the cluster name:"
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:50
msgid "Now, create configuration files:"
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:54
#: ../../gettingstarted/k8s-install-openshift-okd.rst:174
msgid ""
"The sample output below is showing the AWS provider, but it should work the "
"same way with other providers."
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:68
msgid "And set ``networkType: Cilium``:"
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:74
msgid "The resulting configuration will look like this:"
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:112
msgid "You may wish to make a few changes, e.g. increase the number of nodes."
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:114
msgid ""
"If you do change any of the CIDRs, you will need to make sure that Helm values "
"in ``${CLUSTER_NAME}/manifests/cluster-network-07-cilium-ciliumconfig.yaml`` "
"reflect those changes. Namely ``clusterNetwork`` should match "
"``ipv4NativeRoutingCIDR``, ``clusterPoolIPv4PodCIDRList`` and "
"``clusterPoolIPv4MaskSize``. Also make sure that the ``clusterNetwork`` does "
"not conflict with ``machineNetwork`` (which represents the VPC CIDR in AWS)."
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:120
msgid ""
"Ensure that there are multiple replicas of the ``controlPlane``. A single "
"``controlPlane`` will lead to failure to bootstrap the cluster during "
"installation."
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:124
msgid "Next, generate OpenShift manifests:"
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:130
msgid ""
"Next, obtain Cilium manifest from ``cilium/cilium-olm`` repository and copy to "
"``${CLUSTER_NAME}/manifests``:"
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:144
msgid ""
"At this stage manifest directory contains all that is needed to install Cilium. "
"To get a list of the Cilium manifests, run:"
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:151
msgid ""
"You can set any custom Helm values by editing ``${CLUSTER_NAME}/manifests/"
"cluster-network-07-cilium-ciliumconfig.yaml``."
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:153
msgid ""
"It is also possible to update Helm values once the cluster is running by "
"changing the ``CiliumConfig`` object, e.g. with ``kubectl edit ciliumconfig -n "
"cilium cilium``. You may need to restart the Cilium agent pods for certain "
"options to take effect."
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:160
msgid ""
"If you are not using a real OpenShift pull secret, you will not be able to "
"install the Cilium OLM operator using RedHat registry. You can fix this by "
"running:"
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:170
msgid "Create the cluster:"
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:200
msgid "Accessing the cluster"
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:202
msgid ""
"To access the cluster you will need to use ``kubeconfig`` file from the ``"
"${CLUSTER_NAME}/auth`` directory:"
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:209
msgid "Prepare cluster for Cilium connectivity test"
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:211
msgid ""
"In order for Cilium connectivity test pods to run on OpenShift, a simple custom "
"``SecurityContextConstraints`` object is required. It will to allow "
"``hostPort``/``hostNetwork`` that some of the connectivity test pods rely on, "
"it sets only ``allowHostPorts`` and ``allowHostNetwork`` without any other "
"privileges."
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:245
msgid "Deploy the connectivity test"
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:250
msgid "Cleanup after connectivity test"
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:252
msgid "Remove the ``SecurityContextConstraints``:"
msgstr ""

#: ../../gettingstarted/k8s-install-openshift-okd.rst:259
msgid "Delete the cluster"
msgstr ""

#: ../../gettingstarted/k8s-install-rancher-existing-nodes.rst:9
msgid "Installation using Rancher"
msgstr ""

#: ../../gettingstarted/k8s-install-rancher-existing-nodes.rst:12
msgid "Pre-Requisites"
msgstr ""

#: ../../gettingstarted/k8s-install-rancher-existing-nodes.rst:14
msgid "`Rancher Version 2.x <https://rancher.com/docs/rancher/v2.x/en/>`_"
msgstr ""

#: ../../gettingstarted/k8s-install-rancher-existing-nodes.rst:17
msgid "Create a New Cluster"
msgstr ""

#: ../../gettingstarted/k8s-install-rancher-existing-nodes.rst:19
msgid ""
"In Rancher UI, navigate to the Clusters page. In the top right, click on the "
"``Add Cluster`` box to create a new cluster."
msgstr ""

#: ../../gettingstarted/k8s-install-rancher-existing-nodes.rst:24
msgid ""
"On the Add Cluster page select to create a new cluster from ``Existing Nodes``:"
msgstr ""

#: ../../gettingstarted/k8s-install-rancher-existing-nodes.rst:28
msgid ""
"On the ``Add Cluster`` page that opens, provide a name for the cluster. Next to "
"the ``Cluster Options`` section click the box to ``Edit as YAML``. The "
"configuration for the cluster will open up in an editor in the window."
msgstr ""

#: ../../gettingstarted/k8s-install-rancher-existing-nodes.rst:34
msgid "Remove all configuration pertaining to the default network plugin:"
msgstr ""

#: ../../gettingstarted/k8s-install-rancher-existing-nodes.rst:38
msgid "Ensure that network plugin type ``none`` is specified:"
msgstr ""

#: ../../gettingstarted/k8s-install-rancher-existing-nodes.rst:42
msgid ""
"Make any additional changes to the configuration that are appropriate for your "
"environment. When you are ready, click ``Create`` and Rancher will create the "
"cluster in the ``Provisioning`` state."
msgstr ""

#: ../../gettingstarted/k8s-install-rancher-existing-nodes.rst:48
msgid ""
"Rancher will also present you with instructions on how to add nodes to the "
"cluster:"
msgstr ""

#: ../../gettingstarted/k8s-install-rancher-existing-nodes.rst:53
msgid ""
"Next, add nodes to the cluster using the Rancher-provided Docker commands. Be "
"sure to add the appropriate number of nodes required for your cluster. After a "
"few minutes, you will see that the Nodes overview will show an error message in "
"the Rancher UI:"
msgstr ""

#: ../../gettingstarted/k8s-install-rancher-existing-nodes.rst:60
msgid ""
"This error is expected. The Kubernetes components are not able to fully "
"communicate with each other because a CNI plugin has not been installed yet."
msgstr ""

#: ../../gettingstarted/k8s-install-rancher-existing-nodes.rst:63
msgid ""
"In the Rancher UI, navigate to ``Tools`` -> ``Catalogs`` and click the ``Add "
"Catalog`` button:"
msgstr ""

#: ../../gettingstarted/k8s-install-rancher-existing-nodes.rst:68
msgid ""
"In the window that opens, add the official Cilium Helm v3 chart repository to "
"the Rancher catalog:"
msgstr ""

#: ../../gettingstarted/k8s-install-rancher-existing-nodes.rst:73
msgid "Once added, you should see the Cilium repository in the catalog list:"
msgstr ""

#: ../../gettingstarted/k8s-install-rancher-existing-nodes.rst:77
msgid ""
"Navigate to the Global Apps list and click the button to ``Launch`` an "
"application:"
msgstr ""

#: ../../gettingstarted/k8s-install-rancher-existing-nodes.rst:81
msgid ""
"Provide a name for the application and select the System project in your "
"cluster as the application target:"
msgstr ""

#: ../../gettingstarted/k8s-install-rancher-existing-nodes.rst:86
msgid "Be sure to configure the application to have cluster-wide scope:"
msgstr ""

#: ../../gettingstarted/k8s-install-rancher-existing-nodes.rst:90
msgid ""
"Configure any additional values for the Cilium chart and click ``Next``. The "
"application should deploy within the target cluster and project."
msgstr ""

#: ../../gettingstarted/k8s-install-rancher-existing-nodes.rst:93
msgid ""
"After a few minutes, you should see all nodes as active, and proper deployments "
"of all Cilium objects."
msgstr ""

#: ../../gettingstarted/k8s-install-rke.rst:11
msgid "Installation using Rancher Kubernetes Engine"
msgstr ""

#: ../../gettingstarted/k8s-install-rke.rst:13
msgid ""
"This guide walks you through installation of Cilium on `Rancher Kubernetes "
"Engine <https://rancher.com/products/rke/>`_, a CNCF-certified Kubernetes "
"distribution that runs entirely within Docker containers. RKE solves the common "
"frustration of installation complexity with Kubernetes by removing most host "
"dependencies and presenting a stable path for deployment, upgrades, and "
"rollbacks."
msgstr ""

#: ../../gettingstarted/k8s-install-rke.rst:20
msgid "Install a Cluster Using RKE"
msgstr ""

#: ../../gettingstarted/k8s-install-rke.rst:22
msgid ""
"The first step is to install a cluster based on the `RKE Installation Guide "
"<https://rancher.com/docs/rke/latest/en/installation/>`_. When creating the "
"cluster, make sure to `change the default network plugin <https://rancher.com/"
"docs/rke/latest/en/config-options/add-ons/network-plugins/custom-network-plugin-"
"example/>`_ in the config.yaml file."
msgstr ""

#: ../../gettingstarted/k8s-install-rke.rst:26
msgid "Change:"
msgstr ""

#: ../../gettingstarted/k8s-install-rke.rst:47
msgid "Helm v3"
msgstr ""

#: ../../gettingstarted/kafka.rst:11
msgid "Securing a Kafka cluster"
msgstr ""

#: ../../gettingstarted/kafka.rst:13
msgid ""
"This document serves as an introduction to using Cilium to enforce Kafka-aware "
"security policies.  It is a detailed walk-through of getting a single-node "
"Cilium environment running on your machine. It is designed to take 15-30 "
"minutes."
msgstr ""

#: ../../gettingstarted/kafka.rst:23
msgid ""
"Now that we have Cilium deployed and ``kube-dns`` operating correctly we can "
"deploy our demo Kafka application.  Since our first demo of Cilium + HTTP-aware "
"security policies was Star Wars-themed we decided to do the same for Kafka.  "
"While the `HTTP-aware Cilium  Star Wars demo <https://cilium.io/blog/2017/5/4/"
"demo-may-the-force-be-with-you/>`_ showed how the Galactic Empire used HTTP-"
"aware security policies to protect the Death Star from the Rebel Alliance, this "
"Kafka demo shows how the lack of Kafka-aware security policies allowed the "
"Rebels to steal the Death Star plans in the first place."
msgstr ""

#: ../../gettingstarted/kafka.rst:31
msgid ""
"Kafka is a powerful platform for passing datastreams between different "
"components of an application. A cluster of \"Kafka brokers\" connect nodes that "
"\"produce\" data into a data stream, or \"consume\" data from a datastream.   "
"Kafka refers to each datastream as a \"topic\". Because scalable and highly-"
"available Kafka clusters are non-trivial to run, the same cluster of Kafka "
"brokers often handles many different topics at once (read this `Introduction to "
"Kafka <https://kafka.apache.org/intro>`_ for more background)."
msgstr ""

#: ../../gettingstarted/kafka.rst:38
msgid ""
"In our simple example, the Empire uses a Kafka cluster to handle two different "
"topics:"
msgstr ""

#: ../../gettingstarted/kafka.rst:40
msgid ""
"*empire-announce* : Used to broadcast announcements to sites spread across the "
"galaxy"
msgstr ""

#: ../../gettingstarted/kafka.rst:41
msgid ""
"*deathstar-plans* : Used by a small group of sites coordinating on building the "
"ultimate battlestation."
msgstr ""

#: ../../gettingstarted/kafka.rst:45
msgid ""
"*kafka-broker* : A single pod running Kafka and Zookeeper representing the "
"Kafka cluster (label app=kafka)."
msgstr ""

#: ../../gettingstarted/kafka.rst:47
msgid ""
"*empire-hq* : A pod representing the Empire's Headquarters, which is the only "
"pod that should produce messages to *empire-announce* or *deathstar-plans* "
"(label app=empire-hq)."
msgstr ""

#: ../../gettingstarted/kafka.rst:49
msgid ""
"*empire-backup* : A secure backup facility located in `Scarif <https://starwars."
"fandom.com/wiki/Scarif_vault>`_ , which is allowed to \"consume\" from the "
"secret *deathstar-plans* topic (label app=empire-backup)."
msgstr ""

#: ../../gettingstarted/kafka.rst:51
msgid ""
"*empire-outpost-8888* : A random outpost in the empire.  It needs to \"consume"
"\" messages from the *empire-announce* topic (label app=empire-outpost)."
msgstr ""

#: ../../gettingstarted/kafka.rst:53
msgid ""
"*empire-outpost-9999* : Another random outpost in the empire that \"consumes\" "
"messages from the *empire-announce* topic (label app=empire-outpost)."
msgstr ""

#: ../../gettingstarted/kafka.rst:56
msgid ""
"All pods other than *kafka-broker* are Kafka clients, which need access to the "
"*kafka-broker* container on TCP port 9092 in order to send Kafka protocol "
"messages."
msgstr ""

#: ../../gettingstarted/kafka.rst:61
msgid ""
"The file ``kafka-sw-app.yaml`` contains a Kubernetes Deployment for each of the "
"pods described above, as well as a Kubernetes Service for both Kafka and "
"Zookeeper."
msgstr ""

#: ../../gettingstarted/kafka.rst:76
msgid ""
"Kubernetes will deploy the pods and service  in the background. Running "
"``kubectl get svc,pods`` will inform you about the progress of the operation. "
"Each pod will go through several states until it reaches ``Running`` at which "
"point the setup is ready."
msgstr ""

#: ../../gettingstarted/kafka.rst:98
msgid "Setup Client Terminals"
msgstr ""

#: ../../gettingstarted/kafka.rst:100
msgid ""
"First we will open a set of windows to represent the different Kafka clients "
"discussed above. For consistency, we recommend opening them in the pattern "
"shown in the image below, but this is optional."
msgstr ""

#: ../../gettingstarted/kafka.rst:105
msgid ""
"In each window, use copy-paste to have each terminal provide a shell inside "
"each pod."
msgstr ""

#: ../../gettingstarted/kafka.rst:107
msgid "empire-hq terminal:"
msgstr ""

#: ../../gettingstarted/kafka.rst:113
msgid "empire-backup terminal:"
msgstr ""

#: ../../gettingstarted/kafka.rst:119
msgid "outpost-8888 terminal:"
msgstr ""

#: ../../gettingstarted/kafka.rst:125
msgid "outpost-9999 terminal:"
msgstr ""

#: ../../gettingstarted/kafka.rst:133
msgid "Test Basic Kafka Produce & Consume"
msgstr ""

#: ../../gettingstarted/kafka.rst:135
msgid ""
"First, let's start the consumer clients listening to their respective Kafka "
"topics.  All of the consumer commands below will hang intentionally, waiting to "
"print data they consume from the Kafka topic:"
msgstr ""

#: ../../gettingstarted/kafka.rst:138
msgid ""
"In the *empire-backup* window, start listening on the top-secret *deathstar-"
"plans* topic:"
msgstr ""

#: ../../gettingstarted/kafka.rst:144
msgid "In the *outpost-8888* window, start listening to *empire-announcement*:"
msgstr ""

#: ../../gettingstarted/kafka.rst:150
msgid "Do the same in the *outpost-9999* window:"
msgstr ""

#: ../../gettingstarted/kafka.rst:156
msgid ""
"Now from the *empire-hq*, first produce a message to the *empire-announce* "
"topic:"
msgstr ""

#: ../../gettingstarted/kafka.rst:162
msgid ""
"This message will be posted to the *empire-announce* topic, and shows up in "
"both the *outpost-8888* and *outpost-9999* windows who consume that topic.   It "
"will not show up in *empire-backup*."
msgstr ""

#: ../../gettingstarted/kafka.rst:165
msgid ""
"*empire-hq* can also post a version of the top-secret deathstar plans to the "
"*deathstar-plans* topic:"
msgstr ""

#: ../../gettingstarted/kafka.rst:171
msgid ""
"This message shows up in the *empire-backup* window, but not for the outposts."
msgstr ""

#: ../../gettingstarted/kafka.rst:173
msgid "Congratulations, Kafka is working as expected :)"
msgstr ""

#: ../../gettingstarted/kafka.rst:176
msgid "The Danger of a Compromised Kafka Client"
msgstr ""

#: ../../gettingstarted/kafka.rst:178
msgid ""
"But what if a rebel spy gains access to any of the remote outposts that act as "
"Kafka clients? Since every client has access to the Kafka broker on port 9092, "
"it can do some bad stuff. For starters, the outpost container can actually "
"switch roles from a consumer to a producer, sending \"malicious\" data to all "
"other consumers on the topic."
msgstr ""

#: ../../gettingstarted/kafka.rst:183
msgid ""
"To prove this, kill the existing ``kafka-consume.sh`` command in the "
"outpost-9999 window by typing control-C and instead run:"
msgstr ""

#: ../../gettingstarted/kafka.rst:190
msgid ""
"Uh oh!  Outpost-8888 and all of the other outposts in the empire have now "
"received this fake announcement."
msgstr ""

#: ../../gettingstarted/kafka.rst:192
msgid ""
"But even more nasty from a security perspective is that the outpost container "
"can access any topic on the kafka-broker."
msgstr ""

#: ../../gettingstarted/kafka.rst:195
msgid "In the outpost-9999 container, run:"
msgstr ""

#: ../../gettingstarted/kafka.rst:202
msgid ""
"We see that any outpost can actually access the secret deathstar plans.  Now we "
"know how the rebels got access to them!"
msgstr ""

#: ../../gettingstarted/kafka.rst:206
msgid "Securing Access to Kafka with Cilium"
msgstr ""

#: ../../gettingstarted/kafka.rst:208
msgid ""
"Obviously, it would be much more secure to limit each pod's access to the Kafka "
"broker to be least privilege (i.e., only what is needed for the app to operate "
"correctly and nothing more)."
msgstr ""

#: ../../gettingstarted/kafka.rst:211
msgid ""
"We can do that with the following Cilium security policy.   As with Cilium HTTP "
"policies, we can write policies that identify pods by labels, and then limit "
"the traffic in/out of this pod.  In this case, we'll create a policy that "
"identifies the exact traffic that should be allowed to reach the Kafka broker, "
"and deny the rest."
msgstr ""

#: ../../gettingstarted/kafka.rst:216
msgid ""
"As an example, a policy could limit containers with label *app=empire-outpost* "
"to only be able to consume topic *empire-announce*, but would block any attempt "
"by a compromised container (e.g., empire-outpost-9999) from producing to "
"*empire-announce* or consuming from *deathstar-plans*."
msgstr ""

#: ../../gettingstarted/kafka.rst:222
msgid ""
"Here is the *CiliumNetworkPolicy* rule that limits access of pods with label "
"*app=empire-outpost* to only consume on topic *empire-announce*:"
msgstr ""

#: ../../gettingstarted/kafka.rst:230
msgid ""
"The above rule applies to inbound (i.e., \"ingress\") connections to kafka-"
"broker pods (as indicated by \"app: kafka\" in the \"endpointSelector\" "
"section).  The rule will apply to connections from pods with label \"app: "
"empire-outpost\" as indicated by the \"fromEndpoints\" section.   The rule "
"explicitly matches Kafka connections destined to TCP 9092, and allows consume/"
"produce actions on various topics of interest. For example we are allowing "
"*consume* from topic *empire-announce* in this case."
msgstr ""

#: ../../gettingstarted/kafka.rst:237
msgid ""
"The full policy adds two additional rules that permit the legitimate \"produce"
"\" (topic *empire-announce* and topic *deathstar-plans*) from *empire-hq* and "
"the legitimate consume  (topic = \"deathstar-plans\") from *empire-backup*.  "
"The full policy can be reviewed by opening the URL in the command below in a "
"browser."
msgstr ""

#: ../../gettingstarted/kafka.rst:242
msgid ""
"Apply this Kafka-aware network security policy using ``kubectl`` in the main "
"window:"
msgstr ""

#: ../../gettingstarted/kafka.rst:248
msgid ""
"If we then again try to produce a message from outpost-9999 to *empire-"
"annnounce*, it is denied. Type control-c and then run:"
msgstr ""

#: ../../gettingstarted/kafka.rst:257
msgid ""
"This is because the policy does not allow messages with role = \"produce\" for "
"topic \"empire-announce\" from containers with label app = empire-outpost.  Its "
"worth noting that we don't simply drop the message (which could easily be "
"confused with a network error), but rather we respond with the Kafka access "
"denied error (similar to how HTTP would return an error code of 403 "
"unauthorized)."
msgstr ""

#: ../../gettingstarted/kafka.rst:262
msgid ""
"Likewise, if the outpost container ever tries to consume from topic *deathstar-"
"plans*, it is denied, as role = consume is only allowed for topic *empire-"
"announce*."
msgstr ""

#: ../../gettingstarted/kafka.rst:265
msgid "To test, from the outpost-9999 terminal, run:"
msgstr ""

#: ../../gettingstarted/kafka.rst:272
msgid ""
"This is blocked as well, thanks to the Cilium network policy. Imagine how "
"different things would have been if the empire had been using Cilium from the "
"beginning!"
msgstr ""

#: ../../gettingstarted/kafka.rst:278
msgid ""
"You have now installed Cilium, deployed a demo app, and tested both L7 Kafka-"
"aware network security policies.  To clean up, run:"
msgstr ""

#: ../../gettingstarted/kata.rst:11
msgid "Kata Containers with Cilium"
msgstr ""

#: ../../gettingstarted/kata.rst:13
msgid ""
"`Kata Containers <https://katacontainers.io/>`_ is an open source project that "
"provides a secure container runtime with lightweight virtual machines that feel "
"and perform like containers, but provide stronger workload isolation using "
"hardware virtualization technology as a second layer of defense.  Kata "
"Containers implements OCI runtime spec, just like ``runc`` that is used by "
"Docker. Cilium can be used along with Kata Containers, using both enables "
"higher degree of security. Kata Containers enhances security in the compute "
"layer, while Cilium provides policy and observability in the networking layer."
msgstr ""

#: ../../gettingstarted/kata.rst:22
msgid ""
"This guide shows how to install Cilium along with Kata Containers. It assumes "
"that you have already followed the official `Kata Containers installation user "
"guide <https://github.com/kata-containers/documentation/tree/master/install>`_ "
"to get the Kata Containers runtime up and running on your platform of choice "
"but that you haven't yet setup Kubernetes."
msgstr ""

#: ../../gettingstarted/kata.rst:29
msgid ""
"This guide has been validated by following the Kata Containers guide for Google "
"Compute Engine (GCE) and using Ubuntu 18.04 LTS with the packaged version of "
"Kata Containers, CRI-containerd and Kubernetes 1.18.3."
msgstr ""

#: ../../gettingstarted/kata.rst:34
msgid "Setup Kubernetes with CRI"
msgstr ""

#: ../../gettingstarted/kata.rst:36
msgid ""
"Kata Containers runtime is an OCI compatible runtime and cannot directly "
"interact with the CRI API level. For this reason, it relies on a CRI "
"implementation to translate CRI into OCI. At the time of writing this guide, "
"there are two supported ways called CRI-O and CRI-containerd. It is up to you "
"to choose the one that you want, but you have to pick one."
msgstr ""

#: ../../gettingstarted/kata.rst:42
msgid ""
"Refer to the section :ref:`k8s_requirements` for detailed instruction on how to "
"prepare your Kubernetes environment and make sure to use Kubernetes >= 1.12. "
"Then, follow the `official guide to run Kata Containers with Kubernetes "
"<https://github.com/kata-containers/documentation/blob/master/how-to/run-kata-"
"with-k8s.md>`_."
msgstr ""

#: ../../gettingstarted/kata.rst:48
msgid ""
"Minimum version of kubernetes 1.12 is required to use the RuntimeClass Feature "
"for Kata Container runtime described below."
msgstr ""

#: ../../gettingstarted/kata.rst:51
msgid "With your Kubernetes cluster ready, you can now proceed to deploy Cilium."
msgstr ""

#: ../../gettingstarted/kata.rst:62
msgid "Using CRI-O"
msgstr ""

#: ../../gettingstarted/kata.rst:70
msgid "Using CRI-containerd"
msgstr ""

#: ../../gettingstarted/kata.rst:80
msgid ""
"Kata containers do not work with :ref:`host-services`, or with :ref:`kube-proxy "
"replacement <kubeproxy-free>` in strict mode. These features should be disabled "
"with ``--set hostServices.enabled=false`` (default) and ``--set "
"kubeProxyReplacement=disabled`` (or ``partial``)."
msgstr ""

#: ../../gettingstarted/kata.rst:85
msgid ""
"Both features rely on socket-based load-balancing, which is not possible given "
"that Kata containers are virtual machines running with their own kernel. For "
"kube-proxy replacement, this limitation is tracked with :gh-issue:`15437`."
msgstr ""

#: ../../gettingstarted/kata.rst:93
msgid "Run Kata Containers with Cilium CNI"
msgstr ""

#: ../../gettingstarted/kata.rst:95
msgid ""
"Now that your Kubernetes cluster is configured with the Kata Containers runtime "
"and Cilium as the CNI, you can run a sample workload by following `these "
"instructions <https://github.com/kata-containers/packaging/tree/master/kata-"
"deploy#run-a-sample-workload>`_."
msgstr ""

#: ../../gettingstarted/kind.rst:11
msgid "Getting Started Using Kind"
msgstr ""

#: ../../gettingstarted/kind.rst:13
msgid ""
"This guide uses `kind <https://kind.sigs.k8s.io/>`_ to demonstrate deployment "
"and operation of Cilium in a multi-node Kubernetes cluster running locally on "
"Docker."
msgstr ""

#: ../../gettingstarted/kind.rst:18
msgid "Install Dependencies"
msgstr ""

#: ../../gettingstarted/kind-install-deps.rst:1
msgid ""
"Install ``docker`` stable as described in `Install Docker Engine <https://docs."
"docker.com/engine/install/>`_"
msgstr ""

#: ../../gettingstarted/kind-install-deps.rst:4
msgid ""
"Install ``kubectl`` version >= v1.14.0 as described in the `Kubernetes Docs "
"<https://kubernetes.io/docs/tasks/tools/install-kubectl/>`_"
msgstr ""

#: ../../gettingstarted/kind-install-deps.rst:7
msgid ""
"Install ``helm`` >= v3.0.3 per Helm documentation: `Installing Helm <https://"
"helm.sh/docs/intro/install/>`_"
msgstr ""

#: ../../gettingstarted/kind.rst:23
msgid "Configure kind"
msgstr ""

#: ../../gettingstarted/kind-configure.rst:1
msgid ""
"Configuring kind cluster creation is done using a YAML configuration file. This "
"step is necessary in order to disable the default CNI and replace it with "
"Cilium."
msgstr ""

#: ../../gettingstarted/kind-configure.rst:5
msgid ""
"Create a :download:`kind-config.yaml <./kind-config.yaml>` file based on the "
"following template. It will create a cluster with 3 worker nodes and 1 control-"
"plane node."
msgstr ""

#: ../../gettingstarted/kind-configure.rst:12
msgid ""
"By default, the latest version of Kubernetes from when the kind release was "
"created is used."
msgstr ""

#: ../../gettingstarted/kind-configure.rst:15
msgid ""
"To change the version of Kubernetes being run,  ``image`` has to be defined for "
"each node. See the `Node Configuration <https://kind.sigs.k8s.io/docs/user/"
"configuration/#nodes>`_ documentation for more information."
msgstr ""

#: ../../gettingstarted/kind-configure.rst:21
msgid "By default, kind uses the following pod and service subnets::"
msgstr ""

#: ../../gettingstarted/kind-configure.rst:26
msgid ""
"If any of these subnets conflicts with your local network address range, update "
"the ``networking`` section of the kind configuration file to specify different "
"subnets that do not conflict or you risk having connectivity issues when "
"deploying Cilium. For example:"
msgstr ""

#: ../../gettingstarted/kind.rst:28
msgid "Create a cluster"
msgstr ""

#: ../../gettingstarted/kind-create-cluster.rst:1
msgid ""
"To create a cluster with the configuration defined above, pass the ``kind-"
"config.yaml`` you created with the ``--config`` flag of kind."
msgstr ""

#: ../../gettingstarted/kind-create-cluster.rst:8
msgid "After a couple of seconds or minutes, a 4 nodes cluster should be created."
msgstr ""

#: ../../gettingstarted/kind-create-cluster.rst:10
msgid ""
"A new ``kubectl`` context (``kind-kind``) should be added to ``KUBECONFIG`` or, "
"if unset, to ``${HOME}/.kube/config``:"
msgstr ""

#: ../../gettingstarted/kind-create-cluster.rst:18
msgid ""
"The cluster nodes will remain in state ``NotReady`` until Cilium is deployed. "
"This behavior is expected."
msgstr ""

#: ../../gettingstarted/kind-preload.rst:1
msgid "Preload the ``cilium`` image into each worker node in the kind cluster:"
msgstr ""

#: ../../gettingstarted/kind.rst:40
msgid "Then, install Cilium release via Helm:"
msgstr ""

#: ../../gettingstarted/kind.rst:57
msgid ""
"To fully enable Cilium's kube-proxy replacement (:ref:`kubeproxy-free`), cgroup "
"v2 needs to be enabled by setting the kernel ``systemd."
"unified_cgroup_hierarchy=1`` parameter. Also, cgroup v1 controllers ``net_cls`` "
"and ``net_prio`` have to be disabled, or cgroup v1 has to be disabled (e.g. by "
"setting the kernel ``cgroup_no_v1=\"all\"`` parameter). This ensures that Kind "
"nodes have their own cgroup namespace, and Cilium can attach BPF programs at "
"the right cgroup hierarchy. To verify this, run the following commands on the "
"host, and check that the output values are different."
msgstr ""

#: ../../gettingstarted/kind.rst:70
msgid ""
"See the `Pull Request <https://github.com/cilium/cilium/pull/16259>`__ for more "
"details."
msgstr ""

#: ../../gettingstarted/kind.rst:80
msgid "Unable to contact k8s api-server"
msgstr ""

#: ../../gettingstarted/kind.rst:82
msgid "In the :ref:`Cilium agent logs <ts_agent_logs>` you will see::"
msgstr ""

#: ../../gettingstarted/kind.rst:88
msgid ""
"As Kind is running nodes as containers in Docker, they're sharing your host "
"machines' kernel. If :ref:`host-services` wasn't disabled, the eBPF programs "
"attached by Cilium may be out of date and no longer routing api-server requests "
"to the current ``kind-control-plane`` container."
msgstr ""

#: ../../gettingstarted/kind.rst:92
msgid ""
"Recreating the kind cluster and using the helm command :ref:"
"`kind_install_cilium` will detach the inaccurate eBPF programs."
msgstr ""

#: ../../gettingstarted/kind.rst:96
msgid "Crashing Cilium agent pods"
msgstr ""

#: ../../gettingstarted/kind.rst:98
msgid ""
"Check if Cilium agent pods are crashing with following logs. This may indicate "
"that you are deploying a kind cluster in an environment where Cilium is already "
"running (for example, in the Cilium development VM). This can also happen if "
"you have other overlapping BPF ``cgroup`` type programs attached to the parent "
"``cgroup`` hierarchy of the kind container nodes. In such cases, either tear "
"down Cilium, or manually detach the overlapping BPF ``cgroup`` programs running "
"in the parent ``cgroup`` hierarchy by following the `bpftool documentation "
"<https://manpages.ubuntu.com/manpages/focal/man8/bpftool-cgroup.8.html>`_. For "
"more information, see the `Pull Request <https://github.com/cilium/cilium/"
"pull/16259>`__."
msgstr ""

#: ../../gettingstarted/kind.rst:118
msgid "With Kind we can simulate Cluster Mesh in a sandbox too."
msgstr ""

#: ../../gettingstarted/kind.rst:121
msgid "Kind Configuration"
msgstr ""

#: ../../gettingstarted/kind.rst:123
msgid ""
"This time we need to create (2) ``config.yaml``, one for each kubernetes "
"cluster. We will explicitly configure their ``pod-network-cidr`` and ``service-"
"cidr`` to not overlap."
msgstr ""

#: ../../gettingstarted/kind.rst:126
msgid "Example ``kind-cluster1.yaml``:"
msgstr ""

#: ../../gettingstarted/kind.rst:142
msgid "Example ``kind-cluster2.yaml``:"
msgstr ""

#: ../../gettingstarted/kind.rst:159
msgid "Create Kind Clusters"
msgstr ""

#: ../../gettingstarted/kind.rst:161
msgid "We can now create the respective clusters:"
msgstr ""

#: ../../gettingstarted/kind.rst:171
msgid ""
"We can deploy Cilium, and complete setup by following the Cluster Mesh guide "
"with :ref:`gs_clustermesh`. For Kind, we'll want to deploy the ``NodePort`` "
"service into the ``kube-system`` namespace."
msgstr ""

#: ../../gettingstarted/kube-router.rst:11
msgid "Using kube-router to run BGP"
msgstr ""

#: ../../gettingstarted/kube-router.rst:13
msgid ""
"This guide explains how to configure Cilium and kube-router to co-operate to "
"use kube-router for BGP peering and route propagation and Cilium for policy "
"enforcement and load-balancing."
msgstr ""

#: ../../gettingstarted/kube-router.rst:20
msgid "Deploy kube-router"
msgstr ""

#: ../../gettingstarted/kube-router.rst:22
msgid "Download the kube-router DaemonSet template:"
msgstr ""

#: ../../gettingstarted/kube-router.rst:28
msgid ""
"Open the file ``generic-kuberouter-only-advertise-routes.yaml`` and edit the "
"``args:`` section. The following arguments are **required** to be set to "
"exactly these values:"
msgstr ""

#: ../../gettingstarted/kube-router.rst:40
msgid ""
"The following arguments are **optional** and may be set according to your "
"needs.  For the purpose of keeping this guide simple, the following values are "
"being used which require the least preparations in your cluster. Please see the "
"`kube-router user guide <https://github.com/cloudnativelabs/kube-router/blob/"
"master/docs/user-guide.md>`_ for more information."
msgstr ""

#: ../../gettingstarted/kube-router.rst:55
msgid ""
"The following arguments are **optional** and should be set if you want BGP "
"peering with an external router. This is useful if you want externally routable "
"Kubernetes Pod and Service IPs. Note the values used here should be changed to "
"whatever IPs and ASNs are configured on your external router."
msgstr ""

#: ../../gettingstarted/kube-router.rst:66
msgid ""
"Apply the DaemonSet file to deploy kube-router and verify it has come up "
"correctly:"
msgstr ""

#: ../../gettingstarted/kube-router.rst:82
msgid ""
"In order for routing to be delegated to kube-router, tunneling/encapsulation "
"must be disabled. This is done by setting the ``tunnel=disabled`` in the "
"ConfigMap ``cilium-config`` or by adjusting the DaemonSet to run the ``cilium-"
"agent`` with the argument ``--tunnel=disabled``. Moreover, in the same "
"ConfigMap, we must explicitly set ``ipam: kubernetes`` since kube-router pulls "
"the pod CIDRs directly from K8s:"
msgstr ""

#: ../../gettingstarted/kube-router.rst:99
msgid ""
"You can then install Cilium according to the instructions in section :ref:"
"`ds_deploy`."
msgstr ""

#: ../../gettingstarted/kube-router.rst:102
msgid "Ensure that Cilium is up and running:"
msgstr ""

#: ../../gettingstarted/kube-router.rst:114
msgid "Verify Installation"
msgstr ""

#: ../../gettingstarted/kube-router.rst:116
msgid "Verify that kube-router has installed routes:"
msgstr ""

#: ../../gettingstarted/kube-router.rst:127
msgid ""
"In the above example, we see three categories of routes that have been "
"installed:"
msgstr ""

#: ../../gettingstarted/kube-router.rst:130
msgid ""
"*Local PodCIDR:* This route points to all pods running on the host and makes "
"these pods available to * ``10.2.0.0/24 via 10.2.0.172 dev cilium_host src "
"10.2.0.172``"
msgstr ""

#: ../../gettingstarted/kube-router.rst:133
msgid ""
"*BGP route:* This type of route is installed if kube-router determines that the "
"remote PodCIDR can be reached via a router known to the local host. It will "
"instruct pod to pod traffic to be forwarded directly to that router without "
"requiring any encapsulation. * ``10.2.1.0/24 via 172.0.51.175 dev eth0 proto "
"17``"
msgstr ""

#: ../../gettingstarted/kube-router.rst:138
msgid ""
"*IPIP tunnel route:*  If no direct routing path exists, kube-router will fall "
"back to using an overlay and establish an IPIP tunnel between the nodes. * "
"``10.2.2.0/24 dev tun-172011760 proto 17 src 172.0.50.227`` * ``10.2.3.0/24 dev "
"tun-1720186231 proto 17 src 172.0.50.227``"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:11
msgid "Kubernetes Without kube-proxy"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:13
msgid ""
"This guide explains how to provision a Kubernetes cluster without ``kube-"
"proxy``, and to use Cilium to fully replace it. For simplicity, we will use "
"``kubeadm`` to bootstrap the cluster."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:17
msgid ""
"For installing ``kubeadm`` and for more provisioning options please refer to "
"`the official kubeadm documentation <https://kubernetes.io/docs/setup/"
"production-environment/tools/kubeadm/create-cluster-kubeadm/>`_."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:22
msgid ""
"Cilium's kube-proxy replacement depends on the :ref:`host-services` feature, "
"therefore a v4.19.57, v5.1.16, v5.2.0 or more recent Linux kernel is required. "
"Linux kernels v5.3 and v5.8 add additional features that Cilium can use to "
"further optimize the kube-proxy replacement implementation."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:27
msgid ""
"Note that v5.0.y kernels do not have the fix required to run the kube-proxy "
"replacement since at this point in time the v5.0.y stable kernel is end-of-life "
"(EOL) and not maintained anymore on kernel.org. For individual distribution "
"maintained kernels, the situation could differ. Therefore, please check with "
"your distribution."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:34
msgid "Quick-Start"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:36
msgid ""
"Initialize the control-plane node via ``kubeadm init`` and skip the "
"installation of the ``kube-proxy`` add-on:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:52
msgid ""
"Please ensure that `kubelet <https://kubernetes.io/docs/reference/command-line-"
"tools-reference/kubelet/>`_'s ``--node-ip`` is set correctly on each worker if "
"you have multiple interfaces. Cilium's kube-proxy replacement may not work "
"correctly otherwise. You can validate this by running ``kubectl get nodes -o "
"wide`` to see whether each node has an ``InternalIP`` which is assigned to a "
"device with the same name on each node."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:60
msgid ""
"For existing installations with ``kube-proxy`` running as a DaemonSet, remove "
"it by using the following commands below. **Careful:** Be aware that this will "
"break existing service connections. It will also stop service related traffic "
"until the Cilium replacement has been installed:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:75
msgid ""
"Next, generate the required YAML files and deploy them. **Important:** Replace "
"``REPLACE_WITH_API_SERVER_IP`` and ``REPLACE_WITH_API_SERVER_PORT`` below with "
"the concrete control-plane node IP address and the kube-apiserver port number "
"reported by ``kubeadm init`` (usually, it is port ``6443``)."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:80
msgid ""
"Specifying this is necessary as ``kubeadm init`` is run explicitly without "
"setting up kube-proxy and as a consequence, although it exports "
"``KUBERNETES_SERVICE_HOST`` and ``KUBERNETES_SERVICE_PORT`` with a ClusterIP of "
"the kube-apiserver service to the environment, there is no kube-proxy in our "
"setup provisioning that service. The Cilium agent therefore needs to be made "
"aware of this information through below configuration."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:97
msgid ""
"Cilium will automatically mount cgroup v2 filesystem required to attach BPF "
"cgroup programs by default at the path ``/run/cilium/cgroupv2``. In order to do "
"that, it needs to mount the host ``/proc`` inside an init container launched by "
"the daemonset temporarily. If you need to disable the auto-mount, specify ``--"
"set cgroup.autoMount.enabled=false``, and set the host mount point where cgroup "
"v2 filesystem is already mounted by using ``--set cgroup.hostRoot``. For "
"example, if not already mounted, you can mount cgroup v2 filesystem by running "
"the below command on the host, and specify ``--set cgroup.hostRoot=/sys/fs/"
"cgroup``."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:110
msgid ""
"This will install Cilium as a CNI plugin with the eBPF kube-proxy replacement "
"to implement handling of Kubernetes services of type ClusterIP, NodePort, "
"LoadBalancer and services with externalIPs. On top of that the eBPF kube-proxy "
"replacement also supports hostPort for containers such that using portmap is "
"not necessary anymore."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:115
msgid ""
"Finally, as a last step, verify that Cilium has come up correctly on all nodes "
"and is ready to operate:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:125
msgid ""
"Note, in above Helm configuration, the ``kubeProxyReplacement`` has been set to "
"``strict`` mode. This means that the Cilium agent will bail out in case the "
"underlying Linux kernel support is missing."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:129
msgid ""
"By default, Helm sets ``kubeProxyReplacement=probe``, which automatically "
"disables a subset of the features to implement the kube-proxy replacement "
"instead of bailing out if the kernel support is missing. This makes the "
"assumption that Cilium's eBPF kube-proxy replacement would co-exist with kube-"
"proxy on the system to optimize Kubernetes services. Given we've used kubeadm "
"to explicitly deploy a kube-proxy-free setup, ``strict`` mode is explicitly set "
"in this guide to ensure that we do not rely on a (non-existing) fallback."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:137
msgid ""
"Cilium's eBPF kube-proxy replacement is supported in direct routing as well as "
"in tunneling mode."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:143
msgid ""
"After deploying Cilium with above Quick-Start guide, we can first validate that "
"the Cilium agent is running in the desired mode:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:151
msgid "Use ``--verbose`` for full details:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:175
msgid ""
"As a optional next step, we deploy nginx pods, create a new NodePort service "
"and validate that Cilium installed the service correctly."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:178
msgid "The following yaml is used for the backend pods:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:202
msgid "Verify that the nginx pods are up and running:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:211
msgid "In the next step, we create a NodePort service for the two instances:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:218
msgid "Verify that the NodePort service has been created:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:226
msgid ""
"With the help of the ``cilium service list`` command, we can validate that "
"Cilium's eBPF kube-proxy replacement created the new NodePort services under "
"port ``31940`` (one for each of devices ``eth0`` and ``eth1``):"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:244
msgid ""
"At the same time we can verify, using ``iptables`` in the host namespace, that "
"no ``iptables`` rule for the service is present:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:252
msgid ""
"Last but not least, a simple ``curl`` test shows connectivity for the exposed "
"NodePort port ``31940`` as well as for the ClusterIP:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:291
msgid "As can be seen, Cilium's eBPF kube-proxy replacement is set up correctly."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:294
msgid "Advanced Configuration"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:296
msgid ""
"This section covers a few advanced configuration modes for the kube-proxy "
"replacement that go beyond the above Quick-Start guide and are entirely "
"optional."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:300
msgid "Client Source IP Preservation"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:302
msgid ""
"Cilium's eBPF kube-proxy replacement implements a number of options in order to "
"avoid performing SNAT on NodePort requests where the client source IP address "
"would otherwise be lost on its path to the service endpoint."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:306
msgid ""
"``externalTrafficPolicy=Local``: The ``Local`` policy is generally supported "
"through the eBPF implementation. In-cluster connectivity for services with "
"``externalTrafficPolicy=Local`` is possible and can also be reached from nodes "
"which have no local backends, meaning, given SNAT does not need to be "
"performed, all service endpoints are available for load balancing from in-"
"cluster side."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:312
msgid ""
"``externalTrafficPolicy=Cluster``: For the ``Cluster`` policy which is the "
"default upon service creation, multiple options exist for achieving client "
"source IP preservation for external traffic, that is, operating the kube-proxy "
"replacement in :ref:`DSR<DSR Mode>` or :ref:`Hybrid<Hybrid Mode>` mode if only "
"TCP-based services are exposed to the outside world for the latter."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:321
msgid "Maglev Consistent Hashing (Beta)"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:323
#, python-format
msgid ""
"Cilium's eBPF kube-proxy replacement supports consistent hashing by "
"implementing a variant of `The Maglev paper <https://storage.googleapis.com/pub-"
"tools-public-publication-data/pdf/44824.pdf>`_ hashing in its load balancer for "
"backend selection. This improves resiliency in case of failures as well as "
"better load balancing properties since nodes added to the cluster will make the "
"same, consistent backend selection throughout the cluster for a given 5-tuple "
"without having to synchronize state with the other nodes. Similarly, upon "
"backend removal the backend lookup tables are reprogrammed with minimal "
"disruption for unrelated backends (at most 1% difference in the reassignments) "
"for the given service."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:332
msgid ""
"Maglev hashing for services load balancing can be enabled by setting "
"``loadBalancer.algorithm=maglev``:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:343
msgid ""
"Note that Maglev hashing is applied only to external (N-S) traffic. For in-"
"cluster service connections (E-W), sockets are assigned to service backends "
"directly, e.g. at TCP connect time, without any intermediate hop and thus are "
"not subject to Maglev. Maglev hashing is also supported for Cilium's :ref:"
"`XDP<XDP Acceleration>` acceleration."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:349
msgid ""
"There are two more Maglev-specific configuration settings: ``maglev.tableSize`` "
"and ``maglev.hashSeed``."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:352
#, python-format
msgid ""
"``maglev.tableSize`` specifies the size of the Maglev lookup table for each "
"single service. `Maglev <https://storage.googleapis.com/pub-tools-public-"
"publication-data/pdf/44824.pdf>`__ recommends the table size (``M``) to be "
"significantly larger than the number of maximum expected backends (``N``). In "
"practice that means that ``M`` should be larger than ``100 * N`` in order to "
"guarantee the property of at most 1% difference in the reassignments on backend "
"changes. ``M`` must be a prime number. Cilium uses a default size of ``16381`` "
"for ``M``. The following sizes for ``M`` are supported as ``maglev.tableSize`` "
"Helm option:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:361
msgid "``maglev.tableSize`` value"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:363
msgid "251"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:365
msgid "509"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:367
msgid "1021"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:369
msgid "2039"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:371
msgid "4093"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:373
msgid "8191"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:375
msgid "16381"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:377
msgid "32749"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:379
msgid "65521"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:381
msgid "131071"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:384
msgid ""
"For example, a ``maglev.tableSize`` of ``16381`` is suitable for a maximum of "
"``~160`` backends per service. If a higher number of backends are provisioned "
"under this setting, then the difference in reassignments on backend changes "
"will increase."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:388
msgid ""
"The ``maglev.hashSeed`` option is recommended to be set in order for Cilium to "
"not rely on the fixed built-in seed. The seed is a base64-encoded 12 byte-"
"random number, and can be generated once through ``head -c12 /dev/urandom | "
"base64 -w0``, for example. Every Cilium agent in the cluster must use the same "
"hash seed in order for Maglev to work."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:393
#, python-format
msgid ""
"The below deployment example is generating and passing such seed to Helm as "
"well as setting the Maglev table size to ``65521`` in order to allow for "
"``~650`` maximum number of backends for a given service (with the property of "
"at most 1% difference on backend reassignments):"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:409
msgid ""
"Note that enabling Maglev will have a higher memory consumption on each Cilium-"
"managed node compared to the default of ``loadBalancer.algorithm=random`` given "
"``random`` does not need the extra lookup tables. However, ``random`` won't "
"have consistent backend selection."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:416
msgid "Direct Server Return (DSR)"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:418
msgid ""
"By default, Cilium's eBPF NodePort implementation operates in SNAT mode. That "
"is, when node-external traffic arrives and the node determines that the backend "
"for the LoadBalancer, NodePort or services with externalIPs is at a remote "
"node, then the node is redirecting the request to the remote backend on its "
"behalf by performing SNAT. This does not require any additional MTU changes at "
"the cost that replies from the backend need to make the extra hop back that "
"node in order to perform the reverse SNAT translation there before returning "
"the packet directly to the external client."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:427
msgid ""
"This setting can be changed through the ``loadBalancer.mode`` Helm option to "
"``dsr`` in order to let Cilium's eBPF NodePort implementation operate in DSR "
"mode. In this mode, the backends reply directly to the external client without "
"taking the extra hop, meaning, backends reply by using the service IP/port as a "
"source. DSR currently requires Cilium to be deployed in :ref:"
"`arch_direct_routing`, i.e. it will not work in either tunneling mode."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:434
msgid ""
"Another advantage in DSR mode is that the client's source IP is preserved, so "
"policy can match on it at the backend node. In the SNAT mode this is not "
"possible. Given a specific backend can be used by multiple services, the "
"backends need to be made aware of the service IP/port which they need to reply "
"with. Therefore, Cilium encodes this information in a Cilium-specific IPv4 "
"option or IPv6 Destination Option extension header at the cost of advertising a "
"lower MTU. For TCP services, Cilium only encodes the service IP/port for the "
"SYN packet, but not subsequent ones. The latter also allows to operate Cilium "
"in a hybrid mode as detailed in the next subsection where DSR is used for TCP "
"and SNAT for UDP in order to avoid an otherwise needed MTU reduction."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:445
msgid ""
"Note that usage of DSR mode might not work in some public cloud provider "
"environments due to the Cilium-specific IP options that could be dropped by an "
"underlying fabric. Therefore, in case of connectivity issues to services where "
"backends are located on a remote node from the node that is processing the "
"given NodePort request, it is advised to first check whether the NodePort "
"request actually arrived on the node containing the backend. If this was not "
"the case, then switching back to the default SNAT mode would be advised as a "
"workaround."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:453
msgid ""
"Also, in some public cloud provider environments, which implement a source / "
"destination IP address checking (e.g. AWS), the checking has to be disabled in "
"order for the DSR mode to work."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:457
msgid ""
"The above Helm example configuration in a kube-proxy-free environment with DSR-"
"only mode enabled would look as follows:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:474
msgid "Hybrid DSR and SNAT Mode"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:476
msgid ""
"Cilium also supports a hybrid DSR and SNAT mode, that is, DSR is performed for "
"TCP and SNAT for UDP connections. This has the advantage that it removes the "
"need for manual MTU changes in the network while still benefiting from the "
"latency improvements through the removed extra hop for replies, in particular, "
"when TCP is the main transport for workloads."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:482
msgid ""
"The mode setting ``loadBalancer.mode`` allows to control the behavior through "
"the options ``dsr``, ``snat`` and ``hybrid``. By default the ``snat`` mode is "
"used in the agent."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:486
msgid ""
"A Helm example configuration in a kube-proxy-free environment with DSR enabled "
"in hybrid mode would look as follows:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:501
msgid "Socket LoadBalancer Bypass in Pod Namespace"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:503
msgid ""
"Cilium has built-in support for bypassing the socket-level loadbalancer and "
"falling back to the tc loadbalancer at the veth interface when a custom "
"redirection/operation relies on the original ClusterIP within pod namespace (e."
"g., Istio side-car) or due to the Pod's nature the socket-level loadbalancer is "
"ineffective (e.g., KubeVirt, Kata Containers)."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:508
msgid ""
"Setting ``hostServices.hostNamespaceOnly=true`` enables this bypassing mode. "
"When enabled, this circumvents socket rewrite in the ``connect()`` and "
"``sendmsg()`` syscall bpf hook and will pass the original packet to next stage "
"of operation (e.g., stack in ``per-endpoint-routing`` mode) and re-enables "
"service lookup in the tc bpf program."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:513
msgid ""
"A Helm example configuration in a kube-proxy-free environment with socket LB "
"bypass looks as follows:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:528
msgid "LoadBalancer & NodePort XDP Acceleration"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:530
msgid ""
"Cilium has built-in support for accelerating NodePort, LoadBalancer services "
"and services with externalIPs for the case where the arriving request needs to "
"be forwarded and the backend is located on a remote node. This feature was "
"introduced in Cilium version `1.8 <https://cilium.io/blog/2020/06/22/cilium-18/"
"#kube-proxy-replacement-at-the-xdp-layer>`_ at the XDP (eXpress Data Path) "
"layer where eBPF is operating directly in the networking driver instead of a "
"higher layer."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:537
msgid ""
"The mode setting ``loadBalancer.acceleration`` allows to enable this "
"acceleration through the option ``native``. The option ``disabled`` is the "
"default and disables the acceleration. The majority of drivers supporting 10G "
"or higher rates also support ``native`` XDP on a recent kernel. For cloud based "
"deployments most of these drivers have SR-IOV variants that support native XDP "
"as well. For on-prem deployments the Cilium XDP acceleration can be used in "
"combination with LoadBalancer service implementations for Kubernetes such as "
"`MetalLB <https://metallb.universe.tf/>`_. The acceleration can be enabled only "
"on a single device which is used for direct routing."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:546
msgid ""
"For high-scale environments, also consider tweaking the default map sizes to a "
"larger number of entries e.g. through setting a higher ``config."
"bpfMapDynamicSizeRatio``. See :ref:`bpf_map_limitations` for further details."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:550
msgid ""
"The ``loadBalancer.acceleration`` setting is supported for DSR, SNAT and hybrid "
"modes and can be enabled as follows for ``loadBalancer.mode=hybrid`` in this "
"example:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:565
msgid ""
"In case of a multi-device environment, where Cilium's device auto-detection "
"selects more than a single device to expose NodePort or a user specifies "
"multiple devices with ``devices``, the XDP acceleration is enabled on all "
"devices. This means that each underlying device's driver must have native XDP "
"support on all Cilium managed nodes. In addition, for the performance reasons "
"we recommend kernel >= 5.5 for the multi-device XDP acceleration."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:572
msgid ""
"A list of drivers supporting native XDP can be found in the table below. The "
"corresponding network driver name of an interface can be determined as follows:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:582
msgid "Vendor"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:582
msgid "Driver"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:582
msgid "XDP Support"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:584
msgid "Amazon"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:584
msgid "ena"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:584
#: ../../gettingstarted/kubeproxy-free.rst:606
msgid ">= 5.6"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:586
msgid "Broadcom"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:586
msgid "bnxt_en"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:586
msgid ">= 4.11"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:588
msgid "Cavium"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:588
msgid "thunderx"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:588
#: ../../gettingstarted/kubeproxy-free.rst:592
msgid ">= 4.12"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:590
msgid "Freescale"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:590
msgid "dpaa2"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:590
msgid ">= 5.0"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:592
msgid "Intel"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:592
msgid "ixgbe"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:594
msgid "ixgbevf"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:594
msgid ">= 4.17"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:596
msgid "i40e"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:596
msgid ">= 4.13"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:598
msgid "ice"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:598
#: ../../gettingstarted/kubeproxy-free.rst:600
#: ../../gettingstarted/kubeproxy-free.rst:618
msgid ">= 5.5"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:600
msgid "Marvell"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:600
msgid "mvneta"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:602
msgid "Mellanox"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:602
msgid "mlx4"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:602
msgid ">= 4.8"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:604
msgid "mlx5"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:604
msgid ">= 4.9"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:606
msgid "Microsoft"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:606
msgid "hv_netvsc"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:608
msgid "Netronome"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:608
msgid "nfp"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:608
#: ../../gettingstarted/kubeproxy-free.rst:610
#: ../../gettingstarted/kubeproxy-free.rst:614
msgid ">= 4.10"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:610
msgid "Others"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:610
msgid "virtio_net"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:612
msgid "tun/tap"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:612
msgid ">= 4.14"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:614
msgid "Qlogic"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:614
msgid "qede"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:616
msgid "Socionext"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:616
msgid "netsec"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:616
#: ../../gettingstarted/kubeproxy-free.rst:620
msgid ">= 5.3"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:618
msgid "Solarflare"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:618
msgid "sfc"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:620
msgid "Texas Instruments"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:620
msgid "cpsw"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:623
msgid ""
"The current Cilium kube-proxy XDP acceleration mode can also be introspected "
"through the ``cilium status`` CLI command. If it has been enabled successfully, "
"``Native`` is shown:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:632
msgid ""
"Note that packets which have been pushed back out of the device for NodePort "
"handling right at the XDP layer are not visible in tcpdump since packet taps "
"come at a much later stage in the networking stack. Cilium's monitor or metric "
"counters can be used instead for gaining visibility."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:638
msgid "NodePort XDP on AWS"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:640
msgid ""
"In order to run with NodePort XDP on AWS, follow the instructions in the :ref:"
"`k8s_install_quick` guide to set up an EKS cluster or use any other method of "
"your preference to set up a Kubernetes cluster."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:644
msgid ""
"If you are following the EKS guide, make sure to create a node group with SSH "
"access, since we need few additional setup steps as well as create a larger "
"instance type which supports the `Elastic Network Adapter <https://docs.aws."
"amazon.com/AWSEC2/latest/UserGuide/enhanced-networking-ena.html>`__ (ena). As "
"an instance example, ``m5n.xlarge`` is used in the config ``nodegroup-config."
"yaml``:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:676
msgid "The nodegroup is created with:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:682
msgid ""
"Each of the nodes need the ``kernel-ng`` and ``ethtool`` package installed. The "
"former is needed in order to run a sufficiently recent kernel for eBPF in "
"general and native XDP support on the ena driver. The latter is needed to "
"configure channel parameters for the NIC."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:692
msgid ""
"Once the nodes come back up their kernel version should say ``5.4.58-27.104."
"amzn2.x86_64`` or similar through ``uname -r``. In order to run XDP on ena, "
"make sure the driver version is at least `2.2.8 <https://github.com/amzn/amzn-"
"drivers/commit/ccbb1fe2c2f2ab3fc6d7827b012ba8ec06f32c39>`__. The driver version "
"can be inspected through ``ethtool -i eth0``. For the given kernel version the "
"driver version should be reported as ``2.2.10g``."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:698
msgid ""
"Before Cilium's XDP acceleration can be deployed, there are two settings needed "
"on the network adapter side, that is, MTU needs to be lowered in order to be "
"able to operate with XDP, and number of combined channels need to be adapted."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:702
msgid ""
"The default MTU is set to 9001 on the ena driver. Given XDP buffers are linear, "
"they operate on a single page. A driver typically reserves some headroom for "
"XDP as well (e.g. for encapsulation purpose), therefore, the highest possible "
"MTU for XDP would be 3498."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:707
msgid ""
"In terms of ena channels, the settings can be gathered via ``ethtool -l eth0``. "
"For the ``m5n.xlarge`` instance, the default output should look like::"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:722
msgid ""
"In order to use XDP the channels must be set to at most 1/2 of the value from "
"``Combined`` above. Both, MTU and channel changes are applied as follows:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:730
msgid ""
"In order to deploy Cilium, the Kubernetes API server IP and port is needed:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:737
msgid ""
"Finally, the deployment can be upgraded and later rolled-out with the "
"``loadBalancer.acceleration=native`` setting to enable XDP in Cilium:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:753
msgid "NodePort XDP on Azure"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:755
msgid ""
"To enable NodePort XDP on Azure AKS or a self-managed Kubernetes running on "
"Azure, the virtual machines running Kubernetes must have `Accelerated "
"Networking <https://azure.microsoft.com/en-us/updates/accelerated-networking-in-"
"expanded-preview/>`_ enabled. In addition, the Linux kernel on the nodes must "
"also have support for native XDP in the ``hv_netvsc`` driver, which is "
"available in kernel >= 5.6 and was backported to the Azure Linux kernel in "
"5.4.0-1022."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:762
msgid ""
"On AKS, make sure to use the AKS Ubuntu 18.04 node image with Kubernetes "
"version v1.18 which will provide a Linux kernel with the necessary backports to "
"the ``hv_netvsc`` driver. Please refer to the documentation on `how to "
"configure an AKS cluster <https://docs.microsoft.com/en-us/azure/aks/cluster-"
"configuration>`_ for more details."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:767
msgid ""
"To enable accelerated networking when creating a virtual machine or virtual "
"machine scale set, pass the ``--accelerated-networking`` option to the Azure "
"CLI. Please refer to the guide on how to `create a Linux virtual machine with "
"Accelerated Networking using Azure CLI <https://docs.microsoft.com/en-us/azure/"
"virtual-network/create-vm-accelerated-networking-cli>`_ for more details."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:774
msgid ""
"When *Accelerated Networking* is enabled, ``lspci`` will show a Mellanox "
"ConnectX-3 or ConnectX-4 Lx NIC:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:782
msgid ""
"In order to run XDP, large receive offload (LRO) needs to be disabled on the "
"``hv_netvsc`` device. If not the case already, this can be achieved by:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:789
msgid ""
"NodePort XDP requires Cilium to run in direct routing mode "
"(``tunnel=disabled``). It is recommended to use Azure IPAM for the pod IP "
"address allocation, which will automatically configure your virtual network to "
"route pod traffic correctly:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:813
msgid ""
"When running Azure IPAM on a self-managed Kubernetes cluster, each ``v1.Node`` "
"must have the resource ID of its VM in the ``spec.providerID`` field. Refer to "
"the :ref:`ipam_azure` reference for more information."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:818
msgid "NodePort XDP on GCP"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:820
msgid ""
"NodePort XDP on the Google Cloud Platform is currently not supported. Both "
"virtual network interfaces available on Google Compute Engine (the older virtIO-"
"based interface and the newer `gVNIC <https://cloud.google.com/compute/docs/"
"instances/create-vm-with-gvnic>`_) are currently lacking support for native XDP."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:829
msgid "NodePort Devices, Port and Bind settings"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:831
msgid ""
"When running Cilium's eBPF kube-proxy replacement, by default, a NodePort or "
"LoadBalancer service or a service with externalIPs will be accessible through "
"the IP addresses of native devices which have the default route on the host or "
"have Kubernetes InternalIP or ExternalIP assigned. InternalIP is preferred over "
"ExternalIP if both exist. To change the devices, set their names in the "
"``devices`` Helm option, e.g. ``devices='{eth0,eth1,eth2}'``. Each listed "
"device has to be named the same on all Cilium managed nodes. Alternatively if "
"the devices do not match across different nodes, the wildcard option can be "
"used, e.g. ``devices=eth+``, which would match any device starting with prefix "
"``eth``. If no device can be matched the Cilium agent will try to perform auto "
"detection."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:843
msgid ""
"When multiple devices are used, only one device can be used for direct routing "
"between Cilium nodes. By default, if a single device was detected or specified "
"via ``devices`` then Cilium will use that device for direct routing. Otherwise, "
"Cilium will use a device with Kubernetes InternalIP or ExternalIP being set. "
"InternalIP is preferred over ExternalIP if both exist. To change the direct "
"routing device, set the ``nodePort.directRoutingDevice`` Helm option, e.g. "
"``nodePort.directRoutingDevice=eth1``. The wildcard option can be used as well "
"as the devices option, e.g. ``directRoutingDevice=eth+``. If more than one "
"devices match the wildcard option, Cilium will sort them in increasing "
"alphanumerical order and pick the first one. If the direct routing device does "
"not exist within ``devices``, Cilium will add the device to the latter list. "
"The direct routing device is used for :ref:`the NodePort XDP acceleration<XDP "
"Acceleration>` as well (if enabled)."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:857
msgid ""
"In addition, thanks to the :ref:`host-services` feature, the NodePort service "
"can be accessed by default from a host or a pod within a cluster via its "
"public, any local (except for ``docker*`` prefixed names) or loopback address, "
"e.g. ``127.0.0.1:NODE_PORT``."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:862
msgid ""
"If ``kube-apiserver`` was configured to use a non-default NodePort port range, "
"then the same range must be passed to Cilium via the ``nodePort.range`` option, "
"for example, as ``nodePort.range=\"10000\\,32767\"`` for a range of "
"``10000-32767``. The default Kubernetes NodePort range is ``30000-32767``."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:867
msgid ""
"If the NodePort port range overlaps with the ephemeral port range (``net.ipv4."
"ip_local_port_range``), Cilium will append the NodePort range to the reserved "
"ports (``net.ipv4.ip_local_reserved_ports``). This is needed to prevent a "
"NodePort service from hijacking traffic of a host local application which "
"source port matches the service port. To disable the modification of the "
"reserved ports, set ``nodePort.autoProtectPortRanges`` to ``false``."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:874
msgid ""
"By default, the NodePort implementation prevents application ``bind(2)`` "
"requests to NodePort service ports. In such case, the application will "
"typically see a ``bind: Operation not permitted`` error. This happens either "
"globally for older kernels or starting from v5.7 kernels only for the host "
"namespace by default and therefore not affecting any application pod "
"``bind(2)`` requests anymore. In order to opt-out from this behavior in "
"general, this setting can be changed for expert users by switching ``nodePort."
"bindProtection`` to ``false``."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:883
msgid "NodePort with FHRP & VPC"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:885
msgid ""
"When using Cilium's kube-proxy replacement in conjunction with a `FHRP <https://"
"en.wikipedia.org/wiki/First-hop_redundancy_protocol>`_ such as VRRP or Cisco's "
"HSRP and VPC (also known as multi-chassis EtherChannel), the default "
"configuration can cause issues or unwanted traffic flows. This is due to an "
"optimization that causes the source IP of ingress packets destined for a "
"NodePort to be associated with the corresponding MAC address, and later in the "
"reply, the MAC address is used as the destination when forwarding the L2 frame, "
"bypassing the FIB lookup."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:892
msgid ""
"In such an environment, it may be preferred to instruct Cilium not to attempt "
"this optimization. This will ensure the response is always forwarded to the MAC "
"address of the currently active FHRP peer, no matter the origin of the incoming "
"packet."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:896
msgid "To disable the optimization set ``bpf.lbBypassFIBLookup`` to ``false``."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:908
msgid "Configuring BPF Map Sizes"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:910
msgid ""
"For high-scale environments, Cilium's BPF maps can be configured to have higher "
"limits on the number of entries. Overriding Helm options can be used to tweak "
"these limits."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:914
msgid ""
"To increase the number of entries in Cilium's BPF LB service, backend and "
"affinity maps consider overriding ``bpf.lbMapMax`` Helm option. The default "
"value of this LB map size is 65536."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:928
msgid "Container HostPort Support"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:930
msgid ""
"Although not part of kube-proxy, Cilium's eBPF kube-proxy replacement also "
"natively supports ``hostPort`` service mapping without having to use the Helm "
"CNI chaining option of ``cni.chainingMode=portmap``."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:934
msgid ""
"By specifying ``kubeProxyReplacement=strict`` or ``kubeProxyReplacement=probe`` "
"the native hostPort support is automatically enabled and therefore no further "
"action is required. Otherwise ``hostPort.enabled=true`` can be used to enable "
"the setting."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:939
msgid ""
"If the ``hostPort`` is specified without an additional ``hostIP``, then the Pod "
"will be exposed to the outside world with the same local addresses from the "
"node that were detected and used for exposing NodePort services, e.g. the "
"Kubernetes InternalIP or ExternalIP if set. Additionally, the Pod is also "
"accessible through the loopback address on the node such as ``127.0.0.1:"
"hostPort``. If in addition to ``hostPort`` also a ``hostIP`` has been specified "
"for the Pod, then the Pod will only be exposed on the given ``hostIP`` instead. "
"A ``hostIP`` of ``0.0.0.0`` will have the same behavior as if a ``hostIP`` was "
"not specified. The ``hostPort`` must not reside in the configured NodePort port "
"range to avoid collisions."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:950
msgid ""
"An example deployment in a kube-proxy-free environment therefore is the same as "
"in the earlier getting started deployment:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:961
msgid ""
"Also, ensure that each node IP is known via ``INTERNAL-IP`` or ``EXTERNAL-IP``, "
"for example:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:971
msgid ""
"If this is not the case, then ``kubelet`` needs to be made aware of it through "
"specifying ``--node-ip`` through ``KUBELET_EXTRA_ARGS``. Assuming ``eth0`` is "
"the public facing interface, this can be achieved by:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:979
msgid "After updating ``/etc/default/kubelet``, kubelet needs to be restarted."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:981
msgid ""
"In order to verify whether the HostPort feature has been enabled in Cilium, the "
"``cilium status`` CLI command provides visibility through the "
"``KubeProxyReplacement`` info line. If it has been enabled successfully, "
"``HostPort`` is shown as ``Enabled``, for example:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:991
msgid ""
"The following modified example yaml from the setup validation with an "
"additional ``hostPort: 8080`` parameter can be used to verify the mapping:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1017
msgid ""
"After deployment, we can validate that Cilium's eBPF kube-proxy replacement "
"exposed the container as HostPort under the specified port ``8080``:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1027
msgid ""
"Similarly, we can inspect through ``iptables`` in the host namespace that no "
"``iptables`` rule for the HostPort service is present:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1035
msgid ""
"Last but not least, a simple ``curl`` test shows connectivity for the exposed "
"HostPort container under the node's IP:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1047
msgid ""
"Removing the deployment also removes the corresponding HostPort from the "
"``cilium service list`` dump:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1055
msgid "kube-proxy Hybrid Modes"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1057
msgid ""
"Cilium's eBPF kube-proxy replacement can be configured in several modes, i.e. "
"it can replace kube-proxy entirely or it can co-exist with kube-proxy on the "
"system if the underlying Linux kernel requirements do not support a full kube-"
"proxy replacement."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1061
msgid ""
"**Careful:** When deploying the eBPF kube-proxy replacement under co-existence "
"with kube-proxy on the system, be aware that both mechanisms operate "
"independent of each other. Meaning, if the eBPF kube-proxy replacement is added "
"or removed on an already *running* cluster in order to delegate operation from "
"respectively back to kube-proxy, then it must be expected that existing "
"connections will break since, for example, both NAT tables are not aware of "
"each other. If deployed in co-existence on a newly spawned up node/cluster "
"which does not yet serve user traffic, then this is not an issue."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1070
msgid "This section elaborates on the various ``kubeProxyReplacement`` options:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1072
msgid ""
"``kubeProxyReplacement=strict``: This option expects a kube-proxy-free "
"Kubernetes setup where Cilium is expected to fully replace all kube-proxy "
"functionality. Once the Cilium agent is up and running, it takes care of "
"handling Kubernetes services of type ClusterIP, NodePort, LoadBalancer, "
"services with externalIPs as well as HostPort. If the underlying kernel version "
"requirements are not met (see :ref:`kubeproxy-free` note), then the Cilium "
"agent will bail out on start-up with an error message."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1080
msgid ""
"``kubeProxyReplacement=probe``: This option is only intended for a hybrid "
"setup, that is, kube-proxy is running in the Kubernetes cluster where Cilium "
"partially replaces and optimizes kube-proxy functionality. Once the Cilium "
"agent is up and running, it probes the underlying kernel for the availability "
"of needed eBPF kernel features and, if not present, disables a subset of the "
"functionality in eBPF by relying on kube-proxy to complement the remaining "
"Kubernetes service handling. The Cilium agent will emit an info message into "
"its log in such case. For example, if the kernel does not support :ref:`host-"
"services`, then the ClusterIP translation for the node's host-namespace is done "
"through kube-proxy's iptables rules. Also, the Cilium agent will set ``nodePort."
"bindProtection`` to ``false`` in this mode in order to defer to kube-proxy for "
"performing the bind-protection of the host namespace. This is done to avoid "
"having kube-proxy throw (harmless) warnings to its log stating that it could "
"not perform bind calls. In the ``strict`` mode this bind protection is "
"performed by Cilium in a more efficient manner with the help of eBPF instead of "
"allocating and binding actual sockets."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1096
msgid ""
"``kubeProxyReplacement=partial``: Similarly to ``probe``, this option is "
"intended for a hybrid setup, that is, kube-proxy is running in the Kubernetes "
"cluster where Cilium partially replaces and optimizes kube-proxy functionality. "
"As opposed to ``probe`` which checks the underlying kernel for available eBPF "
"features and automatically disables components responsible for the eBPF kube-"
"proxy replacement when kernel support is missing, the ``partial`` option "
"requires the user to manually specify which components for the eBPF kube-proxy "
"replacement should be used. When ``kubeProxyReplacement`` is set to ``partial`` "
"make sure to also set ``enableHealthCheckNodeport`` to ``false``, so that the "
"Cilium agent does not start the NodePort health check server. Similarly to "
"``strict`` mode, the Cilium agent will bail out on start-up with an error "
"message if the underlying kernel requirements are not met. For fine-grained "
"configuration, ``hostServices.enabled``, ``nodePort.enabled``, ``externalIPs."
"enabled`` and ``hostPort.enabled`` can be set to ``true``. By default all four "
"options are set to ``false``. A few example configurations for the ``partial`` "
"option are provided below."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1111
msgid ""
"The following Helm setup below would be equivalent to "
"``kubeProxyReplacement=strict`` in a kube-proxy-free environment:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1126
msgid ""
"The following Helm setup below would be equivalent to the default Cilium "
"service handling in v1.6 or earlier in a kube-proxy environment, that is, "
"serving ClusterIP for pods:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1136
msgid ""
"The following Helm setup below would optimize Cilium's ClusterIP handling for "
"TCP in a kube-proxy environment (``hostServices.protocols`` default is ``tcp,"
"udp``):"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1147
msgid ""
"The following Helm setup below would optimize Cilium's NodePort, LoadBalancer "
"and services with externalIPs handling for external traffic ingressing into the "
"Cilium managed node in a kube-proxy environment:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1159
msgid ""
"``kubeProxyReplacement=disabled``: This option disables any Kubernetes service "
"handling by fully relying on kube-proxy instead, except for ClusterIP services "
"accessed from pods (pre-v1.6 behavior)."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1163
msgid ""
"In Cilium's Helm chart, the default mode is ``kubeProxyReplacement=probe`` for "
"new deployments."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1166
msgid ""
"The current Cilium kube-proxy replacement mode can also be introspected through "
"the ``cilium status`` CLI command:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1175
msgid "Graceful Termination"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1177
msgid ""
"Cilium's eBPF kube-proxy replacement supports graceful termination of service "
"endpoint pods. The feature requires at least Kubernetes version 1.20, and the "
"feature gate ``EndpointSliceTerminatingCondition`` needs to be enabled. By "
"default, the Cilium agent then detects such terminating Pod state. If needed, "
"the feature can be disabled with the configuration option ``enable-k8s-"
"terminating-endpoint``."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1183
msgid ""
"The cilium agent feature flag can be probed by running ``cilium status`` "
"command:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1194
msgid ""
"When Cilium agent receives a Kubernetes update event for a terminating "
"endpoint, the datapath state for the endpoint is removed such that it won't "
"service new connections, but the endpoint's active connections are able to "
"terminate gracefully. The endpoint state is fully removed when the agent "
"receives a Kubernetes delete event for the endpoint. The `Kubernetes pod "
"termination <https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/"
"#pod-termination>`_ documentation contains more background on the behavior and "
"configuration using ``terminationGracePeriodSeconds``."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1205
msgid "Session Affinity"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1207
msgid ""
"Cilium's eBPF kube-proxy replacement supports Kubernetes service session "
"affinity. Each connection from the same pod or host to a service configured "
"with ``sessionAffinity: ClientIP`` will always select the same service "
"endpoint. The default timeout for the affinity is three hours (updated by each "
"request to the service), but it can be configured through Kubernetes' "
"``sessionAffinityConfig`` if needed."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1214
msgid ""
"The source for the affinity depends on the origin of a request. If a request is "
"sent from outside the cluster to the service, the request's source IP address "
"is used for determining the endpoint affinity. If a request is sent from inside "
"the cluster, the client's network namespace cookie is used. The latter was "
"introduced in the 5.7 Linux kernel to implement the affinity at the socket "
"layer at which :ref:`host-services` operate (a source IP is not available "
"there, as the endpoint selection happens before a network packet has been built "
"by the kernel)."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1222
msgid ""
"The session affinity support is enabled by default for Cilium's kube-proxy "
"replacement. For users who run on older kernels which do not support the "
"network namespace cookies, a fallback in-cluster mode is implemented, which is "
"based on a fixed cookie value as a trade-off. This makes all applications on "
"the host to select the same service endpoint for a given service with session "
"affinity configured. To disable the feature, set ``config."
"sessionAffinity=false``."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1229
msgid ""
"When the fixed cookie value is not used, the session affinity of a service with "
"multiple ports is per service IP and port. Meaning that all requests for a "
"given service sent from the same source and to the same service port will be "
"routed to the same service endpoints; but two requests for the same service, "
"sent from the same source but to different service ports may be routed to "
"distinct service endpoints."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1237
msgid "kube-proxy Replacement Health Check server"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1238
msgid ""
"To enable health check server for the kube-proxy replacement, the "
"``kubeProxyReplacementHealthzBindAddr`` option has to be set (disabled by "
"default). The option accepts the IP address with port for the health check "
"server to serve on. E.g. to enable for IPv4 interfaces set "
"``kubeProxyReplacementHealthzBindAddr='0.0.0.0:10256'``, for IPv6 - "
"``kubeProxyReplacementHealthzBindAddr='[::]:10256'``. The health check server "
"is accessible via the HTTP ``/healthz`` endpoint."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1247
msgid "LoadBalancer Source Ranges Checks"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1249
msgid ""
"When a ``LoadBalancer`` service is configured with ``spec."
"loadBalancerSourceRanges``, Cilium's eBPF kube-proxy replacement restricts "
"access from outside (e.g. external world traffic) to the service to the white-"
"listed CIDRs specified in the field. If the field is empty, no restrictions for "
"the access will be applied."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1254
msgid ""
"When accessing the service from inside a cluster, the kube-proxy replacement "
"will ignore the field regardless whether it is set. This means that any pod or "
"any host process in the cluster will be able to access the ``LoadBalancer`` "
"service internally."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1258
msgid ""
"The load balancer source range check feature is enabled by default, and it can "
"be disabled by setting ``config.svcSourceRangeCheck=false``. It makes sense to "
"disable the check when running on some cloud providers. E.g. `Amazon NLB "
"<https://kubernetes.io/docs/concepts/services-networking/service/#aws-nlb-"
"support>`__ natively implements the check, so the kube-proxy replacement's "
"feature can be disabled. Meanwhile `GKE internal TCP/UDP load balancer <https://"
"cloud.google.com/kubernetes-engine/docs/how-to/internal-load-"
"balancing#lb_source_ranges>`__ does not, so the feature must be kept enabled in "
"order to restrict the access."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1268
msgid "Service Proxy Name Configuration"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1270
msgid ""
"Like kube-proxy, Cilium also honors the ``service.kubernetes.io/service-proxy-"
"name`` service annotation and only manages services that contain a matching "
"service-proxy-name label. This name can be configured by setting ``k8s."
"serviceProxyName`` option and the behavior is identical to that of kube-proxy. "
"The service proxy name defaults to an empty string which instructs Cilium to "
"only manage services not having ``service.kubernetes.io/service-proxy-name`` "
"label."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1276
msgid ""
"For more details on the usage of ``service.kubernetes.io/service-proxy-name`` "
"label and its working, take a look at `this KEP <https://github.com/kubernetes/"
"enhancements/blob/3ad891202dab1fd5211946f10f31b48003bf8113/keps/sig-"
"network/2447-Make-kube-proxy-service-abstraction-optional/README.md>`__."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1282
msgid ""
"If Cilium with a non-empty service proxy name is meant to manage all services "
"in kube-proxy free mode, make sure that default Kubernetes services like ``kube-"
"dns`` and ``kubernetes`` have the required label value."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1287
msgid "Topology Aware Hints"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1289
msgid ""
"The kube-proxy replacement implements the K8s service `Topology Aware Hints "
"<https://kubernetes.io/docs/concepts/services-networking/topology-aware-"
"hints>`__. This allows Cilium nodes to prefer service endpoints residing in the "
"same zone. To enable the feature, set ``loadBalancer.serviceTopology=true``."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1295
msgid "Neighbor Discovery"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1297
msgid ""
"When kube-proxy replacement is enabled, Cilium does L2 neighbor discovery of "
"nodes in the cluster. This is required for the service load-balancing to "
"populate L2 addresses for backends since it is not possible to dynamically "
"resolve neighbors on demand in the fast-path."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1302
msgid ""
"In Cilium 1.10 or earlier, the agent itself contained an ARP resolution library "
"where it triggered discovery and periodic refresh of new nodes joining the "
"cluster. The resolved neighbor entries were pushed into the kernel and "
"refreshed as PERMANENT entries. In some rare cases, Cilium 1.10 or earlier "
"might have left stale entries behind in the neighbor table causing packets "
"between some nodes to be dropped. To skip the neighbor discovery and instead "
"rely on the Linux kernel to discover neighbors, you can pass the ``--enable-l2-"
"neigh-discovery=false`` flag to the cilium-agent. However, note that relying on "
"the Linux Kernel might also cause some packets to be dropped. For example, a "
"NodePort request can be dropped on an intermediate node (i.e., the one which "
"received a service packet and is going to forward it to a destination node "
"which runs the selected service endpoint). This could happen if there is no L2 "
"neighbor entry in the kernel (due to the entry being garbage collected or given "
"that the neighbor resolution has not been done by the kernel). This is because "
"it is not possible to drive the neighbor resolution from BPF programs in the "
"fast-path e.g. at the XDP layer."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1317
msgid ""
"From Cilium 1.11 onwards, the neighbor discovery has been fully reworked and "
"the Cilium internal ARP resolution library has been removed from the agent. The "
"agent now fully relies on the Linux kernel to discover gateways or hosts on the "
"same L2 network. Both IPv4 and IPv6 neighbor discovery is supported in the "
"Cilium agent. As per our recent kernel work `presented at Plumbers <https://"
"linuxplumbersconf.org/event/11/contributions/953/>`__, \"managed\" neighbor "
"entries have been `upstreamed <https://lore.kernel.org/"
"netdev/20211011121238.25542-1-daniel@iogearbox.net/>`__ and will be available "
"in Linux kernel v5.16 or later which the Cilium agent will detect and "
"transparently use. In this case, the agent pushes down L3 addresses of new "
"nodes joining the cluster as externally learned \"managed\" neighbor entries. "
"For introspection, iproute2 displays them as \"managed extern_learn\". The "
"\"extern_learn\" attribute prevents garbage collection of the entries by the "
"kernel's neighboring subsystem. Such \"managed\" neighbor entries are "
"dynamically resolved and periodically refreshed by the Linux kernel itself in "
"case there is no active traffic for a certain period of time. That is, the "
"kernel attempts to always keep them in REACHABLE state. For Linux kernels v5.15 "
"or earlier where \"managed\" neighbor entries are not present, the Cilium agent "
"similarly pushes L3 addresses of new nodes into the kernel for dynamic "
"resolution, but with an agent triggered periodic refresh. For introspection, "
"iproute2 displays them only as \"extern_learn\" in this case. If there is no "
"active traffic for a certain period of time, then a Cilium agent controller "
"triggers the Linux kernel-based re-resolution for attempting to keep them in "
"REACHABLE state. The refresh interval can be changed if needed through a ``--"
"arping-refresh-period=30s`` flag passed to the cilium-agent. The default period "
"is ``30s`` which corresponds to the kernel's base reachable time."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1341
msgid "External Access To ClusterIP Services"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1343
msgid ""
"As per `k8s Service <https://kubernetes.io/docs/concepts/services-networking/"
"service/#publishing-services-service-types>`__, Cilium's eBPF kube-proxy "
"replacement by default disallows access to a ClusterIP service from outside the "
"cluster. This can be allowed by setting ``bpf.lbExternalClusterIP=true``."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1352
msgid "Validate BPF cgroup programs attachment"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1354
msgid ""
"Cilium attaches BPF ``cgroup`` programs to enable socket-based load-balancing "
"(aka ``host-reachable`` services). If you see connectivity issues for "
"``clusterIP`` services, check if the programs are attached to the host ``cgroup "
"root``. The default ``cgroup`` root is set to ``/run/cilium/cgroupv2``. Run the "
"following commands from a Cilium agent pod as well as the underlying kubernetes "
"node where the pod is running. If the container runtime in your cluster is "
"running in the cgroup namespace mode, Cilium agent pod can attach BPF "
"``cgroup`` programs to the ``virtualized cgroup root``. In such cases, Cilium "
"kube-proxy replacement based load-balancing may not be effective leading to "
"connectivity issues. For more information, ensure that you have the fix `Pull "
"Request <https://github.com/cilium/cilium/pull/16259>`__."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1387
msgid ""
"Cilium's eBPF kube-proxy replacement currently cannot be used with :ref:"
"`gsg_encryption`."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1388
msgid ""
"Cilium's eBPF kube-proxy replacement relies upon the :ref:`host-services` "
"feature which uses eBPF cgroup hooks to implement the service translation. "
"Using it with libceph deployments currently requires support for the "
"getpeername(2) hook address translation in eBPF, which is only available for "
"kernels v5.8 and higher."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1392
msgid ""
"Cilium's DSR NodePort mode currently does not operate well in environments with "
"TCP Fast Open (TFO) enabled. It is recommended to switch to ``snat`` mode in "
"this situation."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1395
msgid ""
"Cilium's eBPF kube-proxy replacement does not support the SCTP transport "
"protocol. Only TCP and UDP is supported as a transport for services at this "
"point."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1397
msgid ""
"Cilium's eBPF kube-proxy replacement does not allow ``hostPort`` port "
"configurations for Pods that overlap with the configured NodePort range. In "
"such case, the ``hostPort`` setting will be ignored and a warning emitted to "
"the Cilium agent log. Similarly, explicitly binding the ``hostIP`` to the "
"loopback address in the host namespace is currently not supported and will log "
"a warning to the Cilium agent log."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1402
msgid ""
"When Cilium's kube-proxy replacement is used with Kubernetes versions(< 1.19) "
"that have support for ``EndpointSlices``, ``Services`` without selectors and "
"backing ``Endpoints`` don't work. The reason is that Cilium only monitors "
"changes made to ``EndpointSlices`` objects if support is available and ignores "
"``Endpoints`` in those cases. Kubernetes 1.19 release introduces "
"``EndpointSliceMirroring`` controller that mirrors custom ``Endpoints`` "
"resources to corresponding ``EndpointSlices`` and thus allowing backing "
"``Endpoints`` to work. For a more detailed discussion see :gh-issue:`12438`."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1409
msgid ""
"When deployed on kernels older than 5.7, Cilium is unable to distinguish "
"between host and pod namespaces due to the lack of kernel support for network "
"namespace cookies. As a result, Kubernetes services are reachable from all pods "
"via the loopback address."
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1414
msgid "Further Readings"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1416
msgid ""
"The following presentations describe inner-workings of the kube-proxy "
"replacement in eBPF in great details:"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1419
msgid ""
"\"Liberating Kubernetes from kube-proxy and iptables\" (KubeCon North America "
"2019, `slides <https://docs.google.com/presentation/d/1cZJ-"
"pcwB9WG88wzhDm2jxQY4Sh8adYg0-N3qWQ8593I/edit>`__, `video <https://www.youtube."
"com/watch?v=bIRwSIwNHC0>`__)"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1422
msgid ""
"\"Kubernetes service load-balancing at scale with BPF & XDP\" (Linux Plumbers "
"2020, `slides <https://linuxplumbersconf.org/event/7/contributions/674/"
"attachments/568/1002/plumbers_2020_cilium_load_balancer.pdf>`__, `video "
"<https://www.youtube.com/watch?v=UkvxPyIJAko&t=21s>`__)"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1425
msgid ""
"\"eBPF as a revolutionary technology for the container landscape\" (Fosdem "
"2020, `slides <https://docs.google.com/presentation/"
"d/1VOUcoIxgM_c6M_zAV1dLlRCjyYCMdR3tJv6CEdfLMh8/edit>`__, `video <https://fosdem."
"org/2020/schedule/event/containers_bpf/>`__)"
msgstr ""

#: ../../gettingstarted/kubeproxy-free.rst:1428
msgid ""
"\"Kernel improvements for Cilium socket LB\" (LSF/MM/BPF 2020, `slides <https://"
"docs.google.com/presentation/d/1w2zlpGWV7JUhHYd37El_AUZzyUNSvDfktrF5MJ5G8Bs/"
"edit#slide=id.g746fc02b5b_2_0>`__)"
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:11
msgid "Local Redirect Policy (beta)"
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:13
msgid ""
"This document explains how to configure Cilium's Local Redirect Policy, that "
"enables pod traffic destined to an IP address and port/protocol tuple or "
"Kubernetes service to be redirected locally to backend pod(s) within a node, "
"using eBPF. The namespace of backend pod(s) need to match with that of the "
"policy. The CiliumLocalRedirectPolicy is configured as a "
"``CustomResourceDefinition``."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:19
msgid ""
"There are two types of Local Redirect Policies supported. When traffic for a "
"Kubernetes service needs to be redirected, use the `ServiceMatcher` type. The "
"service needs to be of type ``clusterIP``. When traffic matching IP address and "
"port/protocol, that doesn't belong to any Kubernetes service, needs to be "
"redirected, use the `AddressMatcher` type."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:25
msgid ""
"The policies can be gated by Kubernetes Role-based access control (RBAC) "
"framework. See the official `RBAC documentation <https://kubernetes.io/docs/"
"reference/access-authn-authz/rbac/>`_."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:29
msgid ""
"When policies are applied, matched pod traffic is redirected. If desired, RBAC "
"configurations can be used such that application developers can not escape the "
"redirection."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:41
msgid ""
"Local Redirect Policy feature requires a v4.19.x or more recent Linux kernel."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:45
msgid ""
"The Cilium Local Redirect Policy feature relies on :ref:`kubeproxy-free`, "
"follow the guide to create a new deployment. The beta feature is disabled by "
"default. Enable the feature by setting the ``localRedirectPolicy`` value to "
"``true``."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:54
msgid "Verify that Cilium agent pod is running."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:63
msgid "Validate that the Cilium Local Redirect Policy CRD has been registered."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:73
msgid "Create backend and client pods"
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:75
msgid ""
"Deploy a backend pod where traffic needs to be redirected to based on the "
"configurations specified in a CiliumLocalRedirectPolicy. The metadata labels "
"and container port and protocol respectively match with the labels, port and "
"protocol fields specified in the CiliumLocalRedirectPolicy custom resources "
"that will be created in the next step."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:87
msgid "Verify that the pod is running."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:94
msgid ""
"Deploy a client pod that will generate traffic which will be redirected based "
"on the configurations specified in the CiliumLocalRedirectPolicy."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:105
msgid "Create Cilium Local Redirect Policy Custom Resources"
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:106
msgid ""
"There are two types of configurations supported in the "
"CiliumLocalRedirectPolicy in order to match the traffic that needs to be "
"redirected."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:112
msgid "AddressMatcher"
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:114
msgid ""
"This type of configuration is specified using an IP address and a Layer 4 port/"
"protocol. When multiple ports are specified for frontend in ``toPorts``, the "
"ports need to be named. The port names will be used to map frontend ports with "
"backend ports."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:118
msgid ""
"Verify that the ports specified in ``toPorts`` under ``redirectBackend`` exist "
"in the backend pod spec."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:121
msgid ""
"The example shows how to redirect from traffic matching, IP address "
"``169.254.169.254`` and Layer 4 port ``8080`` with protocol ``TCP``, to a "
"backend pod deployed with labels ``app=proxy`` and Layer 4 port ``80`` with "
"protocol ``TCP``. The ``localEndpointSelector`` set to ``app=proxy`` in the "
"policy is used to select the backend pods where traffic is redirected to."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:127
msgid ""
"Create a custom resource of type CiliumLocalRedirectPolicy with "
"``addressMatcher`` configuration."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:136
#: ../../gettingstarted/local-redirect-policy.rst:252
msgid "Verify that the custom resource is created."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:144
msgid ""
"Verify that Cilium's eBPF kube-proxy replacement created a ``LocalRedirect`` "
"service entry with the backend IP address of that of the ``lrp-pod`` that was "
"selected by the policy. Make sure that ``cilium service list`` is run in Cilium "
"pod running on the same node as ``lrp-pod``."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:161
msgid ""
"Invoke a curl command from the client pod to the IP address and port "
"configuration specified in the ``lrp-addr`` custom resource above."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:177
#: ../../gettingstarted/local-redirect-policy.rst:288
msgid ""
"Verify that the traffic was redirected to the ``lrp-pod`` that was deployed. "
"``tcpdump`` should be run on the same node that ``lrp-pod`` is running on."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:196
msgid "ServiceMatcher"
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:198
msgid ""
"This type of configuration is specified using Kubernetes service name and "
"namespace for which traffic needs to be redirected. The service must be of type "
"``clusterIP``. When ``toPorts`` under ``redirectFrontend`` are not specified, "
"traffic for all the service ports will be redirected. However, if traffic "
"destined to only a subset of ports needs to be redirected, these ports need to "
"be specified in the spec. Additionally, when multiple service ports are "
"specified in the spec, they must be named. The port names will be used to map "
"frontend ports with backend ports. Verify that the ports specified in "
"``toPorts`` under ``redirectBackend`` exist in the backend pod spec. The "
"``localEndpointSelector`` set to ``app=proxy`` in the policy is used to select "
"the backend pods where traffic is redirected to."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:209
msgid ""
"When a policy of this type is applied, the existing service entry created by "
"Cilium's eBPF kube-proxy replacement will be replaced with a new service entry "
"of type ``LocalRedirect``. This entry may only have node-local backend pods."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:213
msgid ""
"The example shows how to redirect from traffic matching ``my-service``, to a "
"backend pod deployed with labels ``app=proxy`` and Layer 4 port ``80`` with "
"protocol ``TCP``. The ``localEndpointSelector`` set to ``app=proxy`` in the "
"policy is used to select the backend pods where traffic is redirected to."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:218
msgid "Deploy the Kubernetes service for which traffic needs to be redirected."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:226
msgid "Verify that the service is created."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:233
msgid ""
"Verify that Cilium's eBPF kube-proxy replacement created a ``ClusterIP`` "
"service entry."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:243
msgid ""
"Create a custom resource of type CiliumLocalRedirectPolicy with "
"``serviceMatcher`` configuration."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:260
msgid ""
"Verify that entry Cilium's eBPF kube-proxy replacement updated the service "
"entry with type ``LocalRedirect`` and the node-local backend selected by the "
"policy. Make sure to run ``cilium service list`` in Cilium pod running on the "
"same node as ``lrp-pod``."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:272
msgid ""
"Invoke a curl command from the client pod to the Cluster IP address and port of "
"``my-service`` specified in the ``lrp-svc`` custom resource above."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:311
msgid ""
"When you create a Local Redirect Policy, traffic for all the new connections "
"that get established after the policy is enforced will be redirected. But if "
"you have existing active connections to remote pods that match the "
"configurations specified in the policy, then these might not get redirected. To "
"ensure all such connections are redirected locally, restart the client pods "
"after configuring the CiliumLocalRedirectPolicy."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:318
msgid ""
"Local Redirect Policy updates are currently not supported. If there are any "
"changes to be made, delete the existing policy, and re-create a new one."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:322
msgid "Use Cases"
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:323
msgid "Local Redirect Policy allows Cilium to support the following use cases:"
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:326
msgid "Node-local DNS cache"
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:327
msgid ""
"`DNS node-cache <https://github.com/kubernetes/dns>`_ listens on a static IP to "
"intercept traffic from application pods to the cluster's DNS service VIP by "
"default, which will be bypassed when Cilium is handling service resolution at "
"or before the veth interface of the application pod. To enable the DNS node-"
"cache in a Cilium cluster, the following example steers traffic to a local DNS "
"node-cache which runs as a normal pod."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:333
msgid "Deploy DNS node-cache in pod namespace."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:337
msgid "Quick Deployment"
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:339
msgid "Deploy DNS node-cache."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:343
msgid ""
"The example yaml is populated with default values for ``__PILLAR_LOCAL_DNS__`` "
"and ``__PILLAR_DNS_DOMAIN__``."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:345
msgid ""
"If you have a different deployment, please follow the official `NodeLocal "
"DNSCache Configuration <https://kubernetes.io/docs/tasks/administer-cluster/"
"nodelocaldns/#configuration>`_ to fill in the required template variables "
"``__PILLAR__LOCAL__DNS__``, ``__PILLAR__DNS__DOMAIN__``, and "
"``__PILLAR__DNS__SERVER__`` before applying the yaml."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:358
msgid "Manual Configuration"
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:360
msgid ""
"Follow the official `NodeLocal DNSCache Configuration <https://kubernetes.io/"
"docs/tasks/administer-cluster/nodelocaldns/#configuration>`_ to fill in the "
"required template variables ``__PILLAR__LOCAL__DNS__``, "
"``__PILLAR__DNS__DOMAIN__``, and ``__PILLAR__DNS__SERVER__`` before applying "
"the yaml."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:365
msgid ""
"Make sure to use a Node-local DNS image with a release version >= 1.15.16. This "
"is to ensure that we have a knob to disable dummy network interface creation/"
"deletion in Node-local DNS when we deploy it in non-host namespace."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:369
msgid ""
"Modify Node-local DNS cache's deployment yaml to pass these additional "
"arguments to node-cache: ``-skipteardown=true``, ``-setupinterface=false``, and "
"``-setupiptables=false``."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:372
msgid ""
"Modify Node-local DNS cache's deployment yaml to put it in non-host namespace "
"by setting ``hostNetwork: false`` for the daemonset."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:375
msgid "In the Corefile, bind to ``0.0.0.0`` instead of the static IP."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:377
msgid ""
"In the Corefile, let CoreDNS serve health-check on its own IP instead of the "
"static IP by removing the host IP string after health plugin."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:380
msgid ""
"Modify Node-local DNS cache's deployment yaml to point readiness probe to its "
"own IP by removing the ``host`` field under ``readinessProbe``."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:383
msgid ""
"Deploy Local Redirect Policy (LRP) to steer DNS traffic to the node local dns "
"cache."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:391
msgid ""
"The LRP above uses ``kube-dns`` for the cluster DNS service, however if your "
"cluster DNS service is different, you will need to modify this example LRP to "
"specify it."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:393
msgid ""
"The namespace specified in the LRP above is set to the same namespace as the "
"cluster's dns service."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:394
msgid ""
"The LRP above uses the same port names ``dns`` and ``dns-tcp`` as the example "
"quick deployment yaml, you will need to modify those to match your deployment "
"if they are different."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:397
msgid ""
"After all ``node-local-dns`` pods are in ready status, DNS traffic will now go "
"to the local node-cache first. You can verify by checking the DNS cache's "
"metrics ``coredns_dns_request_count_total`` via curling ``<node-local-dns pod "
"IP>:9253/metrics``, the metric should increment as new DNS requests being "
"issued from application pods are now redirected to the ``node-local-dns`` pod."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:402
msgid ""
"In the absence of a node-local DNS cache, DNS queries from application pods "
"will get directed to cluster DNS pods backed by the ``kube-dns`` service."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:406
msgid "kiam redirect on EKS"
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:407
msgid ""
"`kiam <https://github.com/uswitch/kiam>`_ agent runs on each node in an EKS "
"cluster, and intercepts requests going to the AWS metadata server to fetch "
"security credentials for pods."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:411
msgid ""
"In order to only redirect traffic from pods to the kiam agent, and pass traffic "
"from the kiam agent to the AWS metadata server without any redirection, we need "
"the socket lookup functionality in the datapath. This functionality requires "
"v5.1.16, v5.2.0 or more recent Linux kernel. Make sure the kernel version "
"installed on EKS cluster nodes satisfies these requirements."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:417
msgid "Deploy `kiam <https://github.com/uswitch/kiam>`_ using helm charts."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:425
msgid ""
"The above command may provide instructions to prepare kiam in the cluster. "
"Follow the instructions before continuing."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:428
msgid ""
"kiam must run in the ``hostNetwork`` mode and without the \"--iptables\" "
"argument. The install instructions above ensure this by default."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:431
msgid ""
"Deploy the Local Redirect Policy to redirect pod traffic to the deployed kiam "
"agent."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:439
msgid ""
"The ``addressMatcher`` ip address in the Local Redirect Policy is set to the ip "
"address of the AWS metadata server and the ``toPorts`` port to the default HTTP "
"server port. The ``toPorts`` field under ``redirectBackend`` configuration in "
"the policy is set to the port that the kiam agent listens on. The port is "
"passed as \"--port\" argument in the ``kiam-agent DaemonSet``."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:445
msgid ""
"The Local Redirect Policy namespace is set to the namespace in which kiam-agent "
"DaemonSet is deployed."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:448
msgid ""
"Once all the kiam agent pods are in ``Running`` state, the metadata requests "
"from application pods will get redirected to the node-local kiam agent pods. "
"You can verify this by running a curl command to the AWS metadata server from "
"one of the application pods, and tcpdump command on the same EKS cluster node "
"as the pod. Following is an example output, where ``192.169.98.118`` is the ip "
"address of an application pod, and ``192.168.60.99`` is the ip address of the "
"kiam agent running on the same node as the application pod."
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:478
msgid "Miscellaneous"
msgstr ""

#: ../../gettingstarted/local-redirect-policy.rst:479
msgid ""
"When a Local Redirect Policy is applied, cilium BPF datapath translates "
"frontend (identified by ip/port/protocol tuple) address from the policy to a "
"node-local backend pod selected by the policy. However, when traffic originates "
"from the node-local backend pod(s), and is destined to the policy frontend, we "
"skip translating the frontend address using ``sk_lookup_`` BPF helpers. This is "
"done in order to avoid forming a loop. As a result, traffic in such cases is "
"forwarded to the original frontend."
msgstr ""

#: ../../gettingstarted/memcached.rst:9
msgid "Getting Started Securing Memcached"
msgstr ""

#: ../../gettingstarted/memcached.rst:11
msgid ""
"This document serves as an introduction to using Cilium to enforce memcached-"
"aware security policies. It walks through a single-node Cilium environment "
"running on your machine. It is designed to take 15-30 minutes."
msgstr ""

#: ../../gettingstarted/memcached.rst:16
msgid ""
"**NOTE:** memcached-aware policy support is still in beta.  It is not yet ready "
"for production use. Additionally, the memcached-specific policy language is "
"highly likely to change in a future Cilium version."
msgstr ""

#: ../../gettingstarted/memcached.rst:20
msgid ""
"`Memcached <https://memcached.org/>`_ is a high performance, distributed memory "
"object caching system. It's simple yet powerful, and used by dynamic web "
"applications to alleviate database load. Memcached is designed to work "
"efficiently for a very large number of open connections. Thus, clients are "
"encouraged to cache their connections rather than the overhead of reopening TCP "
"connections every time they need to store or retrieve data. Multiple clients "
"can benefit from this distributed cache's performance benefits."
msgstr ""

#: ../../gettingstarted/memcached.rst:23
msgid ""
"There are two kinds of data sent in the memcache protocol: text lines and "
"unstructured (binary) data.  We will demonstrate clients using both types of "
"protocols to communicate with a memcached server."
msgstr ""

#: ../../gettingstarted/memcached.rst:29
msgid "Step 2: Deploy the Demo Application"
msgstr ""

#: ../../gettingstarted/memcached.rst:31
msgid ""
"Now that we have Cilium deployed and ``kube-dns`` operating correctly we can "
"deploy our demo memcached application.  Since our first `HTTP-aware Cilium demo "
"<https://cilium.io/blog/2017/5/4/demo-may-the-force-be-with-you/>`_ was based "
"on Star Wars, we continue with the theme for the memcached demo as well."
msgstr ""

#: ../../gettingstarted/memcached.rst:35
msgid ""
"Ever wonder how the Alliance Fleet manages the changing positions of their "
"ships? The Alliance Fleet uses memcached to store the coordinates of their "
"ships. The Alliance Fleet leverages the memcached-svc service implemented as a "
"memcached server. Each ship in the fleet constantly updates its coordinates and "
"has the ability to get the coordinates of other ships in the Alliance Fleet."
msgstr ""

#: ../../gettingstarted/memcached.rst:37
msgid ""
"In this simple example, the Alliance Fleet uses a memcached server for their "
"starfighters to store their own supergalatic coordinates and get those of other "
"starfighters."
msgstr ""

#: ../../gettingstarted/memcached.rst:39
msgid ""
"In order to avoid collisions and protect against compromised starfighters, "
"memcached commands are limited to gets for any starfighter coordinates and sets "
"only to a key specific to the starfighter. Thus the following operations are "
"allowed:"
msgstr ""

#: ../../gettingstarted/memcached.rst:41
msgid ""
"**A-wing**: can set coordinates to key \"awing-coord\" and get the key "
"coordinates."
msgstr ""

#: ../../gettingstarted/memcached.rst:42
msgid ""
"**X-wing**: can set coordinates to key \"xwing-coord\" and get the key "
"coordinates."
msgstr ""

#: ../../gettingstarted/memcached.rst:43
msgid "**Alliance-Tracker**: can get any coordinates but not set any."
msgstr ""

#: ../../gettingstarted/memcached.rst:45
msgid ""
"To keep the setup small, we will launch a small number of pods to represent a "
"larger environment:"
msgstr ""

#: ../../gettingstarted/memcached.rst:47
msgid ""
"**memcached-server** : A Kubernetes service represented by a single pod running "
"a memcached server (label app=memcd-server)."
msgstr ""

#: ../../gettingstarted/memcached.rst:48
msgid ""
"**a-wing** memcached binary client : A pod representing an A-wing starfighter, "
"which can update its coordinates and read it via the binary memcached protocol "
"(label app=a-wing)."
msgstr ""

#: ../../gettingstarted/memcached.rst:49
msgid ""
"**x-wing** memcached text-based client : A pod representing an X-wing "
"starfighter, which can update its coordinates and read it via the text-based "
"memcached protocol (label app=x-wing)."
msgstr ""

#: ../../gettingstarted/memcached.rst:50
msgid ""
"**alliance-tracker** memcached binary client : A pod representing the Alliance "
"Fleet Tracker, able to read the coordinates of all starfighters (label "
"name=fleet-tracker)."
msgstr ""

#: ../../gettingstarted/memcached.rst:53
msgid ""
"Memcached clients access the *memcached-server* on TCP port 11211 and send "
"memcached protocol messages to it."
msgstr ""

#: ../../gettingstarted/memcached.rst:58
msgid ""
"The file ``memcd-sw-app.yaml`` contains a Kubernetes Deployment for each of the "
"pods described above, as well as a Kubernetes Service *memcached-server* for "
"the Memcached server."
msgstr ""

#: ../../gettingstarted/memcached.rst:88
msgid ""
"We suggest having a main terminal window to execute *kubectl* commands and two "
"additional terminal windows dedicated to accessing the **A-Wing** and "
"**Alliance-Tracker**, which use a python library to communicate to the "
"memcached server using the binary protocol."
msgstr ""

#: ../../gettingstarted/memcached.rst:90
msgid ""
"In **all three** terminal windows, set some handy environment variables for the "
"demo with the following script:"
msgstr ""

#: ../../gettingstarted/memcached.rst:98
msgid ""
"In the terminal window dedicated for the A-wing pod, exec in, use python to "
"import the binary memcached library and set the client connection to the "
"memcached server:"
msgstr ""

#: ../../gettingstarted/memcached.rst:110
msgid ""
"In the terminal window dedicated for the Alliance-Tracker, exec in, use python "
"to import the binary memcached library and set the client connection to the "
"memcached server:"
msgstr ""

#: ../../gettingstarted/memcached.rst:125
msgid "Step 3: Test Basic Memcached Access"
msgstr ""

#: ../../gettingstarted/memcached.rst:127
msgid ""
"Let's show that each client is able to access the memcached server. Execute the "
"following to have the A-wing and X-wing starfighters update the Alliance Fleet "
"memcached-server with their respective supergalatic coordinates:"
msgstr ""

#: ../../gettingstarted/memcached.rst:129
msgid ""
"A-wing will access the memcached-server using the *binary protocol*. In your "
"terminal window for A-Wing, set A-wing's coordinates:"
msgstr ""

#: ../../gettingstarted/memcached.rst:139
msgid ""
"In your main terminal window, have X-wing starfighter set their coordinates "
"using the text-based protocol to the memcached server."
msgstr ""

#: ../../gettingstarted/memcached.rst:150
msgid ""
"Check that the Alliance Fleet Tracker is able to get all starfighters' "
"coordinates in your terminal window for the Alliance-Tracker:"
msgstr ""

#: ../../gettingstarted/memcached.rst:161
msgid "Step 4:  The Danger of a Compromised Memcached Client"
msgstr ""

#: ../../gettingstarted/memcached.rst:163
msgid ""
"Imagine if a starfighter ship is captured. Should the starfighter be able to "
"set the coordinates of other ships, or get the coordinates of all other ships? "
"Or if the Alliance-Tracker is compromised, can it modify the coordinates of any "
"starfighter ship? If every client has access to the Memcached API on port "
"11211, all have over-privileged access until further locked down."
msgstr ""

#: ../../gettingstarted/memcached.rst:166
msgid ""
"With L4 port access to the memcached server, all starfighters could write to "
"any key/ship and read all ship coordinates. In your main terminal, execute:"
msgstr ""

#: ../../gettingstarted/memcached.rst:175
msgid "In your A-Wing terminal window, confirm the over-privileged access:"
msgstr ""

#: ../../gettingstarted/memcached.rst:186
msgid "From A-Wing, set the X-Wing coordinates back to their proper position:"
msgstr ""

#: ../../gettingstarted/memcached.rst:193
msgid ""
"Thus, the Alliance Fleet Tracking System could be made weak if a single "
"starfighter ship is compromised."
msgstr ""

#: ../../gettingstarted/memcached.rst:196
msgid "Step 5: Securing Access to Memcached with Cilium"
msgstr ""

#: ../../gettingstarted/memcached.rst:198
msgid ""
"Cilium helps lock down Memcached servers to ensure clients have secure access "
"to it. Beyond just providing access to port 11211, Cilium can enforce specific "
"key value access by understanding both the text-based and the unstructured "
"(binary) memcached protocol."
msgstr ""

#: ../../gettingstarted/memcached.rst:201
msgid ""
"We'll create a policy that limits the scope of what a starfighter can access "
"and write. Thus, only the intended memcached protocol calls to the memcached-"
"server can be made."
msgstr ""

#: ../../gettingstarted/memcached.rst:203
msgid ""
"In this example, we'll only allow A-Wing to get and set the key \"awing-coord"
"\", only allow X-Wing to get and set key \"xwing-coord\", and allow Alliance-"
"Tracker to only get coordinates."
msgstr ""

#: ../../gettingstarted/memcached.rst:208
msgid ""
"Here is the *CiliumNetworkPolicy* rule that limits the access of starfighters "
"to their own key and allows Alliance Tracker to get any coordinate:"
msgstr ""

#: ../../gettingstarted/memcached.rst:212
msgid ""
"A *CiliumNetworkPolicy* contains a list of rules that define allowed memcached "
"commands, and requests that do not match any rules are denied. The rules "
"explicitly match connections destined to the Memcached Service on TCP 11211."
msgstr ""

#: ../../gettingstarted/memcached.rst:215
msgid ""
"The rules apply to inbound (i.e., \"ingress\") connections bound for memcached-"
"server pods (as indicated by ``app:memcached-server`` in the \"endpointSelector"
"\" section).  The rules apply differently depending on the client pod: ``app:a-"
"wing``, ``app:x-wing``, or ``name:fleet-tracker`` as indicated by the "
"\"fromEndpoints\" section."
msgstr ""

#: ../../gettingstarted/memcached.rst:219
msgid ""
"With the policy in place, A-wings can only get and set the key \"awing-coord\"; "
"similarly the X-Wing can only get and set \"xwing-coord\". The Alliance Tracker "
"can only get coordinates - not set."
msgstr ""

#: ../../gettingstarted/memcached.rst:221
msgid ""
"Apply this Memcached-aware network security policy using ``kubectl`` in your "
"main terminal window:"
msgstr ""

#: ../../gettingstarted/memcached.rst:227
msgid ""
"If we then try to perform the attacks from the *X-wing* pod from the main "
"terminal window, we'll see that they are denied:"
msgstr ""

#: ../../gettingstarted/memcached.rst:234
msgid ""
"From the A-Wing terminal window, we can confirm that if *A-wing* goes outside "
"of the bounds of its allowed calls. You may need to run the ``client.get`` "
"command twice for the python call:"
msgstr ""

#: ../../gettingstarted/memcached.rst:249
msgid ""
"Similarly, the Alliance-Tracker cannot set any coordinates, which you can "
"attempt from the Alliance-Tracker terminal window:"
msgstr ""

#: ../../gettingstarted/memcached.rst:266
msgid "The policy is working as expected."
msgstr ""

#: ../../gettingstarted/memcached.rst:268
msgid ""
"With the CiliumNetworkPolicy in place, the allowed Memcached calls are still "
"allowed from the respective pods."
msgstr ""

#: ../../gettingstarted/memcached.rst:270
msgid "In the main terminal window, execute:"
msgstr ""

#: ../../gettingstarted/memcached.rst:286
msgid "In the A-Wing terminal window, execute:"
msgstr ""

#: ../../gettingstarted/memcached.rst:297
msgid "In the Alliance-Tracker terminal window, execute:"
msgstr ""

#: ../../gettingstarted/memcached.rst:312
msgid ""
"You have now installed Cilium, deployed a demo app, and tested L7 memcached-"
"aware network security policies.  To clean up, in your main terminal window, "
"run:"
msgstr ""

#: ../../gettingstarted/memcached.rst:320
msgid "For some handy memcached references, see below:"
msgstr ""

#: ../../gettingstarted/memcached.rst:322
msgid "https://memcached.org/"
msgstr ""

#: ../../gettingstarted/memcached.rst:323
msgid "https://github.com/memcached/memcached/blob/master/doc/protocol.txt"
msgstr ""

#: ../../gettingstarted/memcached.rst:324
msgid "https://python-binary-memcached.readthedocs.io/en/latest/intro/"
msgstr ""

#: ../../gettingstarted/microk8s.rst:11
msgid "Getting Started Using MicroK8s"
msgstr ""

#: ../../gettingstarted/microk8s.rst:13
msgid ""
"This guide uses `microk8s <https://microk8s.io/>`_ to demonstrate deployment "
"and operation of Cilium in a single-node Kubernetes cluster. To run Cilium "
"inside microk8s, a GNU/Linux distribution with kernel 4.9 or later is required "
"(per the :ref:`admin_system_reqs`)."
msgstr ""

#: ../../gettingstarted/microk8s.rst:19
msgid "Install microk8s"
msgstr ""

#: ../../gettingstarted/microk8s.rst:21
msgid ""
"Install ``microk8s`` >= 1.15 as per microk8s documentation: `MicroK8s User "
"guide <https://microk8s.io/docs/>`_."
msgstr ""

#: ../../gettingstarted/microk8s.rst:24
msgid "Enable the microk8s Cilium service"
msgstr ""

#: ../../gettingstarted/microk8s.rst:30
msgid ""
"Cilium is now configured! The ``cilium`` CLI is provided as ``microk8s.cilium``."
msgstr ""

#: ../../gettingstarted/microk8s.rst:33
msgid "Next steps"
msgstr ""

#: ../../gettingstarted/microk8s.rst:35
msgid ""
"Now that you have a Kubernetes cluster with Cilium up and running, you can take "
"a couple of next steps to explore various capabilities:"
msgstr ""

#: ../../gettingstarted/microk8s.rst:39
msgid ":ref:`gs_dns`"
msgstr ""

#: ../../gettingstarted/microk8s.rst:40
msgid ":ref:`gs_cassandra`"
msgstr ""

#: ../../gettingstarted/microk8s.rst:41
msgid ":ref:`gs_kafka`"
msgstr ""

#: ../../gettingstarted/policy-creation.rst:11
msgid "Creating policies from verdicts"
msgstr ""

#: ../../gettingstarted/policy-creation.rst:13
msgid ""
"Policy Audit Mode configures Cilium to allow all traffic while logging all "
"connections that would otherwise be dropped by policy. Policy Audit Mode may be "
"configured for the entire daemon using ``--policy-audit-mode=true``. When "
"Policy Audit Mode is enabled, no network policy is enforced so this setting is "
"**not recommended for production deployment**. Policy Audit Mode supports "
"auditing network policies implemented at networks layers 3 and 4. This guide "
"walks through the process of creating policies using Policy Audit Mode."
msgstr ""

#: ../../gettingstarted/policy-creation.rst:25
msgid "Enable Policy Audit Mode"
msgstr ""

#: ../../gettingstarted/policy-creation.rst:27
msgid "To observe policy audit messages, follow these steps:"
msgstr ""

#: ../../gettingstarted/policy-creation.rst:29
msgid "Enable Policy Audit Mode in the daemon"
msgstr ""

#: ../../gettingstarted/policy-creation.rst:33
#: ../../gettingstarted/policy-creation.rst:177
msgid "Configure via kubectl"
msgstr ""

#: ../../gettingstarted/policy-creation.rst:45
#: ../../gettingstarted/policy-creation.rst:189
msgid "Helm Upgrade"
msgstr ""

#: ../../gettingstarted/policy-creation.rst:47
msgid ""
"If you installed Cilium via ``helm install``, then you can use ``helm upgrade`` "
"to enable Policy Audit Mode:"
msgstr ""

#: ../../gettingstarted/policy-creation.rst:57
msgid "Apply a default-deny policy:"
msgstr ""

#: ../../gettingstarted/policy-creation.rst:61
msgid ""
"CiliumNetworkPolicies match on pod labels using an \"endpointSelector\" to "
"identify the sources and destinations to which the policy applies. The above "
"policy denies traffic sent to any pods with label (``org=empire``). Due to the "
"Policy Audit Mode enabled above, the traffic will not actually be denied but "
"will instead trigger policy verdict notifications."
msgstr ""

#: ../../gettingstarted/policy-creation.rst:73
msgid ""
"With the above policy, we will enable default-deny posture on ingress to pods "
"with the label ``org=empire`` and enable the policy verdict notifications for "
"those pods. The same principle applies on egress as well."
msgstr ""

#: ../../gettingstarted/policy-creation.rst:80
msgid "Observe policy verdicts"
msgstr ""

#: ../../gettingstarted/policy-creation.rst:82
msgid ""
"In this example, we are tasked with applying security policy for the deathstar. "
"First, from the Cilium pod we need to monitor the notifications for policy "
"verdicts using ``cilium monitor -t policy-verdict``. We'll be monitoring for "
"inbound traffic towards the deathstar to identify that traffic and determine "
"whether to extend the network policy to allow that traffic."
msgstr ""

#: ../../gettingstarted/policy-creation.rst:88
msgid ""
"From another terminal with kubectl access, send some traffic from the "
"tiefighter to the deathstar:"
msgstr ""

#: ../../gettingstarted/policy-creation.rst:96
msgid ""
"Back in the Cilium pod, the policy verdict logs are printed in the monitor "
"output:"
msgstr ""

#: ../../gettingstarted/policy-creation.rst:105
msgid ""
"In the above example, we can see that endpoint ``232`` has received traffic "
"(``ingress true``) which doesn't match the policy (``action audit match "
"none``). The source of this traffic has the identity ``31028``. Let's gather a "
"bit more information about what these numbers mean:"
msgstr ""

#: ../../gettingstarted/policy-creation.rst:132
msgid "Create the Network Policy"
msgstr ""

#: ../../gettingstarted/policy-creation.rst:134
msgid ""
"Given the above information, we now know the labels of the target pod, the "
"labels of the peer that's attempting to connect, the direction of the traffic "
"and the port. In this case, we can see clearly that it's an empire craft so "
"once we've determined that we expect this traffic to arrive at the deathstar, "
"we can form a policy to match the traffic:"
msgstr ""

#: ../../gettingstarted/policy-creation.rst:149
msgid ""
"Now if we run the landing requests again, we can observe in the monitor output "
"that the traffic which was previously audited to be dropped by the policy are "
"now reported as allowed:"
msgstr ""

#: ../../gettingstarted/policy-creation.rst:158
msgid "Executed from the cilium pod:"
msgstr ""

#: ../../gettingstarted/policy-creation.rst:165
msgid ""
"Now the policy verdict states that the traffic would be allowed: ``action "
"allow``. Success!"
msgstr ""

#: ../../gettingstarted/policy-creation.rst:171
msgid ""
"These steps should be repeated for each connection in the cluster to ensure "
"that the network policy allows all of the expected traffic. The final step "
"after deploying the policy is to disable Policy Audit Mode again:"
msgstr ""

#: ../../gettingstarted/policy-creation.rst:215
msgid ""
"We hope you enjoyed the tutorial.  Feel free to play more with the setup, "
"follow the `gs_http` guide, and reach out to us on the `Cilium Slack channel "
"<https://cilium.herokuapp.com>`_ with any questions!"
msgstr ""

#: ../../gettingstarted/rancher-desktop.rst:11
msgid "Getting Started Using Rancher Desktop"
msgstr ""

#: ../../gettingstarted/rancher-desktop.rst:13
msgid ""
"This guide walks you through installation of Cilium on `Rancher Desktop "
"<https://rancherdesktop.io>`_, an open-source desktop application for Mac, "
"Windows and Linux."
msgstr ""

#: ../../gettingstarted/rancher-desktop.rst:17
msgid "Configure Rancher Desktop"
msgstr ""

#: ../../gettingstarted/taints.rst:11
msgid "Considerations on node pool taints and unmanaged pods"
msgstr ""

#: ../../gettingstarted/taints.rst:13
msgid ""
"Depending on the environment or cloud provider being used, a CNI plugin and/or "
"configuration file may be pre-installed in nodes belonging to a given cluster "
"where Cilium is being installed or already running. Upon starting on a given "
"node, and if it is intended as the exclusive CNI plugin for the cluster, Cilium "
"does its best to take ownership of CNI on the node. However, a couple "
"situations can prevent this from happening:"
msgstr ""

#: ../../gettingstarted/taints.rst:20
msgid ""
"Cilium can only take ownership of CNI on a node after starting. Pods starting "
"before Cilium runs on a given node may get IPs from the pre-configured CNI."
msgstr ""

#: ../../gettingstarted/taints.rst:23
msgid ""
"Some cloud providers may revert changes made to the CNI configuration by Cilium "
"during operations such as node reboots, updates or routine maintenance."
msgstr ""

#: ../../gettingstarted/taints.rst:26
msgid ""
"This is notably the case with GKE (non-Dataplane V2), in which node reboots and "
"upgrades will undo changes made by Cilium and re-instate the default CNI "
"configuration."
msgstr ""

#: ../../gettingstarted/taints.rst:30
msgid ""
"To help overcome this situation to the largest possible extent in environments "
"and cloud providers where Cilium isn't supported as the single CNI, Cilium can "
"manipulate Kubernetes's `taints <https://kubernetes.io/docs/concepts/scheduling-"
"eviction/taint-and-toleration/>`_ on a given node to help preventing pods from "
"starting before Cilium runs on said node. The mechanism works as follows:"
msgstr ""

#: ../../gettingstarted/taints.rst:36
msgid ""
"The cluster administrator places a taint with key ``node.cilium.io/agent-not-"
"ready`` on a given uninitialized node. Depending on the taint's effect (see "
"below), this prevents pods that don't have a matching toleration from either "
"being scheduled or altogether running on the node until the taint is removed."
msgstr ""

#: ../../gettingstarted/taints.rst:42
msgid ""
"Cilium runs on the node, initializes it and, once ready, removes the ``node."
"cilium.io/agent-not-ready`` taint."
msgstr ""

#: ../../gettingstarted/taints.rst:45
msgid ""
"From this point on, pods will start being scheduled and running on the node, "
"having their networking managed by Cilium."
msgstr ""

#: ../../gettingstarted/taints.rst:48
msgid ""
"The taint's effect should be chosen taking into account the following "
"considerations:"
msgstr ""

#: ../../gettingstarted/taints.rst:51
msgid ""
"If ``NoSchedule`` is used, pods won't be *scheduled* to a node until Cilium has "
"the chance to remove the taint. However, one practical effect of this is that "
"if some external process (such as a reboot) resets the CNI configuration on "
"said node, pods that were already scheduled will be allowed to start "
"concurrently with Cilium when the node next reboots, and hence may become "
"unmanaged and have their networking being managed by another CNI plugin."
msgstr ""

#: ../../gettingstarted/taints.rst:58
msgid ""
"If ``NoExecute`` is used, pods won't be *executed* (nor *scheduled*) on a node "
"until Cilium has had the chance to remove the taint. One practical effect of "
"this is that whenever the taint is added back to the node by some external "
"process (such as during an upgrade or eventually a routine operation), pods "
"will be evicted from the node until Cilium has had the chance to remove the "
"taint."
msgstr ""

#: ../../gettingstarted/taints.rst:65
msgid ""
"Another important thing to consider is the concept of node itself, and the "
"different point of views over a node. For example, the instance/VM which backs "
"a Kubernetes node can be patched or reset filesystem-wise by a cloud provider, "
"or altogether replaced with an entirely new instance/VM that comes back with "
"the same name as the already-existing Kubernetes ``Node`` resource. Even though "
"in said scenarios the node-pool-level taint will be added back to the ``Node`` "
"resource, pods that were already scheduled to the node having this name will "
"run on the node at the same time as Cilium, potentially becoming unmanaged. "
"This is why ``NoExecute`` is recommended, as assuming the taint is added back "
"in this scenario, already-scheduled pods won't run."
msgstr ""

#: ../../gettingstarted/taints.rst:76
msgid ""
"However, on some environments or cloud providers, and as mentioned above, it "
"may happen that a taint established at the node-pool level is added back to a "
"node after Cilium has removed it and for reasons other than a node upgrade/"
"reset. The exact circumstances in which this may happen may vary, but this may "
"lead to unexpected/undesired pod evictions in the particular case when "
"``NoExecute`` is being used as the taint effect. It is, thus, recommended that "
"in each deployment and depending on the environment or cloud provider, a "
"careful decision is made regarding the taint effect (or even regarding whether "
"to use the taint-based approach at all) based on the information above, on the "
"environment or cloud provider's documentation, and on the fact that one is "
"essentially establishing a trade-off between having unmanaged pods in the "
"cluster (which can lead to dropped traffic and other issues) and having "
"unexpected/undesired evictions (which can lead to application downtime)."
msgstr ""

#: ../../gettingstarted/taints.rst:90
msgid ""
"Taking into account all of the above, throughout the Cilium documentation we "
"recommend ``NoExecute`` to be used as we believe it to be the least disruptive "
"mode that users can use to deploy Cilium on cloud providers."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:11
msgid "Inspecting TLS Encrypted Connections with Cilium"
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:13
msgid ""
"This document serves as an introduction for how network security teams can use "
"Cilium to transparently inspect TLS-encrypted connections.  This TLS-aware "
"inspection allows Cilium API-aware visibility and policy to function even for "
"connections where client to server communication is protected by TLS, such as "
"when a client accesses the API service via HTTPS.  This capability is similar "
"to what is possible to traditional hardware firewalls, but is implemented "
"entirely in software on the Kubernetes worker node, and is policy driven, "
"allowing inspection to target only selected network connectivity."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:20
msgid ""
"This type of visibility is extremely valuable to be able to monitor how "
"external API services are being used, for example, understanding which S3 "
"buckets are being accessed by an given application."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:27
msgid "Edit the ClusterRole for Cilium to give it access to Kubernetes secrets"
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:34
msgid "Add the following section at the end of the file:"
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:50
msgid ""
"To demonstrate TLS-interception we will use the same ``mediabot`` application "
"that we used for the DNS-aware policy example. This application will access the "
"Star Wars API service using HTTPS, which would normally mean that network-layer "
"mechanisms like Cilium would not be able to see the HTTP-layer details of the "
"communication, since all application data is encrypted using TLS before that "
"data is sent on the network."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:57
msgid ""
"Creating an internal Certificate Authority (CA) and associated certificates "
"signed by that CA to enable TLS interception."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:58
msgid ""
"Using Cilium network policy to select the traffic to intercept using DNS-based "
"policy rules."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:59
msgid ""
"Inspecting the details of the HTTP request using cilium monitor (accessing this "
"visibility data via Hubble, and applying Cilium network policies to filter/"
"modify the HTTP request is also possible, but is beyond the scope of this "
"simple Getting Started Guide)"
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:63
msgid "First off, we will create a single pod ``mediabot`` application:"
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:74
msgid "A Brief Overview of the TLS Certificate Model"
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:76
msgid ""
"TLS is a protocol that \"wraps\" other protocols like HTTP and ensures that "
"communication between client and server has confidentiality (no one can read "
"the data except the intended recipient), integrity (recipient can confirm that "
"the data has not been modified in transit), and authentication (sender can "
"confirm that it is talking with the intended destination, not an impostor).  We "
"will provide a highly simplified overview of TLS in this document, but for full "
"details, please see `<https://en.wikipedia.org/wiki/"
"Transport_Layer_Security>`_ ."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:83
msgid ""
"From an authentication perspective, the TLS model relies on a \"Certificate "
"Authority\" (CA) which is an entity that is trusted to create proof that a "
"given network service (e.g., www.cilium.io) is who they say they are.   The "
"goal is to prevents a malicious party in the network between the client and the "
"server from intercepting the traffic and pretending to be the destination "
"server."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:88
msgid ""
"In the case of \"friendly interception\" for network security monitoring, "
"Cilium uses a model similar to traditional firewalls with TLS inspection "
"capabilities:  the network security team creates their own \"internal "
"certificate authority\" that can be used to create alternative certificates for "
"external destinations.  This model requires each client workload to also trust "
"this new certificate, otherwise the client's TLS library will reject the "
"connection as invalid.  In this model, the network firewall uses the "
"certificate signed by the internal CA to act like the destination service and "
"terminate the TLS connection.  This allows the firewall to inspect and even "
"modify the application layer data, and then initiate another TLS connect to the "
"actual destination service."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:98
msgid ""
"The CA model within TLS is based on cryptographic keys and certificates.  "
"Realizing the above model requires four primary steps:"
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:101
msgid ""
"Create an internal certificate authority by generating a CA private key and CA "
"certificate."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:103
msgid ""
"For any destination where TLS inspection is desired (e.g., artii.herokuapp.com "
"in the example below), generate a private key and certificate signing request "
"with a common name that matches the destination DNS name."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:107
msgid "Use the CA private key to create a signed certificate."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:109
msgid ""
"Ensure that all clients where TLS inspection is have the CA certificate "
"installed so that they will trust all certificates signed by that CA."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:112
msgid ""
"Given that Cilium will be terminating the initial TLS connection from the "
"client and creating a new TLS connection to the destination, Cilium must be "
"told the set of CAs that it should trust when validating the new TLS connection "
"to the destination service."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:118
msgid ""
"In a non-demo environment it is EXTREMELY important that you keep the above "
"private keys safe, as anyone with access to this private key will be able to "
"inspect TLS-encrypted traffic (certificates on the other hand are public "
"information, and are not at all sensitive).  In the guide below, the CA private "
"key does not need to be provided to Cilium at all (it is used only to create "
"certificates, which can be done offline) and private keys for individual "
"destination services are stored as Kubernetes secrets. These secrets should be "
"stored in a namespace where they can be accessed by Cilium, but not general "
"purpose workloads."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:127
msgid "Generating and Installing TLS Keys and Certificates"
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:129
msgid ""
"Now that we have explained the high-level certificate model used by TLS, we "
"will walk through the concrete steps to generate the appropriate keys and "
"certificates using the ``openssl`` utility."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:132
msgid ""
"The following image describes the different files containing cryptographic data "
"that are generated or copied, and what components in the system need access to "
"those files:"
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:137
msgid ""
"You can use openssl on your local system if it is already installed, but if not "
"a simple shortcut is to use ``kubectl exec`` to execute ``/bin/bash`` within "
"any of the cilium pods, and then run the resulting ``openssl`` commands.  Use "
"``kubectl cp`` to copy the resulting files out of the cilium pod when it is "
"time to use them to create Kubernetes secrets of copy them to the ``mediabot`` "
"pod."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:144
msgid "Create an Internal Certificate Authority (CA)"
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:146
msgid "Generate CA private key named 'myCA.key':"
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:152
msgid "Enter any password, just remember it for some of the later steps."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:154
msgid "Generate CA certificate from the private key:"
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:160
msgid ""
"The values you enter for each prompt do not need to be any specific value, and "
"do not need to be accurate."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:164
msgid "Create Private Key and Certificate Signing Request for a Given DNS Name"
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:166
msgid ""
"Generate an internal private key and certificate signing with a common name "
"that matches the DNS name of the destination service to be intercepted for "
"inspection (in this example, use ``artii.herokuapp.com``)."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:169
msgid "First create the private key:"
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:175
msgid ""
"Next, create a certificate signing request, specifying the DNS name of the "
"destination service for the common name field when prompted.  All other prompts "
"can be filled with any value."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:182
msgid ""
"The only field that must be a specific value is ensuring that ``Common Name`` "
"is the exact DNS destination ``artii.herokuapp.com`` that will be provided to "
"the client."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:186
msgid ""
"This example workflow will work for any DNS name as long as the toFQDNs rule in "
"the policy YAML (below) is also updated to match the DNS name in the "
"certificate."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:191
msgid "Use CA to Generate a Signed Certificate for the DNS Name"
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:193
msgid ""
"Use the internal CA private key to create a signed certificate for artii."
"herokuapp.com named ``internal-artii.crt``."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:199
msgid ""
"Next we create a Kubernetes secret that includes both the private key and "
"signed certificates for the destination service:"
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:206
msgid "Add the Internal CA as a Trusted CA Inside the Client Pod"
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:208
msgid ""
"Once the CA certificate is inside the client pod, we still must make sure that "
"the CA file is picked up by the TLS library used by your application.  Most "
"Linux applications automatically use a set of trusted CA certificates that are "
"bundled along with the Linux distro. In this guide, we are using an Ubuntu "
"container as the client, and so will update it with Ubuntu specific "
"instructions.  Other Linux distros will have different mechanisms.  Also, "
"individual applications may leverage their own certificate stores rather than "
"use the OS certificate store.  Java applications and the aws-cli are two common "
"examples.  Please refer to the application or application runtime documentation "
"for more details."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:215
msgid ""
"For Ubuntu, we first copy the additional CA certificate to the client pod "
"filesystem"
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:221
msgid ""
"Then run the Ubuntu-specific utility that adds this certificate to the global "
"set of trusted certificate authorities in /etc/ssl/certs/ca-certificates.crt ."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:228
msgid "This command will issue a WARNING, but this can be ignored."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:231
msgid "Provide Cilium with List of Trusted CAs"
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:233
msgid ""
"Next, we will provide Cilium with the set of CAs that it should trust when "
"originating the secondary TLS connections. This list should correspond to the "
"standard set of global CAs that your organization trusts.  A logical option for "
"this is the standard CAs that are trusted by your operating system, since this "
"is the set of CAs that were being used prior to introducing TLS inspection."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:237
msgid ""
"To keep things simple, in this example we will simply copy this list out of the "
"Ubuntu filesystem of the mediabot pod, though it is important to understand "
"that this list of trusted CAs is not specific to a particular TLS client or "
"server, and so this step need only be performed once regardless of how many TLS "
"clients or servers are involved in TLS inspection."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:245
msgid ""
"We then will create a Kubernetes secret using this certificate bundle so that "
"Cilium can read the certificate bundle and use it to validate outgoing TLS "
"connections."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:255
msgid "Apply DNS and TLS-aware Egress Policy"
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:257
msgid ""
"Up to this point, we have created keys and certificates to enable TLS "
"inspection, but we have not told Cilium which traffic we want to intercept and "
"inspect.   This is done using the same Cilium Network Policy constructs that "
"are used for other Cilium Network Policies."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:261
msgid ""
"The following Cilium network policy indicates that Cilium should perform HTTP-"
"aware inspect of communication between the ``mediabot`` pod to ``artii."
"herokuapp.com``."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:268
msgid ""
"The ``endpointSelector`` means that this policy will only apply to pods with "
"labels ``class: mediabot, org:empire`` to have the egress access."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:269
msgid ""
"The first egress section uses ``toFQDNs: matchName`` specification to allow TCP "
"port 443 egress to ``artii.herokuapp.com``."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:270
msgid ""
"The ``http`` section below the toFQDNs rule indicates that such connections "
"should be parsed as HTTP, with a policy of ``{}`` which will allow all requests."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:272
msgid ""
"The ``terminatingTLS`` and ``originatingTLS`` sections indicate that TLS "
"interception should be used to terminate the initial TLS connection from "
"mediabot and initiate a new out-bound TLS connection to ``artii.herokuapp.com``."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:274
msgid ""
"The second egress section allows ``mediabot`` pods to access ``kube-dns`` "
"service. Note that ``rules: dns`` instructs Cilium to inspect and allow DNS "
"lookups matching specified patterns. In this case, inspect and allow all DNS "
"queries."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:278
msgid ""
"Note that with this policy the ``mediabot`` doesn't have access to any internal "
"cluster service other than ``kube-dns`` and will have no access to any other "
"external destinations either. Refer to :ref:`Network Policy` to learn more "
"about policies for controlling access to internal cluster services."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:290
msgid "Demonstrating TLS Inspection"
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:292
msgid ""
"Recall that the policy we pushed will allow all HTTPS requests from "
"``mediabot`` to ``artii.herokuapp.com``, but will parse all data at the HTTP-"
"layer, meaning that cilium monitor will report each HTTP request and response."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:295
msgid ""
"To see this, open a new window and run the following command to identity the "
"name of the cilium pod (e.g, cilium-97s78) that is running on the same "
"Kubernetes worker node as the ``mediabot`` pod."
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:298
msgid ""
"Then start running cilium monitor in \"L7 mode\" to monitor for HTTP requests "
"being reported by Cilium:"
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:304
msgid ""
"Next in the original window, from the ``mediabot`` pod we can access ``artii."
"herokuapp.com`` via HTTPS:"
msgstr ""

#: ../../gettingstarted/tls-visibility.rst:316
msgid ""
"Looking back at the cilium monitor window, you will see each individual HTTP "
"request and response.  For example::"
msgstr ""

#: ../../gettingstarted/vtep.rst:11
msgid "VXLAN Tunnel Endpoint (VTEP) Integration (beta)"
msgstr ""

#: ../../gettingstarted/vtep.rst:15
msgid ""
"The VTEP integration allows third party VTEP devices to send and receive "
"traffic to and from Cilium-managed pods directly using VXLAN. This allows for "
"example external load balancers like BIG-IP to load balance traffic to Cilium-"
"managed pods using VXLAN."
msgstr ""

#: ../../gettingstarted/vtep.rst:19
msgid ""
"This document explains how to enable VTEP support and configure Cilium with "
"VTEP endpoint IPs, CIDRs, and MAC addresses."
msgstr ""

#: ../../gettingstarted/vtep.rst:25
msgid ""
"This guide assumes that Cilium has been correctly installed in your Kubernetes "
"cluster. Please see :ref:`k8s_quick_install` for more information. If unsure, "
"run ``cilium status`` and validate that Cilium is up and running. This guide "
"also assumes VTEP devices has been configured with VTEP endpoint IP, VTEP "
"CIDRs, VTEP MAC addresses (VTEP MAC). The VXLAN network identifier (VNI) *must* "
"be configured as VNI ``2``, which represents traffic from the VTEP as the world "
"identity. See :ref:`reserved_labels` for more details."
msgstr ""

#: ../../gettingstarted/vtep.rst:35
msgid ""
"This feature is in beta, and currently, it is partially incompatible the L7 "
"policy. When a pod with an egress L7 policy sends a request to VTEP devices, "
"the VTEP redirection is bypassed. The improvement is tracked in :gh-issue:"
"`19699`."
msgstr ""

#: ../../gettingstarted/vtep.rst:41
msgid "Enable VXLAN Tunnel Endpoint (VTEP) integration"
msgstr ""

#: ../../gettingstarted/vtep.rst:43
msgid ""
"This feature requires a Linux 5.2 kernel or later, and is disabled by default. "
"When enabling the VTEP integration, you must also specify the IPs, CIDR ranges "
"and MACs for each VTEP device as part of the configuration."
msgstr ""

#: ../../gettingstarted/vtep.rst:51
msgid ""
"If you installed Cilium via ``helm install``, you may enable the VTEP support "
"with the following command:"
msgstr ""

#: ../../gettingstarted/vtep.rst:67
msgid ""
"VTEP support can be enabled by setting the following options in the ``cilium-"
"config`` ConfigMap:"
msgstr ""

#: ../../gettingstarted/vtep.rst:78
msgid "Restart Cilium daemonset:"
msgstr ""

#: ../../gettingstarted/vtep.rst:86
msgid "How to test VXLAN Tunnel Endpoint (VTEP) Integration"
msgstr ""

#: ../../gettingstarted/vtep.rst:88
msgid ""
"Start up a Linux VM with node network connectivity to Cilium node. To configure "
"the Linux VM, you will need to be ``root`` user or run the commands below using "
"``sudo``."
msgstr ""

#: ../../gettingstarted/vtep.rst:130
msgid ""
"If you are managing multiple VTEPs, follow the above process for each instance. "
"Once the VTEPs are configured, you can configure Cilium to use the MAC, IP and "
"CIDR ranges that you have configured on the VTEPs. Follow the instructions to :"
"ref:`enable_vtep`."
msgstr ""

#: ../../gettingstarted/vtep.rst:134
msgid "To test the VTEP network connectivity:"
msgstr ""

#: ../../gettingstarted/vtep.rst:144
msgid ""
"This feature does not work with traffic from the host network namespace "
"(including pods with ``hostNetwork=true``)."
msgstr ""

#: ../../gettingstarted/vtep.rst:145
msgid ""
"This feature does not work with ipsec encryption between Cilium managed pod and "
"VTEPs."
msgstr ""
