# SOME DESCRIPTIVE TITLE.
# Copyright (C) Cilium Authors
# This file is distributed under the same license as the Cilium package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
msgid ""
msgstr ""
"Project-Id-Version: Cilium\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-05-25 00:09+0800\n"
"PO-Revision-Date: 2022-06-08 23:54+0800\n"
"Last-Translator: \n"
"Language-Team: \n"
"Language: zh\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"
"X-Generator: Poedit 3.0.1\n"

#: ../../gettingstarted/k8s-install-download-release.rst:3
#: ../../operations/metrics.rst:3 ../../operations/performance/benchmark.rst:3
#: ../../operations/performance/index.rst:3
#: ../../operations/performance/scalability/identity-relevant-labels.rst:3
#: ../../operations/performance/scalability/index.rst:3
#: ../../operations/performance/scalability/report.rst:3
#: ../../operations/performance/tuning.rst:3
#: ../../operations/system_requirements.rst:3
#: ../../operations/troubleshooting.rst:3 ../../operations/upgrade-warning.rst:3
#: ../../operations/upgrade.rst:3
msgid ""
"WARNING: You are looking at unreleased Cilium documentation. Please use the "
"official rendered version released here: https://docs.cilium.io"
msgstr ""
"注意：你正在查看未经正式发布的 Cilium 文档。请阅读官方提供的正式版本：https://"
"docs.cilium.io"

#: ../../operations/metrics.rst:11
msgid "Monitoring & Metrics"
msgstr "监控和指标"

#: ../../operations/metrics.rst:13
msgid ""
"Cilium and Hubble can both be configured to serve `Prometheus <https://"
"prometheus.io>`_ metrics. Prometheus is a pluggable metrics collection and "
"storage system and can act as a data source for `Grafana <https://grafana.com/"
">`_, a metrics visualization frontend. Unlike some metrics collectors like "
"statsd, Prometheus requires the collectors to pull metrics from each source."
msgstr ""
"Cilium 和 Hubble 都可以对接 `Prometheus <https://prometheus.io>`_ 指标。 "
"Prometheus 是一个可插拔的指标收集和存储系统，可以作为指标可视化前端`Grafana "
"<https://grafana.com/>`_ 的数据源。 与 statsd 等一些指标收集器不同，Prometheus "
"要求收集器从每个数据源中提取指标。"

#: ../../operations/metrics.rst:20
msgid "Cilium and Hubble metrics can be enabled independently of each other."
msgstr "Cilium 和 Hubble 指标可以相互独立地启用。"

#: ../../operations/metrics.rst:23
msgid "Cilium Metrics"
msgstr "Cilium 指标"

#: ../../operations/metrics.rst:25
msgid ""
"Cilium metrics provide insights into the state of Cilium itself, namely of the "
"``cilium-agent``, ``cilium-envoy``, and ``cilium-operator`` processes. To run "
"Cilium with Prometheus metrics enabled, deploy it with the ``prometheus."
"enabled=true`` Helm value set."
msgstr ""
"Cilium 指标提供了对 Cilium 本身状态的监测进程，即“cilium-agent”、“cilium-"
"envoy”和“cilium-operator”进程。 要在启用 Prometheus 指标的情况下运行 Cilium，请"
"使用 ``prometheus.enabled=true`` Helm 的配置。"

#: ../../operations/metrics.rst:30
msgid ""
"Cilium metrics are exported under the ``cilium_`` Prometheus namespace. Envoy "
"metrics are exported under the ``envoy_`` Prometheus namespace, of which the "
"Cilium-defined metrics are exported under the ``envoy_cilium_`` namespace. When "
"running and collecting in Kubernetes they will be tagged with a pod name and "
"namespace."
msgstr ""
"Cilium 指标在 ``cilium_`` Prometheus 命名空间下导出。 Envoy 指标在 ``envoy_`` "
"Prometheus 命名空间下导出，其中 Cilium 定义的指标在``envoy_cilium_`` 命名空间下"
"导出。 在 Kubernetes 中运行和收集时，它们将被标记为 pod 名称和命名空间。"

#: ../../operations/metrics.rst:37 ../../operations/metrics.rst:99
msgid "Installation"
msgstr "安装"

#: ../../operations/metrics.rst:39
msgid ""
"You can enable metrics for ``cilium-agent`` (including Envoy) with the Helm "
"value ``prometheus.enabled=true``. To enable metrics for ``cilium-operator``, "
"use ``operator.prometheus.enabled=true``."
msgstr ""
"您可以使用 Helm 配置 “prometheus.enabled=true”为“cilium-agent”（包括 Envoy）启用"
"指标。 要为“cilium-operator”启用指标时，请使用“operator.prometheus."
"enabled=true”。"

#: ../../operations/metrics.rst:50
msgid ""
"The ports can be configured via ``prometheus.port``, ``proxy.prometheus.port``, "
"or ``operator.prometheus.port`` respectively."
msgstr ""
"端口可以分别通过“prometheus.port”、“proxy.prometheus.port”或“operator."
"prometheus.port”进行配置。"

#: ../../operations/metrics.rst:53
msgid ""
"When metrics are enabled, all Cilium components will have the following "
"annotations. They can be used to signal Prometheus whether to scrape metrics:"
msgstr ""
"启用指标后，所有 Cilium 组件都将具有以下注释。 它们可用于向 Prometheus 发出是否"
"要抓取指标的信号："

#: ../../operations/metrics.rst:61
msgid ""
"To collect Envoy metrics the Cilium chart will create a Kubernetes headless "
"service named ``cilium-agent`` with the ``prometheus.io/scrape:'true'`` "
"annotation set:"
msgstr ""
"为了收集 Envoy 指标，Cilium 图表将创建一个名为 cilium-agent 的 Kubernetes 无头服"
"务，并带有 ``prometheus.io/scrape:'true'`` 注释集："

#: ../../operations/metrics.rst:69
msgid ""
"This additional headless service in addition to the other Cilium components is "
"needed as each component can only have one Prometheus scrape and port "
"annotation."
msgstr ""
"除了其他 Cilium 组件之外，还需要这种额外的 Headless Service，因为每个组件只能有"
"一个 Prometheus 抓取和端口注释。"

#: ../../operations/metrics.rst:72
msgid ""
"Prometheus will pick up the Cilium and Envoy metrics automatically if the "
"following option is set in the ``scrape_configs`` section:"
msgstr ""
"如果在“scrape_configs”部分设置了以下选项，Prometheus 将自动获取 Cilium 和 Envoy "
"指标："

#: ../../operations/metrics.rst:92
msgid "Hubble Metrics"
msgstr "Hubble 指标"

#: ../../operations/metrics.rst:94
msgid ""
"While Cilium metrics allow you to monitor the state Cilium itself, Hubble "
"metrics on the other hand allow you to monitor the network behavior of your "
"Cilium-managed Kubernetes pods with respect to connectivity and security."
msgstr ""
"Cilium 指标允许您监控 Cilium 本身的状态，另一方面，Hubble 指标允许您监控 Cilium "
"管理的 Kubernetes pod 在连接性和安全性方面的网络行为。"

#: ../../operations/metrics.rst:101
msgid ""
"To deploy Cilium with Hubble metrics enabled, you need to enable Hubble with "
"``hubble.enabled=true`` and provide a set of Hubble metrics you want to enable "
"via ``hubble.metrics.enabled``."
msgstr ""
"要在启用 Hubble 指标的情况下部署 Cilium，您需要使用“hubble.enabled=true”启用 "
"Hubble，并通过“hubble.metrics.enabled”提供一组要启用的 Hubble 指标。"

#: ../../operations/metrics.rst:105
msgid ""
"Some of the metrics can also be configured with additional options. See the :"
"ref:`Hubble exported metrics<hubble_exported_metrics>` section for the full "
"list of available metrics and their options."
msgstr ""
"一些指标也可以配置额外的选项。 有关可用指标及其选项的完整列表，请参阅 :ref:"
"`Hubble 导出指标<hubble_exported_metrics>` 部分。"

#: ../../operations/metrics.rst:115
msgid ""
"The port of the Hubble metrics can be configured with the ``hubble.metrics."
"port`` Helm value."
msgstr "Hubble 指标的端口可以使用“hubble.metrics.port”Helm 值进行配置。"

#: ../../operations/metrics.rst:120
msgid ""
"L7 metrics such as HTTP, are only emitted for pods that enable :ref:`Layer 7 "
"Protocol Visibility <proxy_visibility>`."
msgstr ""
"L7 指标（例如 HTTP）仅针对启用 :ref:`Layer 7 Protocol Visibility "
"<proxy_visibility>` 的 Pod 。"

#: ../../operations/metrics.rst:123
msgid ""
"When deployed with a non-empty ``hubble.metrics.enabled`` Helm value, the "
"Cilium chart will create a Kubernetes headless service named ``hubble-metrics`` "
"with the ``prometheus.io/scrape:'true'`` annotation set:"
msgstr ""
"当使用非空的“hubble.metrics.enabled” Helm 值部署时，Cilium chart将创建一个名"
"为“hubble-metrics”的 Kubernetes 无头服务，并带有“prometheus.io/scrape:'true'” ` "
"annotation 组："

#: ../../operations/metrics.rst:132
msgid ""
"Set the following options in the ``scrape_configs`` section of Prometheus to "
"have it scrape all Hubble metrics from the endpoints automatically:"
msgstr ""
"在 Prometheus 的“scrape_configs”部分设置以下选项，让它自动从端点抓取所有Hubble相"
"关指标："

#: ../../operations/metrics.rst:154
msgid "Example Prometheus & Grafana Deployment"
msgstr "Prometheus 和 Grafana 部署示例"

#: ../../operations/metrics.rst:156
msgid ""
"If you don't have an existing Prometheus and Grafana stack running, you can "
"deploy a stack with:"
msgstr ""
"如果您没有运行现有的 Prometheus 和 Grafana 堆栈，您可以通过以下方式部署一个："

#: ../../operations/metrics.rst:163
msgid ""
"It will run Prometheus and Grafana in the ``cilium-monitoring`` namespace. If "
"you have either enabled Cilium or Hubble metrics, they will automatically be "
"scraped by Prometheus. You can then expose Grafana to access it via your "
"browser."
msgstr ""
"它将在 cilium-monitoring 命名空间中运行 Prometheus 和 Grafana。 如果您启用了 "
"Cilium 或 Hubble 指标，它们将被 Prometheus 自动抓取。 然后，您可以暴露Grafana 的"
"endpoint 以通过浏览器访问它。"

#: ../../operations/metrics.rst:171
msgid "Open your browser and access http://localhost:3000/"
msgstr "打开浏览器并访问 http://localhost:3000/"

#: ../../operations/metrics.rst:174
msgid "Metrics Reference"
msgstr "指标参考"

#: ../../operations/metrics.rst:177
msgid "cilium-agent"
msgstr "cilium-agent"

#: ../../operations/metrics.rst:180 ../../operations/metrics.rst:433
#: ../../operations/metrics.rst:467
msgid "Configuration"
msgstr "配置"

#: ../../operations/metrics.rst:182
msgid ""
"To expose any metrics, invoke ``cilium-agent`` with the ``--prometheus-serve-"
"addr`` option. This option takes a ``IP:Port`` pair but passing an empty IP (e."
"g. ``:9090``) will bind the server to all available interfaces (there is "
"usually only one in a container)."
msgstr ""
"要暴露任何指标，请使用 —prometheus-serve-addr 选项调用 cilium-agent。 此选项采用"
"``IP:Port`` 的方式配置，但传递一个空 IP（例如 ``:9090``）会将服务器绑定到所有可"
"用接口（容器中通常只有一个）。"

#: ../../operations/metrics.rst:188 ../../operations/metrics.rst:441
#: ../../operations/metrics.rst:512
msgid "Exported Metrics"
msgstr "暴露指标"

#: ../../operations/metrics.rst:191
msgid "Endpoint"
msgstr "Endpoint"

#: ../../operations/metrics.rst:194 ../../operations/metrics.rst:206
#: ../../operations/metrics.rst:215 ../../operations/metrics.rst:226
#: ../../operations/metrics.rst:236 ../../operations/metrics.rst:250
#: ../../operations/metrics.rst:263 ../../operations/metrics.rst:272
#: ../../operations/metrics.rst:290 ../../operations/metrics.rst:302
#: ../../operations/metrics.rst:317 ../../operations/metrics.rst:328
#: ../../operations/metrics.rst:337 ../../operations/metrics.rst:346
#: ../../operations/metrics.rst:356 ../../operations/metrics.rst:365
#: ../../operations/metrics.rst:376 ../../operations/metrics.rst:386
#: ../../operations/metrics.rst:397 ../../operations/metrics.rst:407
#: ../../operations/metrics.rst:418 ../../operations/metrics.rst:451
#: ../../operations/metrics.rst:520 ../../operations/metrics.rst:544
#: ../../operations/metrics.rst:558 ../../operations/metrics.rst:576
#: ../../operations/metrics.rst:598 ../../operations/metrics.rst:614
#: ../../operations/metrics.rst:628 ../../operations/metrics.rst:642
msgid "Name"
msgstr "Name"

#: ../../operations/metrics.rst:194 ../../operations/metrics.rst:206
#: ../../operations/metrics.rst:215 ../../operations/metrics.rst:226
#: ../../operations/metrics.rst:236 ../../operations/metrics.rst:250
#: ../../operations/metrics.rst:263 ../../operations/metrics.rst:272
#: ../../operations/metrics.rst:290 ../../operations/metrics.rst:302
#: ../../operations/metrics.rst:317 ../../operations/metrics.rst:328
#: ../../operations/metrics.rst:337 ../../operations/metrics.rst:346
#: ../../operations/metrics.rst:356 ../../operations/metrics.rst:365
#: ../../operations/metrics.rst:376 ../../operations/metrics.rst:386
#: ../../operations/metrics.rst:397 ../../operations/metrics.rst:407
#: ../../operations/metrics.rst:418 ../../operations/metrics.rst:451
#: ../../operations/metrics.rst:520 ../../operations/metrics.rst:544
#: ../../operations/metrics.rst:558 ../../operations/metrics.rst:576
#: ../../operations/metrics.rst:598 ../../operations/metrics.rst:614
#: ../../operations/metrics.rst:628 ../../operations/metrics.rst:642
msgid "Labels"
msgstr "Labels"

#: ../../operations/metrics.rst:194 ../../operations/metrics.rst:206
#: ../../operations/metrics.rst:215 ../../operations/metrics.rst:226
#: ../../operations/metrics.rst:236 ../../operations/metrics.rst:250
#: ../../operations/metrics.rst:263 ../../operations/metrics.rst:272
#: ../../operations/metrics.rst:290 ../../operations/metrics.rst:302
#: ../../operations/metrics.rst:317 ../../operations/metrics.rst:328
#: ../../operations/metrics.rst:337 ../../operations/metrics.rst:346
#: ../../operations/metrics.rst:356 ../../operations/metrics.rst:365
#: ../../operations/metrics.rst:376 ../../operations/metrics.rst:386
#: ../../operations/metrics.rst:397 ../../operations/metrics.rst:407
#: ../../operations/metrics.rst:418 ../../operations/metrics.rst:451
#: ../../operations/metrics.rst:491 ../../operations/metrics.rst:520
#: ../../operations/metrics.rst:531 ../../operations/metrics.rst:544
#: ../../operations/metrics.rst:558 ../../operations/metrics.rst:576
#: ../../operations/metrics.rst:585 ../../operations/metrics.rst:598
#: ../../operations/metrics.rst:614 ../../operations/metrics.rst:628
#: ../../operations/metrics.rst:642 ../../operations/performance/benchmark.rst:305
#: ../../operations/performance/benchmark.rst:323
#: ../../operations/performance/scalability/identity-relevant-labels.rst:25
#: ../../operations/performance/scalability/identity-relevant-labels.rst:50
#: ../../operations/system_requirements.rst:323
#: ../../operations/system_requirements.rst:343
#: ../../operations/system_requirements.rst:368
msgid "Description"
msgstr "描述"

#: ../../operations/metrics.rst:196
msgid "``endpoint``"
msgstr "``endpoint``"

#: ../../operations/metrics.rst:196
msgid "Number of endpoints managed by this agent"
msgstr "该agent管理的endpoint数量"

#: ../../operations/metrics.rst:197
msgid "``endpoint_regenerations_total``"
msgstr "``endpoint_regenerations_total``"

#: ../../operations/metrics.rst:197
msgid "``outcome``"
msgstr "``outcome``"

#: ../../operations/metrics.rst:197
msgid "Count of all endpoint regenerations that have completed"
msgstr "Count of all endpoint regenerations that have completed"

#: ../../operations/metrics.rst:198
msgid "``endpoint_regeneration_time_stats_seconds``"
msgstr "``endpoint_regeneration_time_stats_seconds``"

#: ../../operations/metrics.rst:198 ../../operations/metrics.rst:307
msgid "``scope``"
msgstr "``scope``"

#: ../../operations/metrics.rst:198
msgid "Endpoint regeneration time stats"
msgstr "Endpoint regeneration time stats"

#: ../../operations/metrics.rst:199
msgid "``endpoint_state``"
msgstr "``endpoint_state``"

#: ../../operations/metrics.rst:199
msgid "``state``"
msgstr "``state``"

#: ../../operations/metrics.rst:199
msgid "Count of all endpoints"
msgstr "为endpoint的总数"

#: ../../operations/metrics.rst:203
msgid "Services"
msgstr "服务"

#: ../../operations/metrics.rst:208
msgid "``services_events_total``"
msgstr "``services_events_total``"

#: ../../operations/metrics.rst:208
msgid "Number of services events labeled by action type"
msgstr "按操作类型标记的服务事件数"

#: ../../operations/metrics.rst:212
msgid "Cluster health"
msgstr "集群健康状况"

#: ../../operations/metrics.rst:217
msgid "``unreachable_nodes``"
msgstr "``unreachable_nodes``"

#: ../../operations/metrics.rst:217
msgid "Number of nodes that cannot be reached"
msgstr "无法访问的node数"

#: ../../operations/metrics.rst:218
msgid "``unreachable_health_endpoints``"
msgstr "``unreachable_health_endpoints``"

#: ../../operations/metrics.rst:218
msgid "Number of health endpoints that cannot be reached"
msgstr "无法访问的健康endpoint数"

#: ../../operations/metrics.rst:219
msgid "``controllers_failing``"
msgstr "``controllers_failing``"

#: ../../operations/metrics.rst:219
msgid "Number of failing controllers"
msgstr "故障的controller数量"

#: ../../operations/metrics.rst:223
msgid "Node Connectivity"
msgstr "节点连接"

#: ../../operations/metrics.rst:228
msgid "``node_connectivity_status``"
msgstr "``node_connectivity_status``"

#: ../../operations/metrics.rst:228
msgid ""
"``source_cluster``, ``source_node_name``, ``target_cluster``, "
"``target_node_name``, ``target_node_type``, ``type``"
msgstr ""
"``source_cluster``, ``source_node_name``, ``target_cluster``, "
"``target_node_name``, ``target_node_type``, ``type``"

#: ../../operations/metrics.rst:228
msgid ""
"The last observed status of both ICMP and HTTP connectivity between the current "
"Cilium agent and other Cilium nodes"
msgstr "当前 Cilium agent和其他 Cilium 节点之间最后观察到的 ICMP 和 HTTP 连接状态"

#: ../../operations/metrics.rst:229
msgid "``node_connectivity_latency_seconds``"
msgstr "``node_connectivity_latency_seconds``"

#: ../../operations/metrics.rst:229
msgid ""
"``address_type``, ``protocol``, ``source_cluster``, ``source_node_name``, "
"``target_cluster``, ``target_node_ip``, ``target_node_name``, "
"``target_node_type``, ``type``"
msgstr ""
"``address_type``, ``protocol``, ``source_cluster``, ``source_node_name``, "
"``target_cluster``, ``target_node_ip``, ``target_node_name``, "
"``target_node_type``, ``type``"

#: ../../operations/metrics.rst:229
msgid ""
"The last observed latency between the current Cilium agent and other Cilium "
"nodes in seconds"
msgstr "当前 Cilium agent和其他 Cilium 节点之间最后观察到的延迟（以秒为单位）"

#: ../../operations/metrics.rst:233
msgid "Clustermesh"
msgstr "Clustermesh"

#: ../../operations/metrics.rst:238
msgid "``clustermesh_global_services``"
msgstr "``clustermesh_global_services``"

#: ../../operations/metrics.rst:238 ../../operations/metrics.rst:239
msgid "``source_cluster``, ``source_node_name``"
msgstr "``source_cluster``, ``source_node_name``"

#: ../../operations/metrics.rst:238
msgid "The total number of global services in the cluster mesh"
msgstr "集群网格中的全局服务总数"

#: ../../operations/metrics.rst:239
msgid "``clustermesh_remote_clusters``"
msgstr "``clustermesh_remote_clusters``"

#: ../../operations/metrics.rst:239
msgid "The total number of remote clusters meshed with the local cluster"
msgstr "与本地集群进行mesh的远程集群总数"

#: ../../operations/metrics.rst:240
msgid "``clustermesh_remote_cluster_failures``"
msgstr "``clustermesh_remote_cluster_failures``"

#: ../../operations/metrics.rst:240 ../../operations/metrics.rst:241
#: ../../operations/metrics.rst:242 ../../operations/metrics.rst:243
msgid "``source_cluster``, ``source_node_name``, ``target_cluster``"
msgstr "``source_cluster``, ``source_node_name``, ``target_cluster``"

#: ../../operations/metrics.rst:240
msgid "The total number of failures related to the remote cluster"
msgstr "与远程集群相关的故障总数"

#: ../../operations/metrics.rst:241
msgid "``clustermesh_remote_cluster_nodes``"
msgstr "``clustermesh_remote_cluster_nodes``"

#: ../../operations/metrics.rst:241
msgid "The total number of nodes in the remote cluster"
msgstr "远程集群中的节点总数"

#: ../../operations/metrics.rst:242
msgid "``clustermesh_remote_cluster_last_failure_ts``"
msgstr "``clustermesh_remote_cluster_last_failure_ts``"

#: ../../operations/metrics.rst:242
msgid "The timestamp of the last failure of the remote cluster"
msgstr "远程集群上一次故障的时间戳"

#: ../../operations/metrics.rst:243
msgid "``clustermesh_remote_cluster_readiness_status``"
msgstr "``clustermesh_remote_cluster_readiness_status``"

#: ../../operations/metrics.rst:243
msgid "The readiness status of the remote cluster"
msgstr "远程集群的就绪状态"

#: ../../operations/metrics.rst:247
msgid "Datapath"
msgstr "Datapath"

#: ../../operations/metrics.rst:252
msgid "``datapath_conntrack_dump_resets_total``"
msgstr "``datapath_conntrack_dump_resets_total``"

#: ../../operations/metrics.rst:252
msgid "``area``, ``name``, ``family``"
msgstr "``area``, ``name``, ``family``"

#: ../../operations/metrics.rst:252
msgid ""
"Number of conntrack dump resets. Happens when a BPF entry gets removed while "
"dumping the map is in progress."
msgstr "为conntrack 转储重置次数，一般会发生在转储地图的过程中删除 BPF 条目时。"

#: ../../operations/metrics.rst:253
msgid "``datapath_conntrack_gc_runs_total``"
msgstr "``datapath_conntrack_gc_runs_total``"

#: ../../operations/metrics.rst:253 ../../operations/metrics.rst:256
#: ../../operations/metrics.rst:348 ../../operations/metrics.rst:349
msgid "``status``"
msgstr "``status``"

#: ../../operations/metrics.rst:253
msgid "Number of times that the conntrack garbage collector process was run"
msgstr "指 conntrack 垃圾回收进程运行的次数"

#: ../../operations/metrics.rst:254
msgid "``datapath_conntrack_gc_key_fallbacks_total``"
msgstr "``datapath_conntrack_gc_key_fallbacks_total``"

#: ../../operations/metrics.rst:254
msgid ""
"The number of alive and deleted conntrack entries at the end of a garbage "
"collector run labeled by datapath family"
msgstr "垃圾回收运行结束时活动和已删除的带有datapath集标记的 conntrack 条目数"

#: ../../operations/metrics.rst:255
msgid "``datapath_conntrack_gc_entries``"
msgstr "``datapath_conntrack_gc_entries``"

#: ../../operations/metrics.rst:255 ../../operations/metrics.rst:379
msgid "``family``"
msgstr "``family``"

#: ../../operations/metrics.rst:255
msgid ""
"The number of alive and deleted conntrack entries at the end of a garbage "
"collector run"
msgstr "垃圾回收运行结束时活动和已删除的 conntrack 条目数"

#: ../../operations/metrics.rst:256
msgid "``datapath_conntrack_gc_duration_seconds``"
msgstr "``datapath_conntrack_gc_duration_seconds``"

#: ../../operations/metrics.rst:256
msgid "Duration in seconds of the garbage collector process"
msgstr "垃圾回收进程的持续时间（以秒为单位）"

#: ../../operations/metrics.rst:260
msgid "IPSec"
msgstr "IPSec"

#: ../../operations/metrics.rst:265
msgid "``ipsec_xfrm_error``"
msgstr "``ipsec_xfrm_error``"

#: ../../operations/metrics.rst:265
msgid "``error``, ``type``"
msgstr "``error``, ``type``"

#: ../../operations/metrics.rst:265
msgid "Total number of xfrm errors."
msgstr "为 xfrm 错误总数。"

#: ../../operations/metrics.rst:269
msgid "eBPF"
msgstr "eBPF"

#: ../../operations/metrics.rst:274
msgid "``bpf_syscall_duration_seconds``"
msgstr "``bpf_syscall_duration_seconds``"

#: ../../operations/metrics.rst:274
msgid "``operation``, ``outcome``"
msgstr "``operation``, ``outcome``"

#: ../../operations/metrics.rst:274
msgid "Duration of eBPF system call performed"
msgstr "执行 eBPF 系统调用的持续时间"

#: ../../operations/metrics.rst:275
msgid "``bpf_map_ops_total``"
msgstr "``bpf_map_ops_total``"

#: ../../operations/metrics.rst:275
msgid "``mapName`` (deprecated), ``map_name``, ``operation``, ``outcome``"
msgstr "``mapName`` (deprecated), ``map_name``, ``operation``, ``outcome``"

#: ../../operations/metrics.rst:275
msgid ""
"Number of eBPF map operations performed. ``mapName`` is deprecated and will be "
"removed in 1.10. Use ``map_name`` instead."
msgstr ""
"执行的 eBPF 映射操作数。 ``mapName`` 已弃用，将在 1.10 中删除。 改用"
"``map_name``。"

#: ../../operations/metrics.rst:276
msgid "``bpf_map_pressure``"
msgstr "``bpf_map_pressure``"

#: ../../operations/metrics.rst:276
msgid "``map_name``"
msgstr "``map_name``"

#: ../../operations/metrics.rst:276
msgid ""
"Map pressure defined as fill-up ratio of the map. Policy maps are exceptionally "
"reported only when ratio is over 0.1."
msgstr "Map pressure定义为map的填充率，仅当比率超过 0.1 时才会报策略map异常。"

#: ../../operations/metrics.rst:277
msgid "``bpf_maps_virtual_memory_max_bytes``"
msgstr "``bpf_maps_virtual_memory_max_bytes``"

#: ../../operations/metrics.rst:277
msgid "Max memory used by eBPF maps installed in the system"
msgstr "系统中安装的 eBPF 映射使用的最大内存"

#: ../../operations/metrics.rst:278
msgid "``bpf_progs_virtual_memory_max_bytes``"
msgstr "``bpf_progs_virtual_memory_max_bytes``"

#: ../../operations/metrics.rst:278
msgid "Max memory used by eBPF programs installed in the system"
msgstr "系统中安装的 eBPF 程序使用的最大内存"

#: ../../operations/metrics.rst:281
msgid ""
"Both ``bpf_maps_virtual_memory_max_bytes`` and "
"``bpf_progs_virtual_memory_max_bytes`` are currently reporting the system-wide "
"memory usage of eBPF that is directly and not directly managed by Cilium. This "
"might change in the future and only report the eBPF memory usage directly "
"managed by Cilium."
msgstr ""
"目前 bpf_maps_virtual_memory_max_bytes`` 和 "
"``bpf_progs_virtual_memory_max_bytes`` 都会上报 eBPF 的系统内存使用情况，它直接"
"由 Cilium 管理，而不是直接由 Cilium 管理。 之后可能会改为只报告由 Cilium 直接管"
"理的 eBPF 内存使用情况。"

#: ../../operations/metrics.rst:287
msgid "Drops/Forwards (L3/L4)"
msgstr "Drops/Forwards (L3/L4)"

#: ../../operations/metrics.rst:292
msgid "``drop_count_total``"
msgstr "``drop_count_total``"

#: ../../operations/metrics.rst:292 ../../operations/metrics.rst:293
msgid "``reason``, ``direction``"
msgstr "``reason``, ``direction``"

#: ../../operations/metrics.rst:292
msgid "Total dropped packets"
msgstr "总丢包数"

#: ../../operations/metrics.rst:293
msgid "``drop_bytes_total``"
msgstr "``drop_bytes_total``"

#: ../../operations/metrics.rst:293
msgid "Total dropped bytes"
msgstr "丢弃的字节总数"

#: ../../operations/metrics.rst:294
msgid "``forward_count_total``"
msgstr "``forward_count_total``"

#: ../../operations/metrics.rst:294 ../../operations/metrics.rst:295
msgid "``direction``"
msgstr "``direction``"

#: ../../operations/metrics.rst:294
msgid "Total forwarded packets"
msgstr "转发数据包总数"

#: ../../operations/metrics.rst:295
msgid "``forward_bytes_total``"
msgstr "``forward_bytes_total``"

#: ../../operations/metrics.rst:295
msgid "Total forwarded bytes"
msgstr "总转发字节数"

#: ../../operations/metrics.rst:299
msgid "Policy"
msgstr "策略"

#: ../../operations/metrics.rst:304
msgid "``policy``"
msgstr "``policy``"

#: ../../operations/metrics.rst:304
msgid "Number of policies currently loaded"
msgstr "当前加载的策略数"

#: ../../operations/metrics.rst:305
msgid "``policy_count``"
msgstr "``policy_count``"

#: ../../operations/metrics.rst:305
msgid "Number of policies currently loaded (deprecated, use ``policy``)"
msgstr "当前加载的策略数（已弃用，使用``policy``）"

#: ../../operations/metrics.rst:306
msgid "``policy_regeneration_total``"
msgstr "``policy_regeneration_total``"

#: ../../operations/metrics.rst:306
msgid "Total number of policies regenerated successfully"
msgstr "成功重新生成的策略总数"

#: ../../operations/metrics.rst:307
msgid "``policy_regeneration_time_stats_seconds``"
msgstr "``policy_regeneration_time_stats_seconds``"

#: ../../operations/metrics.rst:307
msgid "Policy regeneration time stats labeled by the scope"
msgstr "带scope标签的策略重新生成时间统计信息"

#: ../../operations/metrics.rst:308
msgid "``policy_max_revision``"
msgstr "``policy_max_revision``"

#: ../../operations/metrics.rst:308
msgid "Highest policy revision number in the agent"
msgstr "为agent中的最新的策略revision号"

#: ../../operations/metrics.rst:309
msgid "``policy_import_errors_total``"
msgstr "``policy_import_errors_total``"

#: ../../operations/metrics.rst:309
msgid "Number of times a policy import has failed"
msgstr "策略导入失败的次数"

#: ../../operations/metrics.rst:310
msgid "``policy_endpoint_enforcement_status``"
msgstr "``policy_endpoint_enforcement_status``"

#: ../../operations/metrics.rst:310
msgid "Number of endpoints labeled by policy enforcement status"
msgstr "按策略执行状态标记的endpoint数"

#: ../../operations/metrics.rst:314
msgid "Policy L7 (HTTP/Kafka)"
msgstr "L7策略 (HTTP/Kafka)"

#: ../../operations/metrics.rst:319
msgid "``proxy_redirects``"
msgstr "``proxy_redirects``"

#: ../../operations/metrics.rst:319
msgid "``protocol``"
msgstr "``protocol``"

#: ../../operations/metrics.rst:319
msgid "Number of redirects installed for endpoints"
msgstr "为endpoint配置的重定向数"

#: ../../operations/metrics.rst:320
msgid "``proxy_upstream_reply_seconds``"
msgstr "``proxy_upstream_reply_seconds``"

#: ../../operations/metrics.rst:320
msgid "Seconds waited for upstream server to reply to a request"
msgstr "等待上游服务器回复请求的秒数"

#: ../../operations/metrics.rst:321
msgid "``policy_l7_total``"
msgstr "``policy_l7_total``"

#: ../../operations/metrics.rst:321 ../../operations/metrics.rst:453
msgid "``type``"
msgstr "``type``"

#: ../../operations/metrics.rst:321
msgid "Number of total L7 requests/responses"
msgstr "为 L7 请求/响应总数"

#: ../../operations/metrics.rst:325
msgid "Identity"
msgstr "Identity"

#: ../../operations/metrics.rst:330 ../../operations/metrics.rst:493
msgid "``identity``"
msgstr "``identity``"

#: ../../operations/metrics.rst:330
msgid "Number of identities currently allocated"
msgstr "当前分配的Identity数量"

#: ../../operations/metrics.rst:334
msgid "Events external to Cilium"
msgstr "Cilium 之外的事件"

#: ../../operations/metrics.rst:339
msgid "``event_ts``"
msgstr "``event_ts``"

#: ../../operations/metrics.rst:339
msgid "``source``"
msgstr "``source``"

#: ../../operations/metrics.rst:339
msgid "Last timestamp when we received an event"
msgstr "最后收到事件的时间戳"

#: ../../operations/metrics.rst:343
msgid "Controllers"
msgstr "Controllers"

#: ../../operations/metrics.rst:348
msgid "``controllers_runs_total``"
msgstr "``controllers_runs_total``"

#: ../../operations/metrics.rst:348
msgid "Number of times that a controller process was run"
msgstr "指controller进程运行的次数"

#: ../../operations/metrics.rst:349
msgid "``controllers_runs_duration_seconds``"
msgstr "``controllers_runs_duration_seconds``"

#: ../../operations/metrics.rst:349
msgid "Duration in seconds of the controller process"
msgstr "为 controller进程的持续时间（以秒为单位）"

#: ../../operations/metrics.rst:353
msgid "SubProcess"
msgstr "SubProcess"

#: ../../operations/metrics.rst:358
msgid "``subprocess_start_total``"
msgstr "``subprocess_start_total``"

#: ../../operations/metrics.rst:358
msgid "``subsystem``"
msgstr "``subsystem``"

#: ../../operations/metrics.rst:358
msgid "Number of times that Cilium has started a subprocess"
msgstr "Cilium 启动子进程的次数"

#: ../../operations/metrics.rst:362 ../../operations/troubleshooting.rst:29
msgid "Kubernetes"
msgstr "Kubernetes"

#: ../../operations/metrics.rst:367
msgid "``kubernetes_events_received_total``"
msgstr "``kubernetes_events_received_total``"

#: ../../operations/metrics.rst:367
msgid "``scope``, ``action``, ``validity``, ``equal``"
msgstr "``scope``, ``action``, ``validity``, ``equal``"

#: ../../operations/metrics.rst:367
msgid "Number of Kubernetes events received"
msgstr "收到的 Kubernetes 事件数"

#: ../../operations/metrics.rst:368
msgid "``kubernetes_events_total``"
msgstr "``kubernetes_events_total``"

#: ../../operations/metrics.rst:368
msgid "``scope``, ``action``, ``outcome``"
msgstr "``scope``, ``action``, ``outcome``"

#: ../../operations/metrics.rst:368
msgid "Number of Kubernetes events processed"
msgstr "处理的 Kubernetes 事件数"

#: ../../operations/metrics.rst:369
msgid "``k8s_cnp_status_completion_seconds``"
msgstr "``k8s_cnp_status_completion_seconds``"

#: ../../operations/metrics.rst:369
msgid "``attempts``, ``outcome``"
msgstr "``attempts``, ``outcome``"

#: ../../operations/metrics.rst:369
msgid "Duration in seconds in how long it took to complete a CNP status update"
msgstr "完成 CNP 状态更新所需的持续时间（以秒为单位）"

#: ../../operations/metrics.rst:373 ../../operations/metrics.rst:448
msgid "IPAM"
msgstr "IPAM"

#: ../../operations/metrics.rst:378
msgid "``ipam_events_total``"
msgstr "``ipam_events_total``"

#: ../../operations/metrics.rst:378
msgid "Number of IPAM events received labeled by action and datapath family type"
msgstr "接收到的带action和数据路径类型标记的 IPAM 事件数"

#: ../../operations/metrics.rst:379
msgid "``ip_addresses``"
msgstr "``ip_addresses``"

#: ../../operations/metrics.rst:379
msgid "Number of allocated IP addresses"
msgstr "分配的 IP 地址数"

#: ../../operations/metrics.rst:383
msgid "KVstore"
msgstr "KVstore"

#: ../../operations/metrics.rst:388
msgid "``kvstore_operations_duration_seconds``"
msgstr "``kvstore_operations_duration_seconds``"

#: ../../operations/metrics.rst:388
msgid "``action``, ``kind``, ``outcome``, ``scope``"
msgstr "``action``, ``kind``, ``outcome``, ``scope``"

#: ../../operations/metrics.rst:388
msgid "Duration of kvstore operation"
msgstr "为 kvstore 操作的持续时间"

#: ../../operations/metrics.rst:389
msgid "``kvstore_events_queue_seconds``"
msgstr "``kvstore_events_queue_seconds``"

#: ../../operations/metrics.rst:389
msgid "``action``, ``scope``"
msgstr "``action``, ``scope``"

#: ../../operations/metrics.rst:389
msgid ""
"Duration of seconds of time received event was blocked before it could be queued"
msgstr "接收到的事件在排队之前被blocked的持续时间"

#: ../../operations/metrics.rst:390
msgid "``kvstore_quorum_errors_total``"
msgstr "``kvstore_quorum_errors_total``"

#: ../../operations/metrics.rst:390
msgid "``error``"
msgstr "``error``"

#: ../../operations/metrics.rst:390
msgid "Number of quorum errors"
msgstr "集群错误数"

#: ../../operations/metrics.rst:394
msgid "Agent"
msgstr "Agent"

#: ../../operations/metrics.rst:399
msgid "``agent_bootstrap_seconds``"
msgstr "``agent_bootstrap_seconds``"

#: ../../operations/metrics.rst:399
msgid "``scope``, ``outcome``"
msgstr "``scope``, ``outcome``"

#: ../../operations/metrics.rst:399
msgid "Duration of various bootstrap phases"
msgstr "各种引导阶段的持续时间"

#: ../../operations/metrics.rst:400
msgid "``api_process_time_seconds``"
msgstr "``api_process_time_seconds``"

#: ../../operations/metrics.rst:400
msgid ""
"Processing time of all the API calls made to the cilium-agent, labeled by API "
"method, API path and returned HTTP code."
msgstr ""
"由 API 方法、API 路径和返回的 HTTP 代码标记的，由 cilium-agent 进行的所有 API 调"
"用的处理时间。"

#: ../../operations/metrics.rst:404
msgid "FQDN"
msgstr "FQDN"

#: ../../operations/metrics.rst:409
msgid "``qdn_gc_deletions_total``"
msgstr "``qdn_gc_deletions_total``"

#: ../../operations/metrics.rst:409
msgid "Number of FQDNs that have been cleaned on FQDN garbage collector job"
msgstr "已经由 FQDN 垃圾回收作业清理的 FQDN 数量"

#: ../../operations/metrics.rst:415
msgid "API Rate Limiting"
msgstr "API 速率限制"

#: ../../operations/metrics.rst:420
msgid "``cilium_api_limiter_adjustment_factor``"
msgstr "``cilium_api_limiter_adjustment_factor``"

#: ../../operations/metrics.rst:420 ../../operations/metrics.rst:426
msgid "``api_call``"
msgstr "``api_call``"

#: ../../operations/metrics.rst:420
msgid "Most recent adjustment factor for automatic adjustment"
msgstr "自动调整的最新调整系数"

#: ../../operations/metrics.rst:421
msgid "``cilium_api_limiter_processed_requests_total``"
msgstr "``cilium_api_limiter_processed_requests_total``"

#: ../../operations/metrics.rst:421
msgid "``api_call``, ``outcome``"
msgstr "``api_call``, ``outcome``"

#: ../../operations/metrics.rst:421
msgid "Total number of API requests processed"
msgstr "处理的 API 请求总数"

#: ../../operations/metrics.rst:422
msgid "``cilium_api_limiter_processing_duration_seconds``"
msgstr "``cilium_api_limiter_processing_duration_seconds``"

#: ../../operations/metrics.rst:422 ../../operations/metrics.rst:423
#: ../../operations/metrics.rst:425
msgid "``api_call``, ``value``"
msgstr "``api_call``, ``value``"

#: ../../operations/metrics.rst:422
msgid "Mean and estimated processing duration in seconds"
msgstr "平均和估计处理持续时间，以秒为单位"

#: ../../operations/metrics.rst:423
msgid "``cilium_api_limiter_rate_limit``"
msgstr "``cilium_api_limiter_rate_limit``"

#: ../../operations/metrics.rst:423
msgid "Current rate limiting configuration (limit and burst)"
msgstr "限流配置（限流和突发）"

#: ../../operations/metrics.rst:424
msgid "``cilium_api_limiter_requests_in_flight``"
msgstr "``cilium_api_limiter_requests_in_flight``"

#: ../../operations/metrics.rst:424
msgid "``api_call``  ``value``"
msgstr "``api_call``  ``value``"

#: ../../operations/metrics.rst:424
msgid "Current and maximum allowed number of requests in flight"
msgstr "当前和最大允许的请求数"

#: ../../operations/metrics.rst:425
msgid "``cilium_api_limiter_wait_duration_seconds``"
msgstr "``cilium_api_limiter_wait_duration_seconds``"

#: ../../operations/metrics.rst:425
msgid "Mean, min, and max wait duration"
msgstr "平均、最小和最大等待时间"

#: ../../operations/metrics.rst:426
msgid "``cilium_api_limiter_wait_history_duration_seconds``"
msgstr "``cilium_api_limiter_wait_history_duration_seconds``"

#: ../../operations/metrics.rst:426
msgid "Histogram of wait duration per API call processed"
msgstr "处理每个 API 调用所需等待时间的直方图"

#: ../../operations/metrics.rst:430
msgid "cilium-operator"
msgstr ""

#: ../../operations/metrics.rst:435
msgid ""
"``cilium-operator`` can be configured to serve metrics by running with the "
"option ``--enable-metrics``.  By default, the operator will expose metrics on "
"port 6942, the port can be changed with the option ``--operator-prometheus-"
"serve-addr``."
msgstr ""

#: ../../operations/metrics.rst:443
msgid ""
"All metrics are exported under the ``cilium_operator_`` Prometheus namespace."
msgstr ""

#: ../../operations/metrics.rst:453
msgid "``ipam_ips``"
msgstr ""

#: ../../operations/metrics.rst:453
msgid "Number of IPs allocated"
msgstr ""

#: ../../operations/metrics.rst:454
msgid "``ipam_allocation_ops``"
msgstr ""

#: ../../operations/metrics.rst:454
msgid "``subnet_id``"
msgstr ""

#: ../../operations/metrics.rst:454
msgid "Number of IP allocation operations."
msgstr ""

#: ../../operations/metrics.rst:455
msgid "``ipam_interface_creation_ops``"
msgstr ""

#: ../../operations/metrics.rst:455
msgid "``subnet_id``, ``status``"
msgstr ""

#: ../../operations/metrics.rst:455
msgid "Number of interfaces creation operations."
msgstr ""

#: ../../operations/metrics.rst:456
msgid "``ipam_available``"
msgstr ""

#: ../../operations/metrics.rst:456
msgid "Number of interfaces with addresses available"
msgstr ""

#: ../../operations/metrics.rst:457
msgid "``ipam_nodes_at_capacity``"
msgstr ""

#: ../../operations/metrics.rst:457
msgid "Number of nodes unable to allocate more addresses"
msgstr ""

#: ../../operations/metrics.rst:458
msgid "``ipam_resync_total``"
msgstr ""

#: ../../operations/metrics.rst:458
msgid "Number of synchronization operations with external IPAM API"
msgstr ""

#: ../../operations/metrics.rst:459
msgid "``ipam_api_duration_seconds``"
msgstr ""

#: ../../operations/metrics.rst:459
msgid "``operation``, ``response_code``"
msgstr ""

#: ../../operations/metrics.rst:459
msgid "Duration of interactions with external IPAM API."
msgstr ""

#: ../../operations/metrics.rst:460
msgid "``ipam_api_rate_limit_duration_seconds``"
msgstr ""

#: ../../operations/metrics.rst:460
msgid "``operation``"
msgstr ""

#: ../../operations/metrics.rst:460
msgid "Duration of rate limiting while accessing external IPAM API"
msgstr ""

#: ../../operations/metrics.rst:464 ../../operations/performance/tuning.rst:76
msgid "Hubble"
msgstr ""

#: ../../operations/metrics.rst:469
msgid ""
"Hubble metrics are served by a Hubble instance running inside ``cilium-agent``. "
"The command-line options to configure them are ``--enable-hubble``, ``--hubble-"
"metrics-server``, and ``--hubble-metrics``. ``--hubble-metrics-server`` takes "
"an ``IP:Port`` pair, but passing an empty IP (e.g. ``:9091``) will bind the "
"server to all available interfaces. ``--hubble-metrics`` takes a comma-"
"separated list of metrics."
msgstr ""

#: ../../operations/metrics.rst:476
msgid ""
"Some metrics can take additional semicolon-separated options per metric, e.g. "
"``--hubble-metrics=\"dns:query;ignoreAAAA,http:destinationContext=pod-short\"`` "
"will enable the ``dns`` metric with the ``query`` and ``ignoreAAAA`` options, "
"and the ``http`` metric with the ``destinationContext=pod-short`` option."
msgstr ""

#: ../../operations/metrics.rst:484
msgid "Context Options"
msgstr ""

#: ../../operations/metrics.rst:486
msgid ""
"Most Hubble metrics can be configured to add the source and/or destination "
"context as a label. The options are called ``sourceContext`` and "
"``destinationContext``. The possible values are:"
msgstr ""

#: ../../operations/metrics.rst:491 ../../operations/metrics.rst:531
#: ../../operations/metrics.rst:585
msgid "Option Value"
msgstr ""

#: ../../operations/metrics.rst:493
msgid "All Cilium security identity labels"
msgstr ""

#: ../../operations/metrics.rst:494
msgid "``namespace``"
msgstr ""

#: ../../operations/metrics.rst:494
msgid "Kubernetes namespace name"
msgstr ""

#: ../../operations/metrics.rst:495
msgid "``pod``"
msgstr ""

#: ../../operations/metrics.rst:495
msgid "Kubernetes pod name"
msgstr ""

#: ../../operations/metrics.rst:496
msgid "``pod-short``"
msgstr ""

#: ../../operations/metrics.rst:496
msgid ""
"Short version of the Kubernetes pod name. Typically the deployment/replicaset "
"name."
msgstr ""

#: ../../operations/metrics.rst:497 ../../operations/metrics.rst:517
msgid "``dns``"
msgstr ""

#: ../../operations/metrics.rst:497
msgid "All known DNS names of the source or destination (comma-separated)"
msgstr ""

#: ../../operations/metrics.rst:498
msgid "``ip``"
msgstr ""

#: ../../operations/metrics.rst:498
msgid "The IPv4 or IPv6 address"
msgstr ""

#: ../../operations/metrics.rst:501
msgid ""
"When specifying the source and/or destination context, multiple contexts can be "
"specified by separating them via the ``|`` symbol. When multiple are specified, "
"then the first non-empty value is added to the metric as a label. For example, "
"a metric configuration of ``flow:destinationContext=dns|ip`` will first try to "
"use the DNS name of the target for the label. If no DNS name is known for the "
"target, it will fall back and use the IP address of the target instead."
msgstr ""

#: ../../operations/metrics.rst:514
msgid "Hubble metrics are exported under the ``hubble_`` Prometheus namespace."
msgstr ""

#: ../../operations/metrics.rst:522
msgid "``dns_queries_total``"
msgstr ""

#: ../../operations/metrics.rst:522 ../../operations/metrics.rst:523
msgid "``rcode``, ``qtypes``, ``ips_returned``"
msgstr ""

#: ../../operations/metrics.rst:522
msgid "Number of DNS queries observed"
msgstr ""

#: ../../operations/metrics.rst:523
msgid "``dns_responses_total``"
msgstr ""

#: ../../operations/metrics.rst:523
msgid "Number of DNS responses observed"
msgstr ""

#: ../../operations/metrics.rst:524
msgid "``dns_response_types_total``"
msgstr ""

#: ../../operations/metrics.rst:524
msgid "``type``, ``qtypes``"
msgstr ""

#: ../../operations/metrics.rst:524
msgid "Number of DNS response types"
msgstr ""

#: ../../operations/metrics.rst:528 ../../operations/metrics.rst:550
#: ../../operations/metrics.rst:564 ../../operations/metrics.rst:582
#: ../../operations/metrics.rst:606 ../../operations/metrics.rst:620
#: ../../operations/metrics.rst:634 ../../operations/metrics.rst:648
msgid "Options"
msgstr ""

#: ../../operations/metrics.rst:531 ../../operations/metrics.rst:585
msgid "Option Key"
msgstr ""

#: ../../operations/metrics.rst:533
msgid "``query``"
msgstr ""

#: ../../operations/metrics.rst:533 ../../operations/metrics.rst:534
#: ../../operations/metrics.rst:587 ../../operations/metrics.rst:588
msgid "N/A"
msgstr ""

#: ../../operations/metrics.rst:533
msgid "Include the query as label \"query\""
msgstr ""

#: ../../operations/metrics.rst:534
msgid "``ignoreAAAA``"
msgstr ""

#: ../../operations/metrics.rst:534
msgid "Ignore any AAAA requests/responses"
msgstr ""

#: ../../operations/metrics.rst:537 ../../operations/metrics.rst:552
#: ../../operations/metrics.rst:566 ../../operations/metrics.rst:592
#: ../../operations/metrics.rst:608 ../../operations/metrics.rst:622
#: ../../operations/metrics.rst:636 ../../operations/metrics.rst:650
msgid "This metric supports :ref:`Context Options<hubble_context_options>`."
msgstr ""

#: ../../operations/metrics.rst:541
msgid "``drop``"
msgstr ""

#: ../../operations/metrics.rst:546
msgid "``drop_total``"
msgstr ""

#: ../../operations/metrics.rst:546
msgid "``reason``, ``protocol``"
msgstr ""

#: ../../operations/metrics.rst:546
msgid "Number of drops"
msgstr ""

#: ../../operations/metrics.rst:555
msgid "``flow``"
msgstr ""

#: ../../operations/metrics.rst:560
msgid "``flows_processed_total``"
msgstr ""

#: ../../operations/metrics.rst:560
msgid "``type``, ``subtype``, ``verdict``"
msgstr ""

#: ../../operations/metrics.rst:560
msgid "Total number of flows processed"
msgstr ""

#: ../../operations/metrics.rst:569
msgid "``flows-to-world``"
msgstr ""

#: ../../operations/metrics.rst:571
msgid ""
"This metric counts all non-reply flows containing the ``reserved:world`` label "
"in their destination identity. By default, dropped flows are counted if and "
"only if the drop reason is ``Policy denied``. Set ``any-drop`` option to count "
"all dropped flows."
msgstr ""

#: ../../operations/metrics.rst:578
msgid "``flows_to_world_total``"
msgstr ""

#: ../../operations/metrics.rst:578
msgid "``protocol``, ``verdict``"
msgstr ""

#: ../../operations/metrics.rst:578
msgid "Total number of flows to ``reserved:world``."
msgstr ""

#: ../../operations/metrics.rst:587
msgid "``any-drop``"
msgstr ""

#: ../../operations/metrics.rst:587
msgid "Count any dropped flows regardless of the drop reason."
msgstr ""

#: ../../operations/metrics.rst:588
msgid "``port``"
msgstr ""

#: ../../operations/metrics.rst:588
msgid "Include the destination port as label ``port``."
msgstr ""

#: ../../operations/metrics.rst:595
msgid "``http``"
msgstr ""

#: ../../operations/metrics.rst:600
msgid "``http_requests_total``"
msgstr ""

#: ../../operations/metrics.rst:600
msgid "``method``, ``protocol``"
msgstr ""

#: ../../operations/metrics.rst:600
msgid "Count of HTTP requests"
msgstr ""

#: ../../operations/metrics.rst:601
msgid "``http_responses_total``"
msgstr ""

#: ../../operations/metrics.rst:601
msgid "``method``, ``status``"
msgstr ""

#: ../../operations/metrics.rst:601
msgid "Count of HTTP responses"
msgstr ""

#: ../../operations/metrics.rst:602
msgid "``http_request_duration_seconds``"
msgstr ""

#: ../../operations/metrics.rst:602
msgid "``method``"
msgstr ""

#: ../../operations/metrics.rst:602
msgid "Quantiles of HTTP request duration in seconds"
msgstr ""

#: ../../operations/metrics.rst:611
msgid "``icmp``"
msgstr ""

#: ../../operations/metrics.rst:616
msgid "``icmp_total``"
msgstr ""

#: ../../operations/metrics.rst:616
msgid "``family``, ``type``"
msgstr ""

#: ../../operations/metrics.rst:616
msgid "Number of ICMP messages"
msgstr ""

#: ../../operations/metrics.rst:625
msgid "``port-distribution``"
msgstr ""

#: ../../operations/metrics.rst:630
msgid "``port_distribution_total``"
msgstr ""

#: ../../operations/metrics.rst:630
msgid "``protocol``, ``port``"
msgstr ""

#: ../../operations/metrics.rst:630
msgid "Numbers of packets distributed by destination port"
msgstr ""

#: ../../operations/metrics.rst:639
msgid "``tcp``"
msgstr ""

#: ../../operations/metrics.rst:644
msgid "``tcp_flags_total``"
msgstr ""

#: ../../operations/metrics.rst:644
msgid "``flag``, ``family``"
msgstr ""

#: ../../operations/metrics.rst:644
msgid "TCP flag occurrences"
msgstr ""

#: ../../operations/performance/benchmark.rst:11
msgid "CNI Performance Benchmark"
msgstr ""

#: ../../operations/performance/benchmark.rst:14
#: ../../operations/troubleshooting.rst:526
msgid "Introduction"
msgstr ""

#: ../../operations/performance/benchmark.rst:16
msgid ""
"This chapter contains performance benchmark numbers for a variety of scenarios. "
"All tests are performed between containers running on two different bare metal "
"nodes connected back-to-back by a 100Gbit/s network interface. Upon popular "
"request we have included performance numbers for Calico for comparison."
msgstr ""

#: ../../operations/performance/benchmark.rst:23
msgid ""
"To achieve these performance results, follow the :ref:`performance_tuning`."
msgstr ""

#: ../../operations/performance/benchmark.rst:25
msgid ""
"For more information on the used system and configuration, see :ref:"
"`test_hardware`. For more details on all tested configurations, see :ref:"
"`test_configurations`."
msgstr ""

#: ../../operations/performance/benchmark.rst:29
msgid ""
"The following metrics are collected and reported. Each metric represents a "
"different traffic pattern that can be required for workloads. See the specific "
"sections for an explanation on what type of workloads are represented by each "
"benchmark."
msgstr ""

#: ../../operations/performance/benchmark.rst:36
msgid "Throughput"
msgstr ""

#: ../../operations/performance/benchmark.rst:35
msgid ""
"Maximum transfer rate via a single TCP connection and the total transfer rate "
"of 32 accumulated connections."
msgstr ""

#: ../../operations/performance/benchmark.rst:40
msgid "Request/Response Rate"
msgstr ""

#: ../../operations/performance/benchmark.rst:39
msgid ""
"The number of request/response messages per second that can be transmitted over "
"a single TCP connection and over 32 parallel TCP connections."
msgstr ""

#: ../../operations/performance/benchmark.rst:45
msgid "Connections Rate"
msgstr ""

#: ../../operations/performance/benchmark.rst:43
msgid ""
"The number of connections per second that can be established in sequence with a "
"single request/response payload message transmitted for each new connection. A "
"single process and 32 parallel processes are tested."
msgstr ""

#: ../../operations/performance/benchmark.rst:47
msgid ""
"For the various benchmarks `netperf <https://github.com/HewlettPackard/"
"netperf>`_ has been used to generate the workloads and to collect the metrics. "
"For spawning parallel netperf sessions, `super_netperf <https://raw."
"githubusercontent.com/borkmann/netperf_scripts/master/super_netperf>`_ has been "
"used. Both netperf and super_netperf are also frequently used and well "
"established tools for benchmarking in the Linux kernel networking community."
msgstr ""

#: ../../operations/performance/benchmark.rst:56
msgid "TCP Throughput (TCP_STREAM)"
msgstr ""

#: ../../operations/performance/benchmark.rst:58
msgid ""
"Throughput testing (TCP_STREAM) is useful to understand the maximum throughput "
"that can be achieved with a particular configuration. All or most "
"configurations can achieve line-rate or close to line-rate if enough CPU "
"resources are thrown at the load. It is therefore important to understand the "
"amount of CPU resources required to achieve a certain throughput as these CPU "
"resources will no longer be available to workloads running on the machine."
msgstr ""

#: ../../operations/performance/benchmark.rst:65
msgid ""
"This test represents bulk data transfer workloads, e.g. streaming services or "
"services performing data upload/download."
msgstr ""

#: ../../operations/performance/benchmark.rst:69
msgid "Single-Stream"
msgstr ""

#: ../../operations/performance/benchmark.rst:71
msgid ""
"In this test, a single TCP stream is opened between the containers and maximum "
"throughput is achieved:"
msgstr ""

#: ../../operations/performance/benchmark.rst:76
msgid ""
"We can see that eBPF-based solutions can outperform even the node-to-node "
"baseline on modern kernels despite performing additional work (forwarding into "
"the network namespace of the container, policy enforcement, ...). This is "
"because eBPF is capable of bypassing the iptables layer of the node which is "
"still traversed for the node to node baseline."
msgstr ""

#: ../../operations/performance/benchmark.rst:82
msgid ""
"The following graph shows the total CPU consumption across the entire system "
"while running the benchmark, normalized to a 50Gbit throughput:"
msgstr ""

#: ../../operations/performance/benchmark.rst:89
msgid ""
"**Kernel wisdom:** TCP flow performance is limited by the receiver, since "
"sender can use both TSO super-packets. This can be observed in the increased "
"CPU spending on the server-side above above."
msgstr ""

#: ../../operations/performance/benchmark.rst:94
msgid "Multi-Stream"
msgstr ""

#: ../../operations/performance/benchmark.rst:96
msgid ""
"In this test, 32 processes are opening 32 parallel TCP connections. Each "
"process is attempting to reach maximum throughput and the total is reported:"
msgstr ""

#: ../../operations/performance/benchmark.rst:101
msgid ""
"Given multiple processes are being used, all test configurations can achieve "
"transfer rates close to the line-rate of the network interface. The main "
"difference is the CPU resources required to achieve it:"
msgstr ""

#: ../../operations/performance/benchmark.rst:110
msgid "Request/Response Rate (TCP_RR)"
msgstr ""

#: ../../operations/performance/benchmark.rst:112
msgid ""
"The request/response rate (TCP_RR) primarily measures the latency and "
"efficiency to handle round-trip forwarding of an individual network packet. "
"This benchmark will lead to the most packets per second possible on the wire "
"and stresses the cost performed by a network packet. This is the opposite of "
"the throughput test which maximizes the size of each network packet."
msgstr ""

#: ../../operations/performance/benchmark.rst:118
msgid ""
"A configuration that is doing well in this test (delivering high requests per "
"second rates) will also deliver better (lower) network latencies."
msgstr ""

#: ../../operations/performance/benchmark.rst:121
msgid ""
"This test represents services which maintain persistent connections and "
"exchange request/response type interactions with other services. This is common "
"for services using REST or gRPC APIs."
msgstr ""

#: ../../operations/performance/benchmark.rst:126
#: ../../operations/performance/benchmark.rst:176
msgid "1 Process"
msgstr ""

#: ../../operations/performance/benchmark.rst:128
msgid ""
"In this test, a single TCP connection is opened between the containers and a "
"single byte is sent back and forth between the containers. For each round-trip, "
"one request is counted:"
msgstr ""

#: ../../operations/performance/benchmark.rst:134
msgid ""
"eBPF on modern kernels can achieve almost the same request/response rate as the "
"baseline while only consuming marginally more CPU resources:"
msgstr ""

#: ../../operations/performance/benchmark.rst:140
#: ../../operations/performance/benchmark.rst:196
msgid "32 Processes"
msgstr ""

#: ../../operations/performance/benchmark.rst:142
msgid ""
"In this test, 32 processes are opening 32 parallel TCP connections. Each "
"process is performing single byte round-trips. The total number of requests per "
"second is reported:"
msgstr ""

#: ../../operations/performance/benchmark.rst:148
#, python-format
msgid ""
"Cilium can achieve close to 1M requests/s in this test while consuming about "
"30% of the system resources on both the sender and receiver:"
msgstr ""

#: ../../operations/performance/benchmark.rst:154
msgid "Connection Rate (TCP_CRR)"
msgstr ""

#: ../../operations/performance/benchmark.rst:156
msgid ""
"The connection rate (TCP_CRR) test measures the efficiency in handling new "
"connections. It is similar to the request/response rate test but will create a "
"new TCP connection for each round-trip. This measures the cost of establishing "
"a connection, transmitting a byte in both directions, and closing the "
"connection. This is more expensive than the TCP_RR test and puts stress on the "
"cost related to handling new connections."
msgstr ""

#: ../../operations/performance/benchmark.rst:163
msgid ""
"This test represents a workload that receives or initiates a lot of TCP "
"connections. An example where this is the case is a publicly exposed service "
"that receives connections from many clients. Good examples of this are L4 "
"proxies or services opening many connections to external endpoints. This "
"benchmark puts the most stress on the system with the least work offloaded to "
"hardware so we can expect to see the biggest difference between tested "
"configurations."
msgstr ""

#: ../../operations/performance/benchmark.rst:171
msgid ""
"A configuration that does well in this test (delivering high connection rates) "
"will handle situations with overwhelming connection rates much better, leaving "
"more CPU resources available to workloads on the system."
msgstr ""

#: ../../operations/performance/benchmark.rst:178
msgid ""
"In this test, a single process opens as many TCP connections as possible in "
"sequence:"
msgstr ""

#: ../../operations/performance/benchmark.rst:183
#: ../../operations/performance/benchmark.rst:216
msgid ""
"The following graph shows the total CPU consumption across the entire system "
"while running the benchmark:"
msgstr ""

#: ../../operations/performance/benchmark.rst:190
msgid ""
"**Kernel wisdom:** The CPU resources graph makes it obvious that some "
"additional kernel cost is paid at the sender as soon as network namespace "
"isolation is performed as all container workload benchmarks show signs of this "
"cost. We will investigate and optimize this aspect in a future release."
msgstr ""

#: ../../operations/performance/benchmark.rst:198
msgid ""
"In this test, 32 processes running in parallel open as many TCP connections in "
"sequence as possible. This is by far the most stressful test for the system."
msgstr ""

#: ../../operations/performance/benchmark.rst:203
msgid ""
"This benchmark outlines major differences between the tested configurations. In "
"particular, it illustrates the overall cost of iptables which is optimized to "
"perform most of the required work per connection and then caches the result. "
"This leads to a worst-case performance scenario when a lot of new connections "
"are expected."
msgstr ""

#: ../../operations/performance/benchmark.rst:211
msgid ""
"We have not been able to measure stable results for the Calico eBPF datapath.  "
"We are not sure why. The network packet flow was never steady. We have thus not "
"included the result. We invite the Calico team to work with us to investigate "
"this and then re-test."
msgstr ""

#: ../../operations/performance/benchmark.rst:222
msgid "Encryption (WireGuard/IPsec)"
msgstr ""

#: ../../operations/performance/benchmark.rst:224
msgid ""
"Cilium supports encryption via WireGuard® and IPsec. This first section will "
"look at WireGuard and compare it against using Calico for WireGuard encryption. "
"If you are interested in IPsec performance and how it compares to WireGuard, "
"please see :ref:`performance_wireguard_ipsec`."
msgstr ""

#: ../../operations/performance/benchmark.rst:230
msgid "WireGuard Throughput"
msgstr ""

#: ../../operations/performance/benchmark.rst:232
msgid ""
"Looking at TCP throughput first, the following graph shows results for both "
"1500 bytes MTU and 9000 bytes MTU:"
msgstr ""

#: ../../operations/performance/benchmark.rst:239
msgid ""
"The Cilium eBPF kube-proxy replacement combined with WireGuard is currently "
"slightly slower than Cilium eBPF + kube-proxy. We have identified the problem "
"and will be resolving this deficit in one of the next releases."
msgstr ""

#: ../../operations/performance/benchmark.rst:243
msgid ""
"The following graph shows the total CPU consumption across the entire system "
"while running the WireGuard encryption benchmark:"
msgstr ""

#: ../../operations/performance/benchmark.rst:249
msgid "WireGuard Request/Response"
msgstr ""

#: ../../operations/performance/benchmark.rst:251
msgid ""
"The next benchmark measures the request/response rate while encrypting with "
"WireGuard. See :ref:`request_response` for details on what this test actually "
"entails."
msgstr ""

#: ../../operations/performance/benchmark.rst:257
msgid ""
"All tested configurations performed more or less the same. The following graph "
"shows the total CPU consumption across the entire system while running the "
"WireGuard encryption benchmark:"
msgstr ""

#: ../../operations/performance/benchmark.rst:266
msgid "WireGuard vs IPsec"
msgstr ""

#: ../../operations/performance/benchmark.rst:268
msgid ""
"In this section, we compare Cilium encryption using WireGuard and IPsec. "
"WireGuard is able to achieve a higher maximum throughput:"
msgstr ""

#: ../../operations/performance/benchmark.rst:273
msgid ""
"However, looking at the CPU resources required to achieve 10Gbit/s of "
"throughput, WireGuard is less efficient at achieving the same throughput:"
msgstr ""

#: ../../operations/performance/benchmark.rst:280
msgid ""
"IPsec performing better than WireGuard in in this test is unexpected in some "
"ways. A possible explanation is that the IPsec encryption is making use of AES-"
"NI instructions whereas the WireGuard implementation is not. This would "
"typically lead to IPsec being more efficient when AES-NI offload is available "
"and WireGuard being more efficient if the instruction set is not available."
msgstr ""

#: ../../operations/performance/benchmark.rst:287
msgid ""
"Looking at the request/response rate, IPsec is outperforming WireGuard in our "
"tests. Unlike for the throughput tests, the MTU does not have any effect as the "
"packet sizes remain small:"
msgstr ""

#: ../../operations/performance/benchmark.rst:295
msgid "Test Environment"
msgstr ""

#: ../../operations/performance/benchmark.rst:300
msgid "Test Hardware"
msgstr ""

#: ../../operations/performance/benchmark.rst:302
msgid "All tests are performed using regular off-the-shelf hardware."
msgstr ""

#: ../../operations/performance/benchmark.rst:305
msgid "Item"
msgstr ""

#: ../../operations/performance/benchmark.rst:307
msgid "CPU"
msgstr ""

#: ../../operations/performance/benchmark.rst:307
msgid ""
"`AMD Ryzen 9 3950x <https://www.amd.com/en/products/cpu/amd-ryzen-9-3950x>`_, "
"AM4 platform, 3.5GHz, 16 cores / 32 threads"
msgstr ""

#: ../../operations/performance/benchmark.rst:308
msgid "Mainboard"
msgstr ""

#: ../../operations/performance/benchmark.rst:308
msgid ""
"`x570 Aorus Master <https://www.gigabyte.com/us/Motherboard/X570-AORUS-MASTER-"
"rev-11-12/sp#sp>`_, PCIe 4.0 x16 support"
msgstr ""

#: ../../operations/performance/benchmark.rst:309
msgid "Memory"
msgstr ""

#: ../../operations/performance/benchmark.rst:309
msgid ""
"`HyperX Fury DDR4-3200 <https://www.hyperxgaming.com/us/memory/fury-ddr4>`_ "
"128GB, XMP clocked to 3.2GHz"
msgstr ""

#: ../../operations/performance/benchmark.rst:310
msgid "Network Card"
msgstr ""

#: ../../operations/performance/benchmark.rst:310
msgid ""
"`Intel E810-CQDA2 <https://ark.intel.com/content/www/us/en/ark/products/192558/"
"intel-ethernet-network-adapter-e810-cqda2.html>`_, dual port, 100Gbit/s per "
"port, PCIe 4.0 x16"
msgstr ""

#: ../../operations/performance/benchmark.rst:311
msgid "Kernel"
msgstr ""

#: ../../operations/performance/benchmark.rst:311
msgid "Linux 5.10 LTS, see also :ref:`performance_tuning`"
msgstr ""

#: ../../operations/performance/benchmark.rst:317
msgid "Test Configurations"
msgstr ""

#: ../../operations/performance/benchmark.rst:319
msgid ""
"All tests are performed using standardized configuration. Upon popular request, "
"we have included measurements for Calico for direct comparison."
msgstr ""

#: ../../operations/performance/benchmark.rst:323
msgid "Configuration Name"
msgstr ""

#: ../../operations/performance/benchmark.rst:325
msgid "Baseline (Node to Node)"
msgstr ""

#: ../../operations/performance/benchmark.rst:325
msgid "No Kubernetes"
msgstr ""

#: ../../operations/performance/benchmark.rst:326
msgid "Cilium"
msgstr ""

#: ../../operations/performance/benchmark.rst:326
msgid "Cilium 1.9.6, eBPF host-routing, kube-proxy replacement, No CT"
msgstr ""

#: ../../operations/performance/benchmark.rst:327
msgid "Cilium (legacy host-routing)"
msgstr ""

#: ../../operations/performance/benchmark.rst:327
msgid "Cilium 1.9.6, legacy host-routing, kube-proxy replacement, No CT"
msgstr ""

#: ../../operations/performance/benchmark.rst:328
msgid "Calico"
msgstr ""

#: ../../operations/performance/benchmark.rst:328
msgid "Calico 3.17.3, kube-proxy"
msgstr ""

#: ../../operations/performance/benchmark.rst:329
msgid "Calico eBPF"
msgstr ""

#: ../../operations/performance/benchmark.rst:329
msgid "Calico 3.17.3, eBPF datapath, No CT"
msgstr ""

#: ../../operations/performance/benchmark.rst:333
msgid "How to reproduce"
msgstr ""

#: ../../operations/performance/benchmark.rst:335
msgid ""
"To ease reproducibility, this report is paired with a set of scripts that can "
"be found in `cilium/cilium-perf-networking <https://github.com/cilium/cilium-"
"perf-networking>`_. All scripts in this document refer to this repository. "
"Specifically, we use `Terraform <https://www.terraform.io/>`_ and `Ansible "
"<https://www.ansible.com/>`_ to setup the environment and execute benchmarks. "
"We use `Packet <https://www.packet.com/>`_ bare metal servers as our hardware "
"platform, but the guide is structured so that it can be easily adapted to other "
"environments."
msgstr ""

#: ../../operations/performance/benchmark.rst:344
msgid "Download the Cilium performance evaluation scripts:"
msgstr ""

#: ../../operations/performance/benchmark.rst:352
msgid "Packet Servers"
msgstr ""

#: ../../operations/performance/benchmark.rst:354
msgid ""
"To evaluate both :ref:`arch_overlay` and :ref:`native_routing`, we configure "
"the Packet machines to use a `\"Mixed/Hybrid\" <https://www.packet.com/"
"developers/docs/network/advanced/layer-2/>`_ network mode, where the secondary "
"interfaces of the machines share a flat L2 network. While this can be done on "
"the Packet web UI, we include appropriate Terraform (version 0.13) files to "
"automate this process."
msgstr ""

#: ../../operations/performance/benchmark.rst:370
msgid ""
"The above will provision two servers named ``knb-0`` and ``knb-1`` of type ``c3."
"small.x86`` and configure them to use a \"Mixed/Hybrid\" network mode under a "
"common VLAN named ``knb``.  The machines will be provisioned with an "
"``ubuntu_20_04`` OS.  We also create a ``packet-hosts.ini`` file to use as an "
"inventory file for Ansible."
msgstr ""

#: ../../operations/performance/benchmark.rst:376
msgid ""
"Verify that the servers are successfully provisioned by executing an ad-hoc "
"``uptime`` command on the servers."
msgstr ""

#: ../../operations/performance/benchmark.rst:393
msgid ""
"Next, we use the ``packet-disbond.yaml`` playbook to configure the network "
"interfaces of the machines. This will destroy the ``bond0`` interface and "
"configure the first physical interface with the public and private IPs "
"(``prv_ip``) and the second with the node IP (``node_ip``) that will be used "
"for our evaluations (see `Packet documentation <https://www.packet.com/"
"resources/guides/layer-2-configurations/>`_ and our scripts for more info)."
msgstr ""

#: ../../operations/performance/benchmark.rst:408
msgid ""
"For hardware platforms other than Packet, users need to provide their own "
"inventory file (``packet-hosts.ini``) and follow the subsequent steps."
msgstr ""

#: ../../operations/performance/benchmark.rst:413
msgid "Install Required Software"
msgstr ""

#: ../../operations/performance/benchmark.rst:415
msgid "Install netperf (used for raw host-to-host measurements):"
msgstr ""

#: ../../operations/performance/benchmark.rst:422
msgid "Install ``kubeadm`` and its dependencies:"
msgstr ""

#: ../../operations/performance/benchmark.rst:428
msgid ""
"We use `kubenetbench <https://github.com/cilium/kubenetbench>`_ to execute the "
"`netperf <https://github.com/HewlettPackard/netperf>`_ benchmark in a "
"Kubernetes environment. kubenetbench is a Kubernetes benchmarking project that "
"is agnostic to the CNI or networking plugin that the cluster is deployed with. "
"In this report we focus on pod-to-pod communication between different nodes. To "
"install kubenetbench:"
msgstr ""

#: ../../operations/performance/benchmark.rst:441
msgid "Running Benchmarks"
msgstr ""

#: ../../operations/performance/benchmark.rst:446
msgid "Tunneling"
msgstr ""

#: ../../operations/performance/benchmark.rst:448
msgid "Configure Cilium in tunneling (:ref:`arch_overlay`) mode:"
msgstr ""

#: ../../operations/performance/benchmark.rst:455
msgid ""
"The first command configures Cilium to use tunneling (``-e mode=tunneling``), "
"which by default uses the VXLAN overlay.  The second executes our benchmark "
"suite (the ``conf`` variable is used to identify this benchmark run). Once "
"execution is done, a results directory will be copied back in a folder named "
"after the ``conf`` variable (in this case, ``vxlan``). This directory includes "
"all the benchmark results as generated by kubenetbench, including netperf "
"output and system information."
msgstr ""

#: ../../operations/performance/benchmark.rst:466
msgid "Native Routing"
msgstr ""

#: ../../operations/performance/benchmark.rst:468
msgid ""
"We repeat the same operation as before, but configure Cilium to use :ref:"
"`native_routing` (``-e mode=directrouting``)."
msgstr ""

#: ../../operations/performance/benchmark.rst:479
msgid "Encryption"
msgstr ""

#: ../../operations/performance/benchmark.rst:481
msgid "To use encryption with native routing:"
msgstr ""

#: ../../operations/performance/benchmark.rst:489
msgid "Baseline"
msgstr ""

#: ../../operations/performance/benchmark.rst:491
msgid ""
"To have a point of reference for our results, we execute the same benchmarks "
"between hosts without Kubernetes running. This provides an effective upper "
"limit to the performance achieved by Cilium."
msgstr ""

#: ../../operations/performance/benchmark.rst:500
msgid ""
"The first command removes Kubernetes and reboots the machines to ensure that "
"there are no residues in the systems, whereas the second executes the same set "
"of benchmarks between hosts. An alternative would be to run the raw benchmark "
"before setting up Cilium, in which case one would only need the second command."
msgstr ""

#: ../../operations/performance/benchmark.rst:506
msgid "Cleanup"
msgstr ""

#: ../../operations/performance/benchmark.rst:508
msgid ""
"When done with benchmarking, the allocated Packet resources can be released "
"with:"
msgstr ""

#: ../../operations/performance/index.rst:10
msgid "Performance & Scalability"
msgstr ""

#: ../../operations/performance/index.rst:12
msgid ""
"Welcome to the performance and scalability guides. This section contains best-"
"practices to tune various performance and scalability aspects. It also contains "
"official benchmarks as measured by the development team in a standardized and "
"repeatable bare metal environment."
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:11
msgid "Limiting Identity-Relevant Labels"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:13
msgid ""
"We recommend that operators with larger environments limit the set of identity-"
"relevant labels to avoid frequent creation of new security identities. Many "
"Kubernetes labels are not useful for policy enforcement or visibility. A few "
"good examples of such labels include timestamps or hashes. These labels, when "
"included in evaluation, cause Cilium to generate a unique identity for each pod "
"instead of a single identity for all of the pods that comprise a service or "
"application."
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:21
msgid ""
"By default, Cilium considers all labels to be relevant for identities, with the "
"following exceptions:"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:25
#: ../../operations/performance/scalability/identity-relevant-labels.rst:50
msgid "Label"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:27
msgid "``any:!io.kubernetes``"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:27
msgid "Ignore all ``io.kubernetes`` labels"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:28
msgid "``any:!kubernetes\\.io``"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:28
msgid "Ignore all other ``kubernetes\\.io`` labels"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:29
msgid "``any:!beta.kubernetes\\.io``"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:29
msgid "Ignore all ``beta\\.kubernetes\\.io`` labels"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:30
msgid "``any:!k8s\\.io``"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:30
msgid "Ignore all ``k8s\\.io`` labels"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:31
msgid "``any:!pod-template-generation``"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:31
msgid "Ignore all ``pod-template-generation`` labels"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:32
msgid "``any:!pod-template-hash``"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:32
msgid "Ignore all ``pod-template-hash`` labels"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:33
msgid "``any:!controller-revision-hash``"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:33
msgid "Ignore all ``controller-revision-hash`` labels"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:34
msgid "``any:!annotation.*``"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:34
msgid "Ignore all ``annotation`` labels"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:35
msgid "``any:!etcd_node``"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:35
msgid "Ignore all ``etcd_node`` labels"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:38
msgid ""
"The above label patterns are all *exclusive label patterns*, that is to say "
"they define which label keys should be ignored. These are identified by the "
"presence of the ``!`` character."
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:42
msgid ""
"Label configurations that do not contain the ``!`` character are *inclusive "
"label patterns*. Once at least one inclusive label pattern is added, only "
"labels that match the inclusive label configuration may be considered relevant "
"for identities. Additionally, when at least one inclusive label pattern is "
"configured, the following inclusive label patterns are automatically added to "
"the configuration:"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:52
msgid "``reserved:.*``"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:52
msgid "Include all ``reserved:`` labels"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:53
msgid "``any:io\\.kubernetes\\.pod\\.namespace``"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:53
msgid "Include all ``io.kubernetes.pod.namespace`` labels"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:54
msgid "``any:io\\.cilium\\.k8s\\.namespace\\.labels``"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:54
msgid "Include all ``io.cilium.k8s.namespace.labels`` labels"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:55
msgid "``any:app\\.kubernetes\\.io``"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:55
msgid "Include all ``app.kubernetes.io`` labels"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:61
msgid "Configuring Identity-Relevant Labels"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:63
msgid ""
"To limit the labels used for evaluating Cilium identities, edit the Cilium "
"ConfigMap object using ``kubectl edit cm -n kube-system cilium-config`` and "
"insert a line to define the label patterns to include or exclude. "
"Alternatively, this attribute can also be set via helm option ``--set "
"labels=<values>``."
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:79
msgid ""
"The double backslash in ``\\\\.`` is required to escape the slash in the YAML "
"string so that the regular expression contains ``\\.``."
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:82
msgid ""
"Label patterns are regular expressions that are implicitly anchored at the "
"start of the label. For example ``example\\.com`` will match labels that start "
"with ``example.com``, whereas ``.*example\\.com`` will match labels that "
"contain ``example.com`` anywhere. Be sure to escape periods in domain names to "
"avoid the pattern matching too broadly and therefore including or excluding too "
"many labels."
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:89
msgid ""
"Upon defining a custom list of label patterns in the ConfigMap, Cilium adds the "
"provided list of label patterns to the default list of label patterns. After "
"saving the ConfigMap, restart the Cilium Agents to pickup the new label pattern "
"setting."
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:98
msgid ""
"Existing identities will not change as a result of this new configuration. To "
"apply the new label pattern setting to existing identities, restart the "
"associated pods. Upon restart, new identities will be created. The old "
"identities will be garbage collected by the Cilium Operator once they are no "
"longer used by any Cilium endpoints."
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:104
msgid ""
"When specifying multiple label patterns to evaluate, provide the list of labels "
"as a space-separated string."
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:108
msgid "Including Labels"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:110
msgid ""
"Labels can be defined as a list of labels to include. Only the labels specified "
"and the default inclusive labels will be used to evaluate Cilium identities:"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:117
msgid ""
"The above configuration would only include the following label keys when "
"evaluating Cilium identities:"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:120
msgid "k8s:k8s-app"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:121
msgid "k8s:app"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:122
msgid "k8s:name"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:123
msgid "reserved:.*"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:124
msgid "io\\.kubernetes\\.pod\\.namespace"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:125
msgid "io\\.cilium\\.k8s.namespace\\.labels"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:126
msgid "app\\.kubernetes\\.io"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:128
msgid ""
"Note that ``k8s:io\\.kubernetes\\.pod\\.namespace`` is already included in "
"default label ``io\\.kubernetes\\.pod\\.namespace``."
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:131
msgid ""
"Labels with the same prefix as defined in the configuration will also be "
"considered. This lists some examples of label keys that would also be evaluated "
"for Cilium identities:"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:135
msgid "k8s-app-team"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:136
msgid "app-production"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:137
msgid "name-defined"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:139
msgid ""
"When a single inclusive label is added to the filter, all labels not defined in "
"the default list will be excluded. For example, pods running with the security "
"labels ``team=team-1, env=prod`` will have the label ``env=prod`` ignored as "
"soon Cilium is started with the filter ``k8s:team``."
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:145
msgid "Excluding Labels"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:147
msgid ""
"Label patterns can also be specified as a list of exclusions. Exclude labels by "
"placing an exclamation mark after colon separating the prefix and pattern. When "
"defined as a list of exclusions, Cilium will include the set of default labels, "
"but will exclude any matches in the provided list when evaluating Cilium "
"identities:"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:157
msgid ""
"The provided example would cause Cilium to exclude any of the following label "
"matches:"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:160
msgid "k8s:controller-uid"
msgstr ""

#: ../../operations/performance/scalability/identity-relevant-labels.rst:161
msgid "k8s:job-name"
msgstr ""

#: ../../operations/performance/scalability/index.rst:9
msgid "Scalability"
msgstr ""

#: ../../operations/performance/scalability/report.rst:11
msgid "Scalability report"
msgstr ""

#: ../../operations/performance/scalability/report.rst:13
msgid ""
"This report is intended for users planning to run Cilium on clusters with more "
"than 200 nodes in CRD mode (without a kvstore available). In our development "
"cycle we have deployed Cilium on large clusters and these were the options that "
"were suitable for our testing:"
msgstr ""

#: ../../operations/performance/scalability/report.rst:20
msgid "Setup"
msgstr ""

#: ../../operations/performance/scalability/report.rst:36
msgid ""
"``--set endpointHealthChecking.enabled=false`` and ``--set "
"healthChecking=false`` disable endpoint health checking entirely. However it is "
"recommended that those features be enabled initially on a smaller cluster (3-10 "
"nodes) where it can be used to detect potential packet loss due to firewall "
"rules or hypervisor settings."
msgstr ""

#: ../../operations/performance/scalability/report.rst:42
msgid ""
"``--set ipam.mode=kubernetes`` is set to ``\"kubernetes\"`` since our cloud "
"provider has pod CIDR allocation enabled in ``kube-controller-manager``."
msgstr ""

#: ../../operations/performance/scalability/report.rst:45
msgid ""
"``--set k8sServiceHost`` and ``--set k8sServicePort`` were set with the IP "
"address of the loadbalancer that was in front of ``kube-apiserver``. This "
"allows Cilium to not depend on kube-proxy to connect to ``kube-apiserver``."
msgstr ""

#: ../../operations/performance/scalability/report.rst:49
msgid ""
"``--set prometheus.enabled=true`` and ``--set operator.prometheus."
"enabled=true`` were just set because we had a Prometheus server probing for "
"metrics in the entire cluster."
msgstr ""

#: ../../operations/performance/scalability/report.rst:53
msgid ""
"Our testing cluster consisted of 3 controller nodes and 1000 worker nodes. We "
"have followed the recommended settings from the `official Kubernetes "
"documentation <https://kubernetes.io/docs/setup/best-practices/cluster-large/"
">`_ and have provisioned our machines with the following settings:"
msgstr ""

#: ../../operations/performance/scalability/report.rst:58
msgid "**Cloud provider**: Google Cloud"
msgstr ""

#: ../../operations/performance/scalability/report.rst:60
msgid ""
"**Controllers**: 3x n1-standard-32 (32vCPU, 120GB memory and 50GB SSD, kernel "
"5.4.0-1009-gcp)"
msgstr ""

#: ../../operations/performance/scalability/report.rst:62
msgid ""
"**Workers**: 1 pool of 1000x custom-2-4096 (2vCPU, 4GB memory and 10GB HDD, "
"kernel 5.4.0-1009-gcp)"
msgstr ""

#: ../../operations/performance/scalability/report.rst:64
msgid ""
"**Metrics**: 1x n1-standard-32 (32vCPU, 120GB memory and 10GB HDD + 500GB HDD) "
"this is a dedicated node for prometheus and grafana pods."
msgstr ""

#: ../../operations/performance/scalability/report.rst:69
msgid "All 3 controller nodes were behind a GCE load balancer."
msgstr ""

#: ../../operations/performance/scalability/report.rst:71
msgid ""
"Each controller contained ``etcd``, ``kube-apiserver``, ``kube-controller-"
"manager`` and ``kube-scheduler`` instances."
msgstr ""

#: ../../operations/performance/scalability/report.rst:74
msgid ""
"The CPU, memory and disk size set for the workers might be different for your "
"use case. You might have pods that require more memory or CPU available so you "
"should design your workers based on your requirements."
msgstr ""

#: ../../operations/performance/scalability/report.rst:78
msgid ""
"During our testing we had to set the ``etcd`` option ``quota-backend-"
"bytes=17179869184`` because ``etcd`` failed once it reached around ``2GiB`` of "
"allocated space."
msgstr ""

#: ../../operations/performance/scalability/report.rst:82
msgid ""
"We provisioned our worker nodes without ``kube-proxy`` since Cilium is capable "
"of performing all functionalities provided by ``kube-proxy``. We created a load "
"balancer in front of ``kube-apiserver`` to allow Cilium to access ``kube-"
"apiserver`` without ``kube-proxy``, and configured Cilium with the options ``--"
"set k8sServiceHost=<KUBE-APISERVER-LB-IP-ADDRESS>`` and ``--set "
"k8sServicePort=<KUBE-APISERVER-LB-PORT-NUMBER>``."
msgstr ""

#: ../../operations/performance/scalability/report.rst:89
msgid ""
"Our ``DaemonSet`` ``updateStrategy`` had the ``maxUnavailable`` set to 250 pods "
"instead of 2, but this value highly depends on your requirements when you are "
"performing a rolling update of Cilium."
msgstr ""

#: ../../operations/performance/scalability/report.rst:95
msgid "Steps"
msgstr ""

#: ../../operations/performance/scalability/report.rst:97
msgid ""
"For each step we took, we provide more details below, with our findings and "
"expected behaviors."
msgstr ""

#: ../../operations/performance/scalability/report.rst:102
msgid "1. Install Kubernetes v1.18.3 with EndpointSlice feature enabled"
msgstr ""

#: ../../operations/performance/scalability/report.rst:104
msgid ""
"To test the most up-to-date functionalities from Kubernetes and Cilium, we have "
"performed our testing with Kubernetes v1.18.3 and the EndpointSlice feature "
"enabled to improve scalability."
msgstr ""

#: ../../operations/performance/scalability/report.rst:108
msgid "Since Kubernetes requires an ``etcd`` cluster, we have deployed v3.4.9."
msgstr ""

#: ../../operations/performance/scalability/report.rst:112
msgid "2. Deploy Prometheus, Grafana and Cilium"
msgstr ""

#: ../../operations/performance/scalability/report.rst:114
msgid ""
"We have used Prometheus v2.18.1 and Grafana v7.0.1 to retrieve and analyze "
"``etcd``, ``kube-apiserver``, ``cilium`` and ``cilium-operator`` metrics."
msgstr ""

#: ../../operations/performance/scalability/report.rst:119
msgid "3. Provision 2 worker nodes"
msgstr ""

#: ../../operations/performance/scalability/report.rst:121
msgid ""
"This helped us to understand if our testing cluster was correctly provisioned "
"and all metrics were being gathered."
msgstr ""

#: ../../operations/performance/scalability/report.rst:126
msgid "4. Deploy 5 namespaces with 25 deployments on each namespace"
msgstr ""

#: ../../operations/performance/scalability/report.rst:128
msgid "Each deployment had 1 replica (125 pods in total)."
msgstr ""

#: ../../operations/performance/scalability/report.rst:130
msgid ""
"To measure **only** the resources consumed by Cilium, all deployments used the "
"same base image ``k8s.gcr.io/pause:3.2``. This image does not have any CPU or "
"memory overhead."
msgstr ""

#: ../../operations/performance/scalability/report.rst:134
msgid ""
"We provision a small number of pods in a small cluster to understand the CPU "
"usage of Cilium:"
msgstr ""

#: ../../operations/performance/scalability/report.rst:139
msgid ""
"The mark shows when the creation of 125 pods started. As expected, we can see a "
"slight increase of the CPU usage on both Cilium agents running and in the "
"Cilium operator. The agents peaked at 6.8% CPU usage on a 2vCPU machine."
msgstr ""

#: ../../operations/performance/scalability/report.rst:146
msgid ""
"For the memory usage, we have not seen a significant memory growth in the "
"Cilium agent. On the eBPF memory side, we do see it increasing due to the "
"initialization of some eBPF maps for the new pods."
msgstr ""

#: ../../operations/performance/scalability/report.rst:152
msgid "5. Provision 998 additional nodes (total 1000 nodes)"
msgstr ""

#: ../../operations/performance/scalability/report.rst:156
#, python-format
msgid ""
"The first mark represents the action of creating nodes, the second mark when "
"1000 Cilium pods were in ready state. The CPU usage increase is expected since "
"each Cilium agent receives events from Kubernetes whenever a new node is "
"provisioned in the cluster. Once all nodes were deployed the CPU usage was "
"0.15% on average on a 2vCPU node."
msgstr ""

#: ../../operations/performance/scalability/report.rst:164
msgid ""
"As we have increased the number of nodes in the cluster to 1000, it is expected "
"to see a small growth of the memory usage in all metrics. However, it is "
"relevant to point out that **an increase in the number of nodes does not cause "
"any significant increase in Cilium’s memory consumption in both control and "
"dataplane.**"
msgstr ""

#: ../../operations/performance/scalability/report.rst:172
msgid "6. Deploy 25 more deployments on each namespace"
msgstr ""

#: ../../operations/performance/scalability/report.rst:174
msgid ""
"This will now bring us a total of ``5 namespaces * (25 old deployments + 25 new "
"deployments)=250`` deployments in the entire cluster. We did not install 250 "
"deployments from the start since we only had 2 nodes and that would create 125 "
"pods on each worker node. According to the Kubernetes documentation the maximum "
"recommended number of pods per node is 100."
msgstr ""

#: ../../operations/performance/scalability/report.rst:183
msgid "7. Scale each deployment to 200 replicas (50000 pods in total)"
msgstr ""

#: ../../operations/performance/scalability/report.rst:185
msgid ""
"Having 5 namespaces with 50 deployments means that we have 250 different unique "
"security identities. Having a low cardinality in the labels selected by Cilium "
"helps scale the cluster. By default, Cilium has a limit of 16k security "
"identities, but it can be increased with ``bpf-policy-map-max`` in the Cilium "
"``ConfigMap``."
msgstr ""

#: ../../operations/performance/scalability/report.rst:193
msgid ""
"The first mark represents the action of scaling up the deployments, the second "
"mark when 50000 pods were in ready state."
msgstr ""

#: ../../operations/performance/scalability/report.rst:196
msgid ""
"It is expected to see the CPU usage of Cilium increase since, on each node, "
"Cilium agents receive events from Kubernetes when a new pod is scheduled and "
"started."
msgstr ""

#: ../../operations/performance/scalability/report.rst:200
#, python-format
msgid ""
"The average CPU consumption of all Cilium agents was 3.38% on a 2vCPU machine. "
"At one point, roughly around minute 15:23, one of those Cilium agents picked "
"27.94% CPU usage."
msgstr ""

#: ../../operations/performance/scalability/report.rst:204
msgid ""
"Cilium Operator had a stable 5% CPU consumption while the pods were being "
"created."
msgstr ""

#: ../../operations/performance/scalability/report.rst:209
msgid ""
"Similar to the behavior seen while increasing the number of worker nodes, "
"adding new pods also increases Cilium memory consumption."
msgstr ""

#: ../../operations/performance/scalability/report.rst:212
msgid ""
"As we increased the number of pods from 250 to 50000, we saw a maximum memory "
"usage of 573MiB for one of the Cilium agents while the average was 438 MiB."
msgstr ""

#: ../../operations/performance/scalability/report.rst:214
msgid "For the eBPF memory usage we saw a max usage of 462.7MiB"
msgstr ""

#: ../../operations/performance/scalability/report.rst:215
msgid ""
"This means that each **Cilium agent's memory increased by 10.5KiB per new pod "
"in the cluster.**"
msgstr ""

#: ../../operations/performance/scalability/report.rst:220
msgid "8. Deploy 250 policies for 1 namespace"
msgstr ""

#: ../../operations/performance/scalability/report.rst:222
msgid ""
"Here we have created 125 L4 network policies and 125 L7 policies. Each policy "
"selected all pods on this namespace and was allowed to send traffic to another "
"pod on this namespace. Each of the 250 policies allows access to a disjoint set "
"of ports. In the end we will have 250 different policies selecting 10000 pods."
msgstr ""

#: ../../operations/performance/scalability/report.rst:275
#, python-format
msgid ""
"In this case we saw one of the Cilium agents jumping to 100% CPU usage for 15 "
"seconds while the average peak was 40% during a period of 90 seconds."
msgstr ""

#: ../../operations/performance/scalability/report.rst:280
msgid ""
"As expected, **increasing the number of policies does not have a significant "
"impact on the memory usage of Cilium since the eBPF policy maps have a constant "
"size** once a pod is initialized."
msgstr ""

#: ../../operations/performance/scalability/report.rst:288
msgid ""
"The first mark represents the point in time when we ran ``kubectl create`` to "
"create the ``CiliumNetworkPolicies``. Since we created the 250 policies "
"sequentially, we cannot properly compute the convergence time. To do that, we "
"could use a single CNP with multiple policy rules defined under the ``specs`` "
"field (instead of the ``spec`` field)."
msgstr ""

#: ../../operations/performance/scalability/report.rst:294
msgid ""
"Nevertheless, we can see the time it took the last Cilium agent to increment "
"its Policy Revision, which is incremented individually on each Cilium agent "
"every time a CiliumNetworkPolicy (CNP) is received, between second ``15:45:44`` "
"and ``15:45:46`` and see when was the last time an Endpoint was regenerated by "
"checking the 99th percentile of the \"Endpoint regeneration time\". In this "
"manner, that it took less than 5s. We can also verify **the maximum time was "
"less than 600ms for an endpoint to have the policy enforced.**"
msgstr ""

#: ../../operations/performance/scalability/report.rst:305
msgid "9. Deploy 250 policies for CiliumClusterwideNetworkPolicies (CCNP)"
msgstr ""

#: ../../operations/performance/scalability/report.rst:307
msgid ""
"The difference between these policies and the previous ones installed is that "
"these select all pods in all namespaces. To recap, this means that we will now "
"have **250 different network policies selecting 10000 pods and 250 different "
"network policies selecting 50000 pods on a cluster with 1000 nodes.** Similarly "
"to the previous step we will deploy 125 L4 policies and another 125 L7 policies."
msgstr ""

#: ../../operations/performance/scalability/report.rst:316
msgid ""
"Similar to the creation of the previous 250 CNPs, there was also an increase in "
"CPU usage during the creation of the CCNPs. The CPU usage was similar even "
"though the policies were effectively selecting more pods."
msgstr ""

#: ../../operations/performance/scalability/report.rst:322
msgid ""
"As all pods running in a node are selected by **all 250 CCNPs created**, we see "
"an increase of the **Endpoint regeneration time** which **peaked a little above "
"3s.**"
msgstr ""

#: ../../operations/performance/scalability/report.rst:329
msgid "10. \"Accidentally\" delete 10000 pods"
msgstr ""

#: ../../operations/performance/scalability/report.rst:331
msgid ""
"In this step we have \"accidentally\" deleted 10000 random pods. Kubernetes "
"will then recreate 10000 new pods so it will help us understand what the "
"convergence time is for all the deployed network polices."
msgstr ""

#: ../../operations/performance/scalability/report.rst:339
msgid ""
"The first mark represents the point in time when pods were \"deleted\" and the "
"second mark represents the point in time when Kubernetes finished recreating "
"10k pods."
msgstr ""

#: ../../operations/performance/scalability/report.rst:343
msgid ""
"Besides the CPU usage slightly increasing while pods are being scheduled in the "
"cluster, we did see some interesting data points in the eBPF memory usage. As "
"each endpoint can have one or more dedicated eBPF maps, the eBPF memory usage "
"is directly proportional to the number of pods running in a node. **If the "
"number of pods per node decreases so does the eBPF memory usage.**"
msgstr ""

#: ../../operations/performance/scalability/report.rst:351
msgid ""
"We inferred the time it took for all the endpoints to get regenerated by "
"looking at the number of Cilium endpoints with the policy enforced over time. "
"Luckily enough we had another metric that was showing how many Cilium endpoints "
"had policy being enforced:"
msgstr ""

#: ../../operations/performance/scalability/report.rst:360
msgid "11. Control plane metrics over the test run"
msgstr ""

#: ../../operations/performance/scalability/report.rst:362
msgid ""
"The focus of this test was to study the Cilium agent resource consumption at "
"scale. However, we also monitored some metrics of the control plane nodes such "
"as etcd metrics and CPU usage of the k8s-controllers and we present them in the "
"next figures."
msgstr ""

#: ../../operations/performance/scalability/report.rst:369
msgid ""
"Memory consumption of the 3 etcd instances during the entire scalability "
"testing."
msgstr ""

#: ../../operations/performance/scalability/report.rst:374
msgid ""
"CPU usage for the 3 controller nodes, average latency per request type in the "
"etcd cluster as well as the number of operations per second made to etcd."
msgstr ""

#: ../../operations/performance/scalability/report.rst:379
msgid ""
"All etcd metrics, from left to right, from top to bottom: database size, disk "
"sync duration, client traffic in, client traffic out, peer traffic in, peer "
"traffic out."
msgstr ""

#: ../../operations/performance/scalability/report.rst:385
msgid "Final Remarks"
msgstr ""

#: ../../operations/performance/scalability/report.rst:387
msgid ""
"These experiments helped us develop a better understanding of Cilium running in "
"a large cluster entirely in CRD mode and without depending on etcd. There is "
"still some work to be done to optimize the memory footprint of eBPF maps even "
"further, as well as reducing the memory footprint of the Cilium agent. We will "
"address those in the next Cilium version."
msgstr ""

#: ../../operations/performance/scalability/report.rst:393
msgid ""
"We can also determine that it is scalable to run Cilium in CRD mode on a "
"cluster with more than 200 nodes. However, it is worth pointing out that we "
"need to run more tests to verify Cilium's behavior when it loses the "
"connectivity with ``kube-apiserver``, as can happen during a control plane "
"upgrade for example. This will also be our focus in the next Cilium version."
msgstr ""

#: ../../operations/performance/tuning.rst:11
msgid "Tuning Guide"
msgstr ""

#: ../../operations/performance/tuning.rst:13
msgid ""
"This guide helps you optimize a Cilium installation for optimal performance."
msgstr ""

#: ../../operations/performance/tuning.rst:16
#: ../../operations/performance/tuning.rst:175
msgid "eBPF Host-Routing"
msgstr ""

#: ../../operations/performance/tuning.rst:18
msgid ""
"Even when network routing is performed by Cilium using eBPF, by default network "
"packets still traverse some parts of the regular network stack of the node. "
"This ensures that all packets still traverse through all of the iptables hooks "
"in case you depend on them. However, they add significant overhead. For exact "
"numbers from our test environment, see :ref:`benchmark_throughput` and compare "
"the results for \"Cilium\" and \"Cilium (legacy host-routing)\"."
msgstr ""

#: ../../operations/performance/tuning.rst:25
msgid ""
"We introduced `eBPF-based host-routing <https://cilium.io/blog/2020/11/10/"
"cilium-19#veth>`_ in Cilium 1.9 to fully bypass iptables and the upper host "
"stack, and to achieve a faster network namespace switch compared to regular "
"veth device operation. This option is automatically enabled if your kernel "
"supports it. To validate whether your installation is running with eBPF host-"
"routing, run ``cilium status`` in any of the Cilium pods and look for the line "
"reporting the status for \"Host Routing\" which should state \"BPF\"."
msgstr ""

#: ../../operations/performance/tuning.rst:33
#: ../../operations/performance/tuning.rst:49
#: ../../operations/performance/tuning.rst:124
#: ../../operations/performance/tuning.rst:171
#: ../../operations/performance/tuning.rst:210
msgid "**Requirements:**"
msgstr ""

#: ../../operations/performance/tuning.rst:35
msgid "Kernel >= 5.10"
msgstr ""

#: ../../operations/performance/tuning.rst:36
#: ../../operations/performance/tuning.rst:127
msgid "Direct-routing configuration or tunneling"
msgstr ""

#: ../../operations/performance/tuning.rst:37
#: ../../operations/performance/tuning.rst:53
#: ../../operations/performance/tuning.rst:128
#: ../../operations/performance/tuning.rst:215
msgid "eBPF-based kube-proxy replacement"
msgstr ""

#: ../../operations/performance/tuning.rst:38
msgid "eBPF-based masquerading"
msgstr ""

#: ../../operations/performance/tuning.rst:41
msgid "Bypass iptables Connection Tracking"
msgstr ""

#: ../../operations/performance/tuning.rst:43
msgid ""
"For the case when eBPF Host-Routing cannot be used and thus network packets "
"still need to traverse the regular network stack in the host namespace, "
"iptables can add a significant cost. This traversal cost can be minimized by "
"disabling the connection tracking requirement for all Pod traffic, thus "
"bypassing the iptables connection tracker."
msgstr ""

#: ../../operations/performance/tuning.rst:51
#: ../../operations/performance/tuning.rst:212
msgid "Kernel >= 4.19.57, >= 5.1.16, >= 5.2"
msgstr ""

#: ../../operations/performance/tuning.rst:52
#: ../../operations/performance/tuning.rst:214
msgid "Direct-routing configuration"
msgstr ""

#: ../../operations/performance/tuning.rst:54
msgid "eBPF-based masquerading or no masquerading"
msgstr ""

#: ../../operations/performance/tuning.rst:56
msgid "To enable the iptables connection-tracking bypass:"
msgstr ""

#: ../../operations/performance/tuning.rst:60
#: ../../operations/performance/tuning.rst:86
msgid "Cilium CLI"
msgstr ""

#: ../../operations/performance/tuning.rst:66
#: ../../operations/performance/tuning.rst:92
#: ../../operations/performance/tuning.rst:134
#: ../../operations/performance/tuning.rst:181 ../../operations/upgrade.rst:47
#: ../../operations/upgrade.rst:120 ../../operations/upgrade.rst:186
#: ../../operations/upgrade.rst:254
msgid "Helm"
msgstr ""

#: ../../operations/performance/tuning.rst:78
#, python-format
msgid ""
"Running with Hubble observability enabled can come at the expense of "
"performance. The overhead of Hubble is somewhere between 1-15% depending on "
"your network traffic patterns and Hubble aggregation settings."
msgstr ""

#: ../../operations/performance/tuning.rst:82
msgid "In order to optimize for maximum performance, Hubble can be disabled:"
msgstr ""

#: ../../operations/performance/tuning.rst:101
msgid "MTU"
msgstr ""

#: ../../operations/performance/tuning.rst:103
msgid ""
"The maximum transfer unit (MTU) can have a significant impact on the network "
"throughput of a configuration. Cilium will automatically detect the MTU of the "
"underlying network devices. Therefore, if your system is configured to use "
"jumbo frames, Cilium will automatically make use of it."
msgstr ""

#: ../../operations/performance/tuning.rst:108
msgid ""
"To benefit from this, make sure that your system is configured to use jumbo "
"frames if your network allows for it."
msgstr ""

#: ../../operations/performance/tuning.rst:112
#: ../../operations/performance/tuning.rst:174
msgid "Bandwidth Manager"
msgstr ""

#: ../../operations/performance/tuning.rst:114
msgid ""
"Cilium's Bandwidth Manager is responsible for managing network traffic more "
"efficiently with the goal of improving overall application latency and "
"throughput."
msgstr ""

#: ../../operations/performance/tuning.rst:117
msgid ""
"Aside from natively supporting Kubernetes Pod bandwidth annotations, the "
"`Bandwidth Manager <https://cilium.io/blog/2020/11/10/cilium-19#bwmanager>`_, "
"first introduced in Cilium 1.9, is also setting up Fair Queue (FQ) queueing "
"disciplines to support TCP stack pacing (e.g. from EDT/BBR) on all external-"
"facing network devices as well as setting optimal server-grade sysctl settings "
"for the networking stack."
msgstr ""

#: ../../operations/performance/tuning.rst:126
msgid "Kernel >= 5.1"
msgstr ""

#: ../../operations/performance/tuning.rst:130
msgid "To enable the Bandwidth Manager:"
msgstr ""

#: ../../operations/performance/tuning.rst:143
msgid ""
"To validate whether your installation is running with Bandwidth Manager, run "
"``cilium status`` in any of the Cilium pods and look for the line reporting the "
"status for \"BandwidthManager\" which should state \"EDT with BPF\"."
msgstr ""

#: ../../operations/performance/tuning.rst:148
msgid "BBR congestion control for Pods"
msgstr ""

#: ../../operations/performance/tuning.rst:150
msgid ""
"The base infrastructure around MQ/FQ setup provided by Cilium's Bandwidth "
"Manager also allows for use of TCP `BBR congestion control <https://queue.acm."
"org/detail.cfm?id=3022184>`_ for Pods. BBR is in particular suitable when Pods "
"are exposed behind Kubernetes Services which face external clients from the "
"Internet. BBR achieves higher bandwidths and lower latencies for Internet "
"traffic, for example, it has been `shown <https://cloud.google.com/blog/"
"products/networking/tcp-bbr-congestion-control-comes-to-gcp-your-internet-just-"
"got-faster>`_ that BBR's throughput can reach as much as 2,700x higher than "
"today's best loss-based congestion control and queueing delays can be 25x lower."
msgstr ""

#: ../../operations/performance/tuning.rst:159
msgid ""
"In order for BBR to work reliably for Pods, it requires a 5.18 or higher "
"kernel. As outlined in our `Linux Plumbers 2021 talk <https://lpc.events/"
"event/11/contributions/953/>`_, this is needed since older kernels do not "
"retain timestamps of network packets when switching from Pod to host network "
"namespace. Due to the latter, the kernel's pacing infrastructure does not "
"function properly in general (not specific to Cilium). We helped fixing this "
"issue for recent kernels to retain timestamps and therefore to get BBR for Pods "
"working."
msgstr ""

#: ../../operations/performance/tuning.rst:167
msgid ""
"BBR also needs eBPF Host-Routing in order to retain the network packet's socket "
"association all the way until the packet hits the FQ queueing discipline on the "
"physical device in the host namespace."
msgstr ""

#: ../../operations/performance/tuning.rst:173
msgid "Kernel >= 5.18"
msgstr ""

#: ../../operations/performance/tuning.rst:177
msgid "To enable the Bandwidth Manager with BBR for Pods:"
msgstr ""

#: ../../operations/performance/tuning.rst:191
msgid ""
"To validate whether your installation is running with BBR for Pods, run "
"``cilium status`` in any of the Cilium pods and look for the line reporting the "
"status for \"BandwidthManager\" which should then state ``EDT with BPF`` as "
"well as ``[BBR]``."
msgstr ""

#: ../../operations/performance/tuning.rst:197
msgid "XDP Acceleration"
msgstr ""

#: ../../operations/performance/tuning.rst:199
msgid ""
"Cilium has built-in support for accelerating NodePort, LoadBalancer services "
"and services with externalIPs for the case where the arriving request needs to "
"be pushed back out of the node when the backend is located on a remote node."
msgstr ""

#: ../../operations/performance/tuning.rst:203
msgid ""
"In that case, the network packets do not need to be pushed all the way to the "
"upper networking stack, but with the help of XDP, Cilium is able to process "
"those requests right out of the network driver layer. This helps to reduce "
"latency and scale-out of services given a single node's forwarding capacity is "
"dramatically increased. The kube-proxy replacement at the XDP layer is "
"`available from Cilium 1.8 <https://cilium.io/blog/2020/06/22/"
"cilium-18#kubeproxy-removal>`_."
msgstr ""

#: ../../operations/performance/tuning.rst:213
msgid ""
"Native XDP supported driver, check :ref:`our driver list <XDP acceleration>`"
msgstr ""

#: ../../operations/performance/tuning.rst:217
msgid ""
"To enable the XDP Acceleration, check out :ref:`our getting started guide <XDP "
"acceleration>` which also contains instructions for setting it up on public "
"cloud providers."
msgstr ""

#: ../../operations/performance/tuning.rst:220
msgid ""
"To validate whether your installation is running with XDP Acceleration, run "
"``cilium status`` in any of the Cilium pods and look for the line reporting the "
"status for \"XDP Acceleration\" which should say \"Native\"."
msgstr ""

#: ../../operations/performance/tuning.rst:225
msgid "eBPF Map Sizing"
msgstr ""

#: ../../operations/performance/tuning.rst:227
msgid ""
"All eBPF maps are created with upper capacity limits. Insertion beyond the "
"limit would fail or constrain the scalability of the datapath. Cilium is using "
"auto-derived defaults based on the given ratio of the total system memory."
msgstr ""

#: ../../operations/performance/tuning.rst:232
msgid ""
"However, the upper capacity limits used by the Cilium agent can be overridden "
"for advanced users. Please refer to the :ref:`bpf_map_limitations` guide."
msgstr ""

#: ../../operations/performance/tuning.rst:236
#: ../../operations/system_requirements.rst:104
msgid "Linux Kernel"
msgstr ""

#: ../../operations/performance/tuning.rst:238
msgid ""
"In general, we highly recommend using the most recent LTS stable kernel (such "
"as >= 5.10) provided by the `kernel community <https://www.kernel.org/category/"
"releases.html>`_ or by a downstream distribution of your choice. The newer the "
"kernel, the more likely it is that various datapath optimizations can be used."
msgstr ""

#: ../../operations/performance/tuning.rst:243
msgid ""
"In our Cilium release blogs, we also regularly highlight some of the eBPF based "
"kernel work we conduct which implicitly helps Cilium's datapath performance "
"such as `replacing retpolines with direct jumps in the eBPF JIT <https://cilium."
"io/blog/2020/02/18/cilium-17#linux-kernel-changes>`_."
msgstr ""

#: ../../operations/performance/tuning.rst:247
msgid ""
"Moreover, the kernel allows to configure several options which will help "
"maximize network performance."
msgstr ""

#: ../../operations/performance/tuning.rst:251
msgid "CONFIG_PREEMPT_NONE"
msgstr ""

#: ../../operations/performance/tuning.rst:253
msgid ""
"Run a kernel version with ``CONFIG_PREEMPT_NONE=y`` set. Some Linux "
"distributions offer kernel images with this option set or you can re-compile "
"the Linux kernel. ``CONFIG_PREEMPT_NONE=y`` is the recommended setting for "
"server workloads."
msgstr ""

#: ../../operations/performance/tuning.rst:259
msgid "Further Considerations"
msgstr ""

#: ../../operations/performance/tuning.rst:261
msgid ""
"Various additional settings that we recommend help to tune the system for "
"specific workloads and to reduce jitter:"
msgstr ""

#: ../../operations/performance/tuning.rst:265
msgid "tuned network-* profiles"
msgstr ""

#: ../../operations/performance/tuning.rst:267
msgid ""
"The `tuned <https://tuned-project.org/>`_ project offers various profiles to "
"optimize for deterministic performance at the cost of increased power "
"consumption, that is, ``network-latency`` and ``network-throughput``, for "
"example. To enable the former, run:"
msgstr ""

#: ../../operations/performance/tuning.rst:277
msgid "Set CPU governor to performance"
msgstr ""

#: ../../operations/performance/tuning.rst:279
msgid ""
"The CPU scaling up and down can impact latency tests and lead to sub-optimal "
"performance. To achieve maximum consistent performance. Set the CPU governor to "
"``performance``:"
msgstr ""

#: ../../operations/performance/tuning.rst:290
msgid "Stop ``irqbalance`` and pin the NIC interrupts to specific CPUs"
msgstr ""

#: ../../operations/performance/tuning.rst:292
msgid ""
"In case you are running ``irqbalance``, consider disabling it as it might "
"migrate the NIC's IRQ handling among CPUs and can therefore cause non-"
"deterministic performance:"
msgstr ""

#: ../../operations/performance/tuning.rst:300
msgid ""
"We highly recommend to pin the NIC interrupts to specific CPUs in order to "
"allow for maximum workload isolation!"
msgstr ""

#: ../../operations/performance/tuning.rst:303
msgid ""
"See `this script <https://github.com/borkmann/netperf_scripts/blob/master/"
"set_irq_affinity>`_ for details and initial pointers on how to achieve this. "
"Note that pinning the queues can potentially vary in setup between different "
"drivers."
msgstr ""

#: ../../operations/performance/tuning.rst:307
msgid ""
"We generally also recommend to check various documentation and performance "
"tuning guides from NIC vendors on this matter such as from `Mellanox <https://"
"community.mellanox.com/s/article/performance-tuning-for-mellanox-adapters>`_, "
"`Intel <https://www.intel.com/content/www/us/en/support/articles/000005811/"
"network-and-i-o/ethernet-products.html>`_ or others for more information."
msgstr ""

#: ../../operations/system_requirements.rst:11
msgid "System Requirements"
msgstr ""

#: ../../operations/system_requirements.rst:13
msgid ""
"Before installing Cilium, please ensure that your system meets the minimum "
"requirements below. Most modern Linux distributions already do."
msgstr ""

#: ../../operations/system_requirements.rst:17
msgid "Summary"
msgstr ""

#: ../../operations/system_requirements.rst:19
msgid ""
"When running Cilium using the container image ``cilium/cilium``, the host "
"system must meet these requirements:"
msgstr ""

#: ../../operations/system_requirements.rst:22
msgid "`Linux kernel`_ >= 4.9.17"
msgstr ""

#: ../../operations/system_requirements.rst:24
msgid ""
"When running Cilium as a native process on your host (i.e. **not** running the "
"``cilium/cilium`` container image) these additional requirements must be met:"
msgstr ""

#: ../../operations/system_requirements.rst:27
msgid "`clang+LLVM`_ >= 10.0"
msgstr ""

#: ../../operations/system_requirements.rst:28
msgid "iproute2_ with eBPF templating patches [#iproute2_foot]_"
msgstr ""

#: ../../operations/system_requirements.rst:33
msgid ""
"When running Cilium without Kubernetes these additional requirements must be "
"met:"
msgstr ""

#: ../../operations/system_requirements.rst:36
msgid ":ref:`req_kvstore` etcd >= 3.1.0"
msgstr ""

#: ../../operations/system_requirements.rst:39
msgid "Requirement"
msgstr ""

#: ../../operations/system_requirements.rst:39
#: ../../operations/system_requirements.rst:57
msgid "Minimum Version"
msgstr ""

#: ../../operations/system_requirements.rst:39
msgid "In cilium container"
msgstr ""

#: ../../operations/system_requirements.rst:41
msgid "`Linux kernel`_"
msgstr ""

#: ../../operations/system_requirements.rst:41
msgid ">= 4.9.17"
msgstr ""

#: ../../operations/system_requirements.rst:41
#: ../../operations/system_requirements.rst:42
msgid "no"
msgstr ""

#: ../../operations/system_requirements.rst:42
msgid "Key-Value store (etcd)"
msgstr ""

#: ../../operations/system_requirements.rst:42
msgid ">= 3.1.0"
msgstr ""

#: ../../operations/system_requirements.rst:43
#: ../../operations/system_requirements.rst:272
msgid "clang+LLVM"
msgstr ""

#: ../../operations/system_requirements.rst:43
msgid ">= 10.0"
msgstr ""

#: ../../operations/system_requirements.rst:43
#: ../../operations/system_requirements.rst:44
msgid "yes"
msgstr ""

#: ../../operations/system_requirements.rst:44
#: ../../operations/system_requirements.rst:290
msgid "iproute2"
msgstr ""

#: ../../operations/system_requirements.rst:44
msgid ">= 5.9.0 [#iproute2_foot]_"
msgstr ""

#: ../../operations/system_requirements.rst:47
msgid ""
"Requires support for eBPF templating as documented :ref:`below "
"<iproute2_requirements>`."
msgstr ""

#: ../../operations/system_requirements.rst:51
msgid "Linux Distribution Compatibility Matrix"
msgstr ""

#: ../../operations/system_requirements.rst:53
msgid ""
"The following table lists Linux distributions that are known to work well with "
"Cilium."
msgstr ""

#: ../../operations/system_requirements.rst:57
msgid "Distribution"
msgstr ""

#: ../../operations/system_requirements.rst:59
msgid "`Amazon Linux 2`_"
msgstr ""

#: ../../operations/system_requirements.rst:59
#: ../../operations/system_requirements.rst:60
#: ../../operations/system_requirements.rst:64
#: ../../operations/system_requirements.rst:65
msgid "all"
msgstr ""

#: ../../operations/system_requirements.rst:60
msgid "`Container-Optimized OS`_"
msgstr ""

#: ../../operations/system_requirements.rst:61
msgid "`CentOS`_"
msgstr ""

#: ../../operations/system_requirements.rst:61
msgid ">= 7.0 [#centos_foot]_"
msgstr ""

#: ../../operations/system_requirements.rst:62
msgid "Debian_"
msgstr ""

#: ../../operations/system_requirements.rst:62
msgid ">= 9 Stretch"
msgstr ""

#: ../../operations/system_requirements.rst:63
msgid "`Fedora Atomic/Core`_"
msgstr ""

#: ../../operations/system_requirements.rst:63
msgid ">= 25"
msgstr ""

#: ../../operations/system_requirements.rst:64
msgid "Flatcar_"
msgstr ""

#: ../../operations/system_requirements.rst:65
msgid "LinuxKit_"
msgstr ""

#: ../../operations/system_requirements.rst:66
msgid "`RedHat Enterprise Linux`_"
msgstr ""

#: ../../operations/system_requirements.rst:66
msgid ">= 8.0"
msgstr ""

#: ../../operations/system_requirements.rst:67
msgid "Ubuntu_"
msgstr ""

#: ../../operations/system_requirements.rst:67
msgid ">= 16.04.1 (Azure), >= 16.04.2 (Canonical), >= 16.10"
msgstr ""

#: ../../operations/system_requirements.rst:68
msgid "Opensuse_"
msgstr ""

#: ../../operations/system_requirements.rst:68
msgid "Tumbleweed, >=Leap 15.0"
msgstr ""

#: ../../operations/system_requirements.rst:69
msgid "RancherOS_"
msgstr ""

#: ../../operations/system_requirements.rst:69
msgid ">= 1.5.5"
msgstr ""

#: ../../operations/system_requirements.rst:84
msgid ""
"CentOS 7 requires a third-party kernel provided by `ElRepo <http://elrepo.org/"
"tiki/tiki-index.php>`_ whereas CentOS 8 ships with a supported kernel."
msgstr ""

#: ../../operations/system_requirements.rst:87
msgid ""
"The above list is based on feedback by users. If you find an unlisted Linux "
"distribution that works well, please let us know by opening a GitHub issue or "
"by creating a pull request that updates this guide."
msgstr ""

#: ../../operations/system_requirements.rst:91
msgid ""
"Systemd 245 and above (``systemctl --version``) overrides ``rp_filter`` setting "
"of Cilium network interfaces. This introduces connectivity issues (see :gh-"
"issue:`10645` for details). To avoid that, configure ``rp_filter`` in systemd "
"using the following commands:"
msgstr ""

#: ../../operations/system_requirements.rst:107
msgid "Base Requirements"
msgstr ""

#: ../../operations/system_requirements.rst:109
msgid ""
"Cilium leverages and builds on the kernel eBPF functionality as well as various "
"subsystems which integrate with eBPF. Therefore, host systems are required to "
"run Linux kernel version 4.9.17 or later to run a Cilium agent. More recent "
"kernels may provide additional eBPF functionality that Cilium will "
"automatically detect and use on agent start."
msgstr ""

#: ../../operations/system_requirements.rst:115
msgid ""
"In order for the eBPF feature to be enabled properly, the following kernel "
"configuration options must be enabled. This is typically the case  with "
"distribution kernels. When an option can be built as a module or statically "
"linked, either choice is valid."
msgstr ""

#: ../../operations/system_requirements.rst:135
msgid ""
"Users running Linux 4.10 or earlier with Cilium CIDR policies may face :ref:"
"`cidr_limitations`."
msgstr ""

#: ../../operations/system_requirements.rst:139
msgid "Requirements for Iptables-based Masquerading"
msgstr ""

#: ../../operations/system_requirements.rst:141
msgid ""
"If you are not using BPF for masquerading (``enable-bpf-masquerade=false``, the "
"default value), then you will need the following kernel configuration options."
msgstr ""

#: ../../operations/system_requirements.rst:151
msgid "Requirements for L7 and FQDN Policies"
msgstr ""

#: ../../operations/system_requirements.rst:153
msgid ""
"L7 proxy redirection currently uses ``TPROXY`` iptables actions as well as "
"``socket`` matches. For L7 redirection to work as intended kernel configuration "
"must include the following modules:"
msgstr ""

#: ../../operations/system_requirements.rst:164
msgid ""
"When ``xt_socket`` kernel module is missing the forwarding of redirected L7 "
"traffic does not work in non-tunneled datapath modes. Since some notable "
"kernels (e.g., COS) are shipping without ``xt_socket`` module, Cilium "
"implements a fallback compatibility mode to allow L7 policies and visibility to "
"be used with those kernels. Currently this fallback disables ``ip_early_demux`` "
"kernel feature in non-tunneled datapath modes, which may decrease system "
"networking performance. This guarantees HTTP and Kafka redirection works as "
"intended.  However, if HTTP or Kafka enforcement policies or visibility "
"annotations are never used, this behavior can be turned off by adding the "
"following to the helm configuration command line:"
msgstr ""

#: ../../operations/system_requirements.rst:185
msgid "Requirements for IPsec"
msgstr ""

#: ../../operations/system_requirements.rst:187
msgid ""
"The :ref:`encryption_ipsec` feature requires a lot of kernel configuration "
"options, most of which to enable the actual encryption. Note that the specific "
"options required depend on the algorithm. The list below corresponds to "
"requirements for GMC-128-AES."
msgstr ""

#: ../../operations/system_requirements.rst:214
msgid "Requirements for the Bandwidth Manager"
msgstr ""

#: ../../operations/system_requirements.rst:216
msgid ""
"The :ref:`bandwidth-manager` requires the following kernel configuration option "
"to change the packet scheduling algorithm."
msgstr ""

#: ../../operations/system_requirements.rst:225
msgid "Required Kernel Versions for Advanced Features"
msgstr ""

#: ../../operations/system_requirements.rst:227
msgid ""
"Cilium requires Linux kernel 4.9.17 or higher; however, development on "
"additional kernel features continues to progress in the Linux community. Some "
"of Cilium's features are dependent on newer kernel versions and are thus "
"enabled by upgrading to more recent kernel versions as detailed below."
msgstr ""

#: ../../operations/system_requirements.rst:233
msgid "Cilium Feature"
msgstr ""

#: ../../operations/system_requirements.rst:233
msgid "Minimum Kernel Version"
msgstr ""

#: ../../operations/system_requirements.rst:235
msgid ":ref:`concepts_fragmentation`"
msgstr ""

#: ../../operations/system_requirements.rst:235
msgid ">= 4.10"
msgstr ""

#: ../../operations/system_requirements.rst:236
msgid ":ref:`cidr_limitations`"
msgstr ""

#: ../../operations/system_requirements.rst:236
msgid ">= 4.11"
msgstr ""

#: ../../operations/system_requirements.rst:237
msgid ":ref:`encryption_ipsec` in tunneling mode"
msgstr ""

#: ../../operations/system_requirements.rst:237
msgid ">= 4.19"
msgstr ""

#: ../../operations/system_requirements.rst:238
msgid ":ref:`encryption_wg`"
msgstr ""

#: ../../operations/system_requirements.rst:238
msgid ">= 5.6"
msgstr ""

#: ../../operations/system_requirements.rst:239
msgid ":ref:`host-services`"
msgstr ""

#: ../../operations/system_requirements.rst:239
#: ../../operations/system_requirements.rst:240
#: ../../operations/system_requirements.rst:242
msgid ">= 4.19.57, >= 5.1.16,  >= 5.2"
msgstr ""

#: ../../operations/system_requirements.rst:240
msgid ":ref:`kubeproxy-free`"
msgstr ""

#: ../../operations/system_requirements.rst:241
msgid ":ref:`bandwidth-manager`"
msgstr ""

#: ../../operations/system_requirements.rst:241
msgid ">= 5.1"
msgstr ""

#: ../../operations/system_requirements.rst:242
msgid ":ref:`local-redirect-policy`"
msgstr ""

#: ../../operations/system_requirements.rst:243
msgid "Full support for :ref:`session-affinity`"
msgstr ""

#: ../../operations/system_requirements.rst:243
#: ../../operations/system_requirements.rst:244
#: ../../operations/system_requirements.rst:246
msgid ">= 5.7"
msgstr ""

#: ../../operations/system_requirements.rst:244
msgid "BPF-based proxy redirection"
msgstr ""

#: ../../operations/system_requirements.rst:245
msgid "BPF-based host routing"
msgstr ""

#: ../../operations/system_requirements.rst:245
msgid ">= 5.10"
msgstr ""

#: ../../operations/system_requirements.rst:246
msgid "Socket-level LB bypass in pod netns"
msgstr ""

#: ../../operations/system_requirements.rst:247
msgid ":ref:`egress-gateway`"
msgstr ""

#: ../../operations/system_requirements.rst:247
#: ../../operations/system_requirements.rst:248
msgid ">= 5.2"
msgstr ""

#: ../../operations/system_requirements.rst:248
msgid "VXLAN Tunnel Endpoint (VTEP) Integration"
msgstr ""

#: ../../operations/system_requirements.rst:254
msgid "Key-Value store"
msgstr ""

#: ../../operations/system_requirements.rst:256
msgid ""
"Cilium optionally uses a distributed Key-Value store to manage, synchronize and "
"distribute security identities across all cluster nodes. The following Key-"
"Value stores are currently supported:"
msgstr ""

#: ../../operations/system_requirements.rst:260
msgid "etcd >= 3.1.0"
msgstr ""

#: ../../operations/system_requirements.rst:262
msgid ""
"Cilium can be used without a Key-Value store when CRD-based state management is "
"used with Kubernetes. This is the default for new Cilium installations. Larger "
"clusters will perform better with a Key-Value store backed identity management "
"instead, see :ref:`k8s_quick_install` for more details."
msgstr ""

#: ../../operations/system_requirements.rst:268
msgid ""
"See :ref:`install_kvstore` for details on how to configure the ``cilium-agent`` "
"to use a Key-Value store."
msgstr ""

#: ../../operations/system_requirements.rst:275
msgid ""
"This requirement is only needed if you run ``cilium-agent`` natively. If you "
"are using the Cilium container image ``cilium/cilium``, clang+LLVM is included "
"in the container image."
msgstr ""

#: ../../operations/system_requirements.rst:279
msgid ""
"LLVM is the compiler suite that Cilium uses to generate eBPF bytecode programs "
"to be loaded into the Linux kernel. The minimum supported version of LLVM "
"available to ``cilium-agent`` should be >=5.0. The version of clang installed "
"must be compiled with the eBPF backend enabled."
msgstr ""

#: ../../operations/system_requirements.rst:284
msgid ""
"See https://releases.llvm.org/ for information on how to download and install "
"LLVM."
msgstr ""

#: ../../operations/system_requirements.rst:292
msgid ""
"iproute2 is only needed if you run ``cilium-agent`` directly on the host "
"machine. iproute2 is included in the ``cilium/cilium`` container image."
msgstr ""

#: ../../operations/system_requirements.rst:296
msgid ""
"iproute2_ is a low level tool used to configure various networking related "
"subsystems of the Linux kernel. Cilium uses iproute2 to configure networking "
"and ``tc``, which is part of iproute2, to load eBPF programs into the kernel."
msgstr ""

#: ../../operations/system_requirements.rst:300
msgid ""
"The version of iproute2 must include the eBPF templating patches. Also, it "
"depends on Cilium's libbpf fork. See `Cilium iproute2 source`_ for more details."
msgstr ""

#: ../../operations/system_requirements.rst:308
msgid "Firewall Rules"
msgstr ""

#: ../../operations/system_requirements.rst:310
msgid ""
"If you are running Cilium in an environment that requires firewall rules to "
"enable connectivity, you will have to add the following rules to ensure Cilium "
"works properly."
msgstr ""

#: ../../operations/system_requirements.rst:312
msgid ""
"It is recommended but optional that all nodes running Cilium in a given cluster "
"must be able to ping each other so ``cilium-health`` can report and monitor "
"connectivity among nodes. This requires ICMP Type 0/8, Code 0 open among all "
"nodes. TCP 4240 should also be open among all nodes for ``cilium-health`` "
"monitoring. Note that it is also an option to only use one of these two methods "
"to enable health monitoring. If the firewall does not permit either of these "
"methods, Cilium will still operate fine but will not be able to provide health "
"information."
msgstr ""

#: ../../operations/system_requirements.rst:314
msgid ""
"If you are using VXLAN overlay network mode, Cilium uses Linux's default VXLAN "
"port 8472 over UDP, unless Linux has been configured otherwise. In this case, "
"UDP 8472 must be open among all nodes to enable VXLAN overlay mode. The same "
"applies to Geneve overlay network mode, except the port is UDP 6081."
msgstr ""

#: ../../operations/system_requirements.rst:316
msgid ""
"If you are running in direct routing mode, your network must allow routing of "
"pod IPs."
msgstr ""

#: ../../operations/system_requirements.rst:318
msgid ""
"As an example, if you are running on AWS with VXLAN overlay networking, here is "
"a minimum set of AWS Security Group (SG) rules. It assumes a separation between "
"the SG on the master nodes, ``master-sg``, and the worker nodes, ``worker-sg``. "
"It also assumes ``etcd`` is running on the master nodes."
msgstr ""

#: ../../operations/system_requirements.rst:320
msgid "Master Nodes (``master-sg``) Rules:"
msgstr ""

#: ../../operations/system_requirements.rst:323
#: ../../operations/system_requirements.rst:343
#: ../../operations/system_requirements.rst:368
msgid "Port Range / Protocol"
msgstr ""

#: ../../operations/system_requirements.rst:323
#: ../../operations/system_requirements.rst:343
msgid "Ingress/Egress"
msgstr ""

#: ../../operations/system_requirements.rst:323
#: ../../operations/system_requirements.rst:343
msgid "Source/Destination"
msgstr ""

#: ../../operations/system_requirements.rst:325
#: ../../operations/system_requirements.rst:357
msgid "2379-2380/tcp"
msgstr ""

#: ../../operations/system_requirements.rst:325
#: ../../operations/system_requirements.rst:326
#: ../../operations/system_requirements.rst:327
#: ../../operations/system_requirements.rst:328
#: ../../operations/system_requirements.rst:329
#: ../../operations/system_requirements.rst:330
#: ../../operations/system_requirements.rst:331
#: ../../operations/system_requirements.rst:345
#: ../../operations/system_requirements.rst:346
#: ../../operations/system_requirements.rst:347
#: ../../operations/system_requirements.rst:348
#: ../../operations/system_requirements.rst:349
#: ../../operations/system_requirements.rst:350
msgid "ingress"
msgstr ""

#: ../../operations/system_requirements.rst:325
#: ../../operations/system_requirements.rst:327
#: ../../operations/system_requirements.rst:329
#: ../../operations/system_requirements.rst:331
#: ../../operations/system_requirements.rst:333
#: ../../operations/system_requirements.rst:335
#: ../../operations/system_requirements.rst:337
msgid "``worker-sg``"
msgstr ""

#: ../../operations/system_requirements.rst:325
#: ../../operations/system_requirements.rst:357
msgid "etcd access"
msgstr ""

#: ../../operations/system_requirements.rst:326
#: ../../operations/system_requirements.rst:327
#: ../../operations/system_requirements.rst:332
#: ../../operations/system_requirements.rst:333
#: ../../operations/system_requirements.rst:345
#: ../../operations/system_requirements.rst:346
#: ../../operations/system_requirements.rst:351
#: ../../operations/system_requirements.rst:352
msgid "8472/udp"
msgstr ""

#: ../../operations/system_requirements.rst:326
#: ../../operations/system_requirements.rst:328
#: ../../operations/system_requirements.rst:330
#: ../../operations/system_requirements.rst:332
#: ../../operations/system_requirements.rst:334
#: ../../operations/system_requirements.rst:336
msgid "``master-sg`` (self)"
msgstr ""

#: ../../operations/system_requirements.rst:326
#: ../../operations/system_requirements.rst:327
#: ../../operations/system_requirements.rst:332
#: ../../operations/system_requirements.rst:333
#: ../../operations/system_requirements.rst:345
#: ../../operations/system_requirements.rst:346
#: ../../operations/system_requirements.rst:351
#: ../../operations/system_requirements.rst:352
msgid "VXLAN overlay"
msgstr ""

#: ../../operations/system_requirements.rst:328
#: ../../operations/system_requirements.rst:329
#: ../../operations/system_requirements.rst:334
#: ../../operations/system_requirements.rst:335
#: ../../operations/system_requirements.rst:347
#: ../../operations/system_requirements.rst:348
#: ../../operations/system_requirements.rst:353
#: ../../operations/system_requirements.rst:354
#: ../../operations/system_requirements.rst:370
msgid "4240/tcp"
msgstr ""

#: ../../operations/system_requirements.rst:328
#: ../../operations/system_requirements.rst:329
#: ../../operations/system_requirements.rst:330
#: ../../operations/system_requirements.rst:331
#: ../../operations/system_requirements.rst:334
#: ../../operations/system_requirements.rst:335
#: ../../operations/system_requirements.rst:336
#: ../../operations/system_requirements.rst:337
#: ../../operations/system_requirements.rst:347
#: ../../operations/system_requirements.rst:348
#: ../../operations/system_requirements.rst:349
#: ../../operations/system_requirements.rst:350
#: ../../operations/system_requirements.rst:353
#: ../../operations/system_requirements.rst:354
#: ../../operations/system_requirements.rst:355
#: ../../operations/system_requirements.rst:356
msgid "health checks"
msgstr ""

#: ../../operations/system_requirements.rst:330
#: ../../operations/system_requirements.rst:331
#: ../../operations/system_requirements.rst:336
#: ../../operations/system_requirements.rst:337
#: ../../operations/system_requirements.rst:349
#: ../../operations/system_requirements.rst:350
#: ../../operations/system_requirements.rst:355
#: ../../operations/system_requirements.rst:356
msgid "ICMP 8/0"
msgstr ""

#: ../../operations/system_requirements.rst:332
#: ../../operations/system_requirements.rst:333
#: ../../operations/system_requirements.rst:334
#: ../../operations/system_requirements.rst:335
#: ../../operations/system_requirements.rst:336
#: ../../operations/system_requirements.rst:337
#: ../../operations/system_requirements.rst:351
#: ../../operations/system_requirements.rst:352
#: ../../operations/system_requirements.rst:353
#: ../../operations/system_requirements.rst:354
#: ../../operations/system_requirements.rst:355
#: ../../operations/system_requirements.rst:356
#: ../../operations/system_requirements.rst:357
msgid "egress"
msgstr ""

#: ../../operations/system_requirements.rst:340
msgid "Worker Nodes (``worker-sg``):"
msgstr ""

#: ../../operations/system_requirements.rst:345
#: ../../operations/system_requirements.rst:347
#: ../../operations/system_requirements.rst:349
#: ../../operations/system_requirements.rst:351
#: ../../operations/system_requirements.rst:353
#: ../../operations/system_requirements.rst:355
#: ../../operations/system_requirements.rst:357
msgid "``master-sg``"
msgstr ""

#: ../../operations/system_requirements.rst:346
#: ../../operations/system_requirements.rst:348
#: ../../operations/system_requirements.rst:350
#: ../../operations/system_requirements.rst:352
#: ../../operations/system_requirements.rst:354
#: ../../operations/system_requirements.rst:356
msgid "``worker-sg`` (self)"
msgstr ""

#: ../../operations/system_requirements.rst:360
msgid ""
"If you use a shared SG for the masters and workers, you can condense these "
"rules into ingress/egress to self. If you are using Direct Routing mode, you "
"can condense all rules into ingress/egress ANY port/protocol to/from self."
msgstr ""

#: ../../operations/system_requirements.rst:365
msgid "The following ports should also be available on each node:"
msgstr ""

#: ../../operations/system_requirements.rst:370
msgid "cluster health checks (``cilium-health``)"
msgstr ""

#: ../../operations/system_requirements.rst:371
msgid "4244/tcp"
msgstr ""

#: ../../operations/system_requirements.rst:371
msgid "Hubble server"
msgstr ""

#: ../../operations/system_requirements.rst:372
msgid "4245/tcp"
msgstr ""

#: ../../operations/system_requirements.rst:372
msgid "Hubble Relay"
msgstr ""

#: ../../operations/system_requirements.rst:373
msgid "6060/tcp"
msgstr ""

#: ../../operations/system_requirements.rst:373
msgid "cilium-agent pprof server (listening on 127.0.0.1)"
msgstr ""

#: ../../operations/system_requirements.rst:374
msgid "6061/tcp"
msgstr ""

#: ../../operations/system_requirements.rst:374
msgid "cilium-operator pprof server (listening on 127.0.0.1)"
msgstr ""

#: ../../operations/system_requirements.rst:375
msgid "6062/tcp"
msgstr ""

#: ../../operations/system_requirements.rst:375
msgid "Hubble Relay pprof server (listening on 127.0.0.1)"
msgstr ""

#: ../../operations/system_requirements.rst:376
msgid "6942/tcp"
msgstr ""

#: ../../operations/system_requirements.rst:376
msgid "operator Prometheus metrics"
msgstr ""

#: ../../operations/system_requirements.rst:377
msgid "9090/tcp"
msgstr ""

#: ../../operations/system_requirements.rst:377
msgid "cilium-agent Prometheus metrics"
msgstr ""

#: ../../operations/system_requirements.rst:378
msgid "9879/tcp"
msgstr ""

#: ../../operations/system_requirements.rst:378
msgid "cilium-agent health status API (listening on 127.0.0.1 and/or ::1)"
msgstr ""

#: ../../operations/system_requirements.rst:379
msgid "9890/tcp"
msgstr ""

#: ../../operations/system_requirements.rst:379
msgid "cilium-agent gops server (listening on 127.0.0.1)"
msgstr ""

#: ../../operations/system_requirements.rst:380
msgid "9891/tcp"
msgstr ""

#: ../../operations/system_requirements.rst:380
msgid "operator gops server (listening on 127.0.0.1)"
msgstr ""

#: ../../operations/system_requirements.rst:381
msgid "9892/tcp"
msgstr ""

#: ../../operations/system_requirements.rst:381
msgid "clustermesh-apiserver gops server (listening on 127.0.0.1)"
msgstr ""

#: ../../operations/system_requirements.rst:382
msgid "9893/tcp"
msgstr ""

#: ../../operations/system_requirements.rst:382
msgid "Hubble Relay gops server (listening on 127.0.0.1)"
msgstr ""

#: ../../operations/system_requirements.rst:383
msgid "51871/udp"
msgstr ""

#: ../../operations/system_requirements.rst:383
msgid "WireGuard encryption tunnel endpoint"
msgstr ""

#: ../../operations/system_requirements.rst:389
msgid "Mounted eBPF filesystem"
msgstr ""

#: ../../operations/system_requirements.rst:393
msgid ""
"Some distributions mount the bpf filesystem automatically. Check if the bpf "
"filesystem is mounted by running the command."
msgstr ""

#: ../../operations/system_requirements.rst:401
msgid ""
"If the eBPF filesystem is not mounted in the host filesystem, Cilium will "
"automatically mount the filesystem."
msgstr ""

#: ../../operations/system_requirements.rst:404
msgid ""
"Mounting this BPF filesystem allows the ``cilium-agent`` to persist eBPF "
"resources across restarts of the agent so that the datapath can continue to "
"operate while the agent is subsequently restarted or upgraded."
msgstr ""

#: ../../operations/system_requirements.rst:408
msgid ""
"Optionally it is also possible to mount the eBPF filesystem before Cilium is "
"deployed in the cluster, the following command must be run in the host mount "
"namespace. The command must only be run once during the boot process of the "
"machine."
msgstr ""

#: ../../operations/system_requirements.rst:417
msgid ""
"A portable way to achieve this with persistence is to add the following line to "
"``/etc/fstab`` and then run ``mount /sys/fs/bpf``. This will cause the "
"filesystem to be automatically mounted when the node boots."
msgstr ""

#: ../../operations/system_requirements.rst:425
msgid ""
"If you are using systemd to manage the kubelet, see the section :ref:"
"`bpffs_systemd`."
msgstr ""

#: ../../operations/system_requirements.rst:429
msgid "Privileges"
msgstr ""

#: ../../operations/system_requirements.rst:431
msgid ""
"The following privileges are required to run Cilium. When running the standard "
"Kubernetes :term:`DaemonSet`, the privileges are automatically granted to "
"Cilium."
msgstr ""

#: ../../operations/system_requirements.rst:434
msgid ""
"Cilium interacts with the Linux kernel to install eBPF program which will then "
"perform networking tasks and implement security rules. In order to install eBPF "
"programs system-wide, ``CAP_SYS_ADMIN`` privileges are required. These "
"privileges must be granted to ``cilium-agent``."
msgstr ""

#: ../../operations/system_requirements.rst:439
msgid ""
"The quickest way to meet the requirement is to run ``cilium-agent`` as root and/"
"or as privileged container."
msgstr ""

#: ../../operations/system_requirements.rst:442
msgid ""
"Cilium requires access to the host networking namespace. For this purpose, the "
"Cilium pod is scheduled to run in the host networking namespace directly."
msgstr ""

#: ../../operations/troubleshooting.rst:11
msgid "Troubleshooting"
msgstr ""

#: ../../operations/troubleshooting.rst:13
msgid ""
"This document describes how to troubleshoot Cilium in different deployment "
"modes. It focuses on a full deployment of Cilium within a datacenter or public "
"cloud. If you are just looking for a simple way to experiment, we highly "
"recommend trying out the :ref:`gs_guide` instead."
msgstr ""

#: ../../operations/troubleshooting.rst:18
msgid ""
"This guide assumes that you have read the :ref:`concepts` which explains all "
"the components and concepts."
msgstr ""

#: ../../operations/troubleshooting.rst:21
msgid ""
"We use GitHub issues to maintain a list of `Cilium Frequently Asked Questions "
"(FAQ)`_. You can also check there to see if your question(s) is already "
"addressed."
msgstr ""

#: ../../operations/troubleshooting.rst:26
msgid "Component & Cluster Health"
msgstr ""

#: ../../operations/troubleshooting.rst:31
msgid ""
"An initial overview of Cilium can be retrieved by listing all pods to verify "
"whether all pods have the status ``Running``:"
msgstr ""

#: ../../operations/troubleshooting.rst:43
msgid ""
"If Cilium encounters a problem that it cannot recover from, it will "
"automatically report the failure state via ``cilium status`` which is regularly "
"queried by the Kubernetes liveness probe to automatically restart Cilium pods. "
"If a Cilium pod is in state ``CrashLoopBackoff`` then this indicates a "
"permanent failure scenario."
msgstr ""

#: ../../operations/troubleshooting.rst:50
msgid "Detailed Status"
msgstr ""

#: ../../operations/troubleshooting.rst:52
msgid ""
"If a particular Cilium pod is not in running state, the status and health of "
"the agent on that node can be retrieved by running ``cilium status`` in the "
"context of that pod:"
msgstr ""

#: ../../operations/troubleshooting.rst:70
msgid ""
"Alternatively, the ``k8s-cilium-exec.sh`` script can be used to run ``cilium "
"status`` on all nodes. This will provide detailed status and health information "
"of all nodes in the cluster:"
msgstr ""

#: ../../operations/troubleshooting.rst:79
msgid "... and run ``cilium status`` on all nodes:"
msgstr ""

#: ../../operations/troubleshooting.rst:95
msgid ""
"Detailed information about the status of Cilium can be inspected with the "
"``cilium status --verbose`` command. Verbose output includes detailed IPAM "
"state (allocated addresses), Cilium controller status, and details of the Proxy "
"status."
msgstr ""

#: ../../operations/troubleshooting.rst:103
msgid "Logs"
msgstr ""

#: ../../operations/troubleshooting.rst:105
msgid ""
"To retrieve log files of a cilium pod, run (replace ``cilium-1234`` with a pod "
"name returned by ``kubectl -n kube-system get pods -l k8s-app=cilium``)"
msgstr ""

#: ../../operations/troubleshooting.rst:112
msgid ""
"If the cilium pod was already restarted due to the liveness problem after "
"encountering an issue, it can be useful to retrieve the logs of the pod before "
"the last restart:"
msgstr ""

#: ../../operations/troubleshooting.rst:121
#: ../../operations/troubleshooting.rst:657
msgid "Generic"
msgstr ""

#: ../../operations/troubleshooting.rst:123
msgid ""
"When logged in a host running Cilium, the cilium CLI can be invoked directly, e."
"g.:"
msgstr ""

#: ../../operations/troubleshooting.rst:146
msgid "Observing Flows with Hubble"
msgstr ""

#: ../../operations/troubleshooting.rst:148
msgid ""
"Hubble is a built-in observability tool which allows you to inspect recent flow "
"events on all endpoints managed by Cilium."
msgstr ""

#: ../../operations/troubleshooting.rst:152
msgid "Ensure Hubble is running correctly"
msgstr ""

#: ../../operations/troubleshooting.rst:154
msgid ""
"To ensure the Hubble client can connect to the Hubble server running inside "
"Cilium, you may use the ``hubble status`` command from within a Cilium pod:"
msgstr ""

#: ../../operations/troubleshooting.rst:164
msgid ""
"``cilium-agent`` must be running with the ``--enable-hubble`` option (default) "
"in order for the Hubble server to be enabled. When deploying Cilium with Helm, "
"make sure to set the ``hubble.enabled=true`` value."
msgstr ""

#: ../../operations/troubleshooting.rst:168
msgid ""
"To check if Hubble is enabled in your deployment, you may look for the "
"following output in ``cilium status``:"
msgstr ""

#: ../../operations/troubleshooting.rst:179
msgid ""
"Pods need to be managed by Cilium in order to be observable by Hubble. See how "
"to :ref:`ensure a pod is managed by Cilium<ensure_managed_pod>` for more "
"details."
msgstr ""

#: ../../operations/troubleshooting.rst:184
msgid "Observing flows of a specific pod"
msgstr ""

#: ../../operations/troubleshooting.rst:186
msgid ""
"In order to observe the traffic of a specific pod, you will first have to :ref:"
"`retrieve the name of the cilium instance managing it<retrieve_cilium_pod>`. "
"The Hubble CLI is part of the Cilium container image and can be accessed via "
"``kubectl exec``. The following query for example will show all events related "
"to flows which either originated or terminated in the ``default/tiefighter`` "
"pod in the last three minutes:"
msgstr ""

#: ../../operations/troubleshooting.rst:209
msgid ""
"You may also use ``-o json`` to obtain more detailed information about each "
"flow event."
msgstr ""

#: ../../operations/troubleshooting.rst:213
msgid ""
"**Hubble Relay**  allows you to query multiple Hubble instances simultaneously "
"without having to first manually target a specific node.  See `Observing flows "
"with Hubble Relay`_ for more information."
msgstr ""

#: ../../operations/troubleshooting.rst:218
msgid "Observing flows with Hubble Relay"
msgstr ""

#: ../../operations/troubleshooting.rst:220
msgid ""
"Hubble Relay is a service which allows to query multiple Hubble instances "
"simultaneously and aggregate the results. See :ref:`hubble_setup` to enable "
"Hubble Relay if it is not yet enabled and install the Hubble CLI on your local "
"machine."
msgstr ""

#: ../../operations/troubleshooting.rst:225
msgid "You may access the Hubble Relay service by port-forwarding it locally:"
msgstr ""

#: ../../operations/troubleshooting.rst:231
msgid ""
"This will forward the Hubble Relay service port (``80``) to your local machine "
"on port ``4245`` on all of it's IP addresses."
msgstr ""

#: ../../operations/troubleshooting.rst:234
msgid ""
"You can verify that Hubble Relay can be reached by using the Hubble CLI and "
"running the following command from your local machine:"
msgstr ""

#: ../../operations/troubleshooting.rst:241
msgid "This command should return an output similar to the following:"
msgstr ""

#: ../../operations/troubleshooting.rst:250
msgid ""
"You may see details about nodes that Hubble Relay is connected to by running "
"the following command:"
msgstr ""

#: ../../operations/troubleshooting.rst:257
msgid ""
"As Hubble Relay shares the same API as individual Hubble instances, you may "
"follow the `Observing flows with Hubble`_ section keeping in mind that "
"limitations with regards to what can be seen from individual Hubble instances "
"no longer apply."
msgstr ""

#: ../../operations/troubleshooting.rst:263
msgid "Connectivity Problems"
msgstr ""

#: ../../operations/troubleshooting.rst:266
msgid "Cilium connectivity tests"
msgstr ""

#: ../../operations/troubleshooting.rst:268
msgid ""
"The Cilium connectivity test deploys a series of services, deployments, and "
"CiliumNetworkPolicy which will use various connectivity paths to connect to "
"each other. Connectivity paths include with and without service load-balancing "
"and various network policy combinations."
msgstr ""

#: ../../operations/troubleshooting.rst:274
msgid ""
"The connectivity tests this will only work in a namespace with no other pods or "
"network policies applied. If there is a Cilium Clusterwide Network Policy "
"enabled, that may also break this connectivity check."
msgstr ""

#: ../../operations/troubleshooting.rst:278
msgid ""
"To run the connectivity tests create an isolated test namespace called ``cilium-"
"test`` to deploy the tests with."
msgstr ""

#: ../../operations/troubleshooting.rst:286
msgid ""
"The tests cover various functionality of the system. Below we call out each "
"test type. If tests pass, it suggests functionality of the referenced subsystem."
msgstr ""

#: ../../operations/troubleshooting.rst:290
msgid "Pod-to-pod (intra-host)"
msgstr ""

#: ../../operations/troubleshooting.rst:290
msgid "Pod-to-pod (inter-host)"
msgstr ""

#: ../../operations/troubleshooting.rst:290
msgid "Pod-to-service (intra-host)"
msgstr ""

#: ../../operations/troubleshooting.rst:290
msgid "Pod-to-service (inter-host)"
msgstr ""

#: ../../operations/troubleshooting.rst:290
msgid "Pod-to-external resource"
msgstr ""

#: ../../operations/troubleshooting.rst:292
msgid "eBPF routing is functional"
msgstr ""

#: ../../operations/troubleshooting.rst:292
msgid "Data plane, routing, network"
msgstr ""

#: ../../operations/troubleshooting.rst:292
msgid "eBPF service map lookup"
msgstr ""

#: ../../operations/troubleshooting.rst:292
msgid "VXLAN overlay port if used"
msgstr ""

#: ../../operations/troubleshooting.rst:292
msgid "Egress, CiliumNetworkPolicy, masquerade"
msgstr ""

#: ../../operations/troubleshooting.rst:295
msgid ""
"The pod name indicates the connectivity variant and the readiness and liveness "
"gate indicates success or failure of the test:"
msgstr ""

#: ../../operations/troubleshooting.rst:318
msgid ""
"Information about test failures can be determined by describing a failed test "
"pod"
msgstr ""

#: ../../operations/troubleshooting.rst:330
msgid "Checking cluster connectivity health"
msgstr ""

#: ../../operations/troubleshooting.rst:332
msgid ""
"Cilium can rule out network fabric related issues when troubleshooting "
"connectivity issues by providing reliable health and latency probes between all "
"cluster nodes and a simulated workload running on each node."
msgstr ""

#: ../../operations/troubleshooting.rst:336
msgid ""
"By default when Cilium is run, it launches instances of ``cilium-health`` in "
"the background to determine the overall connectivity status of the cluster. "
"This tool periodically runs bidirectional traffic across multiple paths through "
"the cluster and through each node using different protocols to determine the "
"health status of each path and protocol. At any point in time, cilium-health "
"may be queried for the connectivity status of the last probe."
msgstr ""

#: ../../operations/troubleshooting.rst:363
msgid ""
"For each node, the connectivity will be displayed for each protocol and path, "
"both to the node itself and to an endpoint on that node. The latency specified "
"is a snapshot at the last time a probe was run, which is typically once per "
"minute. The ICMP connectivity row represents Layer 3 connectivity to the "
"networking stack, while the HTTP connectivity row represents connection to an "
"instance of the ``cilium-health`` agent running on the host or as an endpoint."
msgstr ""

#: ../../operations/troubleshooting.rst:373
msgid "Monitoring Datapath State"
msgstr ""

#: ../../operations/troubleshooting.rst:375
msgid ""
"Sometimes you may experience broken connectivity, which may be due to a number "
"of different causes. A main cause can be unwanted packet drops on the "
"networking level. The tool ``cilium monitor`` allows you to quickly inspect and "
"see if and where packet drops happen. Following is an example output (use "
"``kubectl exec`` as in previous examples if running with Kubernetes):"
msgstr ""

#: ../../operations/troubleshooting.rst:393
msgid ""
"The above indicates that a packet to endpoint ID ``25729`` has been dropped due "
"to violation of the Layer 3 policy."
msgstr ""

#: ../../operations/troubleshooting.rst:397
msgid "Handling drop (CT: Map insertion failed)"
msgstr ""

#: ../../operations/troubleshooting.rst:399
msgid ""
"If connectivity fails and ``cilium monitor --type drop`` shows ``xx drop (CT: "
"Map insertion failed)``, then it is likely that the connection tracking table "
"is filling up and the automatic adjustment of the garbage collector interval is "
"insufficient. Set ``--conntrack-gc-interval`` to an interval lower than the "
"default.  Alternatively, the value for ``bpf-ct-global-any-max`` and ``bpf-ct-"
"global-tcp-max`` can be increased. Setting both of these options will be a "
"trade-off of CPU for ``conntrack-gc-interval``, and for ``bpf-ct-global-any-"
"max`` and ``bpf-ct-global-tcp-max`` the amount of memory consumed."
msgstr ""

#: ../../operations/troubleshooting.rst:410
msgid "Enabling datapath debug messages"
msgstr ""

#: ../../operations/troubleshooting.rst:412
msgid ""
"By default, datapath debug messages are disabled, and therefore not shown in "
"``cilium monitor -v`` output. To enable them, add ``\"datapath\"`` to the "
"``debug-verbose`` option."
msgstr ""

#: ../../operations/troubleshooting.rst:417
msgid "Policy Troubleshooting"
msgstr ""

#: ../../operations/troubleshooting.rst:422
msgid "Ensure pod is managed by Cilium"
msgstr ""

#: ../../operations/troubleshooting.rst:424
msgid ""
"A potential cause for policy enforcement not functioning as expected is that "
"the networking of the pod selected by the policy is not being managed by "
"Cilium. The following situations result in unmanaged pods:"
msgstr ""

#: ../../operations/troubleshooting.rst:428
msgid ""
"The pod is running in host networking and will use the host's IP address "
"directly. Such pods have full network connectivity but Cilium will not provide "
"security policy enforcement for such pods."
msgstr ""

#: ../../operations/troubleshooting.rst:432
msgid ""
"The pod was started before Cilium was deployed. Cilium only manages pods that "
"have been deployed after Cilium itself was started. Cilium will not provide "
"security policy enforcement for such pods."
msgstr ""

#: ../../operations/troubleshooting.rst:436
msgid ""
"If pod networking is not managed by Cilium. Ingress and egress policy rules "
"selecting the respective pods will not be applied. See the section :ref:"
"`network_policy` for more details."
msgstr ""

#: ../../operations/troubleshooting.rst:440
msgid ""
"For a quick assessment of whether any pods are not managed by Cilium, the "
"`Cilium CLI <https://github.com/cilium/cilium-cli>`_ will print the number of "
"managed pods. If this prints that all of the pods are managed by Cilium, then "
"there is no problem:"
msgstr ""

#: ../../operations/troubleshooting.rst:466
msgid ""
"You can run the following script to list the pods which are *not* managed by "
"Cilium:"
msgstr ""

#: ../../operations/troubleshooting.rst:481
msgid "Understand the rendering of your policy"
msgstr ""

#: ../../operations/troubleshooting.rst:483
msgid ""
"There are always multiple ways to approach a problem. Cilium can provide the "
"rendering of the aggregate policy provided to it, leaving you to simply compare "
"with what you expect the policy to actually be rather than search (and "
"potentially overlook) every policy. At the expense of reading a very large dump "
"of an endpoint, this is often a faster path to discovering errant policy "
"requests in the Kubernetes API."
msgstr ""

#: ../../operations/troubleshooting.rst:490
msgid ""
"Start by finding the endpoint you are debugging from the following list. There "
"are several cross references for you to use in this list, including the IP "
"address and pod labels:"
msgstr ""

#: ../../operations/troubleshooting.rst:498
msgid ""
"When you find the correct endpoint, the first column of every row is the "
"endpoint ID. Use that to dump the full endpoint information:"
msgstr ""

#: ../../operations/troubleshooting.rst:508
msgid ""
"Importing this dump into a JSON-friendly editor can help browse and navigate "
"the information here. At the top level of the dump, there are two nodes of note:"
msgstr ""

#: ../../operations/troubleshooting.rst:511
msgid "``spec``: The desired state of the endpoint"
msgstr ""

#: ../../operations/troubleshooting.rst:512
msgid "``status``: The current state of the endpoint"
msgstr ""

#: ../../operations/troubleshooting.rst:514
msgid ""
"This is the standard Kubernetes control loop pattern. Cilium is the controller "
"here, and it is iteratively working to bring the ``status`` in line with the "
"``spec``."
msgstr ""

#: ../../operations/troubleshooting.rst:518
msgid ""
"Opening the ``status``, we can drill down through ``policy.realized.l4``. Do "
"your ``ingress`` and ``egress`` rules match what you expect? If not, the "
"reference to the errant rules can be found in the ``derived-from-rules`` node."
msgstr ""

#: ../../operations/troubleshooting.rst:523
msgid "etcd (kvstore)"
msgstr ""

#: ../../operations/troubleshooting.rst:528
msgid ""
"Cilium can be operated in CRD-mode and kvstore/etcd mode. When cilium is "
"running in kvstore/etcd mode, the kvstore becomes a vital component of the "
"overall cluster health as it is required to be available for several operations."
msgstr ""

#: ../../operations/troubleshooting.rst:533
msgid ""
"Operations for which the kvstore is strictly required when running in etcd mode:"
msgstr ""

#: ../../operations/troubleshooting.rst:542
msgid "Scheduling of new workloads:"
msgstr ""

#: ../../operations/troubleshooting.rst:537
msgid ""
"As part of scheduling workloads/endpoints, agents will perform security "
"identity allocation which requires interaction with the kvstore. If a workload "
"can be scheduled due to re-using a known security identity, then state "
"propagation of the endpoint details to other nodes will still depend on the "
"kvstore and thus packets drops due to policy enforcement may be observed as "
"other nodes in the cluster will not be aware of the new workload."
msgstr ""

#: ../../operations/troubleshooting.rst:545
msgid "Multi cluster:"
msgstr ""

#: ../../operations/troubleshooting.rst:545
msgid "All state propagation between clusters depends on the kvstore."
msgstr ""

#: ../../operations/troubleshooting.rst:548
msgid "Node discovery:"
msgstr ""

#: ../../operations/troubleshooting.rst:548
msgid "New nodes require to register themselves in the kvstore."
msgstr ""

#: ../../operations/troubleshooting.rst:553
msgid "Agent bootstrap:"
msgstr ""

#: ../../operations/troubleshooting.rst:551
msgid ""
"The Cilium agent will eventually fail if it can't connect to the kvstore at "
"bootstrap time, however, the agent will still perform all possible operations "
"while waiting for the kvstore to appear."
msgstr ""

#: ../../operations/troubleshooting.rst:555
msgid "Operations which *do not* require kvstore availability:"
msgstr ""

#: ../../operations/troubleshooting.rst:567
msgid "All datapath operations:"
msgstr ""

#: ../../operations/troubleshooting.rst:558
msgid ""
"All datapath forwarding, policy enforcement and visibility functions for "
"existing workloads/endpoints do not depend on the kvstore. Packets will "
"continue to be forwarded and network policy rules will continue to be enforced."
msgstr ""

#: ../../operations/troubleshooting.rst:563
msgid ""
"However, if the agent requires to restart as part of the :ref:"
"`etcd_recovery_behavior`, there can be delays in:"
msgstr ""

#: ../../operations/troubleshooting.rst:566
msgid "processing of flow events and metrics"
msgstr ""

#: ../../operations/troubleshooting.rst:567
msgid "short unavailability of layer 7 proxies"
msgstr ""

#: ../../operations/troubleshooting.rst:570
msgid "NetworkPolicy updates:"
msgstr ""

#: ../../operations/troubleshooting.rst:570
msgid "Network policy updates will continue to be processed and applied."
msgstr ""

#: ../../operations/troubleshooting.rst:573
msgid "Services updates:"
msgstr ""

#: ../../operations/troubleshooting.rst:573
msgid "All updates to services will be processed and applied."
msgstr ""

#: ../../operations/troubleshooting.rst:576
msgid "Understanding etcd status"
msgstr ""

#: ../../operations/troubleshooting.rst:578
msgid ""
"The etcd status is reported when running ``cilium status``. The following line "
"represents the status of etcd::"
msgstr ""

#: ../../operations/troubleshooting.rst:584
msgid "OK:"
msgstr ""

#: ../../operations/troubleshooting.rst:584
msgid "The overall status. Either ``OK`` or ``Failure``."
msgstr ""

#: ../../operations/troubleshooting.rst:587
msgid "1/1 connected:"
msgstr ""

#: ../../operations/troubleshooting.rst:587
msgid "Number of total etcd endpoints and how many of them are reachable."
msgstr ""

#: ../../operations/troubleshooting.rst:590
msgid "lease-ID:"
msgstr ""

#: ../../operations/troubleshooting.rst:590
msgid "UUID of the lease used for all keys owned by this agent."
msgstr ""

#: ../../operations/troubleshooting.rst:593
msgid "lock lease-ID:"
msgstr ""

#: ../../operations/troubleshooting.rst:593
msgid "UUID of the lease used for locks acquired by this agent."
msgstr ""

#: ../../operations/troubleshooting.rst:596
msgid "has-quorum:"
msgstr ""

#: ../../operations/troubleshooting.rst:596
msgid "Status of etcd quorum. Either ``true`` or set to an error."
msgstr ""

#: ../../operations/troubleshooting.rst:599
msgid "consecutive-errors:"
msgstr ""

#: ../../operations/troubleshooting.rst:599
msgid "Number of consecutive quorum errors. Only printed if errors are present."
msgstr ""

#: ../../operations/troubleshooting.rst:604
msgid "https://192.168.60.11:2379 - 3.4.9 (Leader):"
msgstr ""

#: ../../operations/troubleshooting.rst:602
msgid ""
"List of all etcd endpoints stating the etcd version and whether the particular "
"endpoint is currently the elected leader. If an etcd endpoint cannot be "
"reached, the error is shown."
msgstr ""

#: ../../operations/troubleshooting.rst:609
msgid "Recovery behavior"
msgstr ""

#: ../../operations/troubleshooting.rst:611
msgid ""
"In the event of an etcd endpoint becoming unhealthy, etcd should automatically "
"resolve this by electing a new leader and by failing over to a healthy etcd "
"endpoint. As long as quorum is preserved, the etcd cluster will remain "
"functional."
msgstr ""

#: ../../operations/troubleshooting.rst:616
msgid ""
"In addition, Cilium performs a background check in an interval to determine "
"etcd health and potentially take action. The interval depends on the overall "
"cluster size. The larger the cluster, the longer the `interval <https://pkg.go."
"dev/github.com/cilium/cilium/pkg/kvstore?tab=doc#ExtraOptions."
"StatusCheckInterval>`_:"
msgstr ""

#: ../../operations/troubleshooting.rst:621
msgid ""
"If no etcd endpoints can be reached, Cilium will report failure in ``cilium "
"status``. This will cause the liveness and readiness probe of Kubernetes to "
"fail and Cilium will be restarted."
msgstr ""

#: ../../operations/troubleshooting.rst:625
msgid ""
"A lock is acquired and released to test a write operation which requires "
"quorum. If this operation fails, loss of quorum is reported. If quorum fails "
"for three or more intervals in a row, Cilium is declared unhealthy."
msgstr ""

#: ../../operations/troubleshooting.rst:629
msgid ""
"The Cilium operator will constantly write to a heartbeat key (``cilium/."
"heartbeat``). All Cilium agents will watch for updates to this heartbeat key. "
"This validates the ability for an agent to receive key updates from etcd. If "
"the heartbeat key is not updated in time, the quorum check is declared to have "
"failed and Cilium is declared unhealthy after 3 or more consecutive failures."
msgstr ""

#: ../../operations/troubleshooting.rst:636
msgid ""
"Example of a status with a quorum failure which has not yet reached the "
"threshold::"
msgstr ""

#: ../../operations/troubleshooting.rst:641
msgid ""
"Example of a status with the number of quorum failures exceeding the threshold::"
msgstr ""

#: ../../operations/troubleshooting.rst:648
msgid "Cluster Mesh Troubleshooting"
msgstr ""

#: ../../operations/troubleshooting.rst:652
msgid "Install the Cilium CLI"
msgstr ""

#: ../../gettingstarted/cli-download.rst:1
msgid ""
"Install the latest version of the Cilium CLI. The Cilium CLI can be used to "
"install Cilium, inspect the state of a Cilium installation, and enable/disable "
"various features (e.g. clustermesh, Hubble)."
msgstr ""

#: ../../gettingstarted/cli-download.rst:6
msgid "Linux"
msgstr ""

#: ../../gettingstarted/cli-download.rst:15
msgid "macOS"
msgstr ""

#: ../../gettingstarted/cli-download.rst:24
msgid "Other"
msgstr ""

#: ../../gettingstarted/cli-download.rst:26
msgid ""
"See the full page of `releases <https://github.com/cilium/cilium-cli/releases/"
"latest>`_."
msgstr ""

#: ../../operations/troubleshooting.rst:659
msgid ""
"Validate that the ``cilium-xxx`` as well as the ``cilium-operator-xxx`` pods "
"are healthy and ready."
msgstr ""

#: ../../operations/troubleshooting.rst:666
msgid "Validate the Cluster Mesh is enabled correctly and operational:"
msgstr ""

#: ../../operations/troubleshooting.rst:674
msgid "Manual Verification of Setup"
msgstr ""

#: ../../operations/troubleshooting.rst:676
msgid ""
"Validate that the ClusterMesh subsystem is initialized by looking for a "
"``cilium-agent`` log message like this::"
msgstr ""

#: ../../operations/troubleshooting.rst:681
msgid ""
"Validate that the configuration for remote clusters is picked up correctly. For "
"each remote cluster, an info log message ``New remote cluster configuration`` "
"along with the remote cluster name must be logged in the ``cilium-agent`` logs."
msgstr ""

#: ../../operations/troubleshooting.rst:686
msgid "If the configuration is not found, check the following:"
msgstr ""

#: ../../operations/troubleshooting.rst:688
msgid "The Kubernetes secret ``clustermesh-secrets`` is imported correctly."
msgstr ""

#: ../../operations/troubleshooting.rst:690
msgid ""
"The secret contains a file for each remote cluster with the filename matching "
"the name of the remote cluster."
msgstr ""

#: ../../operations/troubleshooting.rst:693
msgid ""
"The contents of the file in the secret is a valid etcd configuration consisting "
"of the IP to reach the remote etcd as well as the required certificates to "
"connect to that etcd."
msgstr ""

#: ../../operations/troubleshooting.rst:697
msgid ""
"Run a ``kubectl exec -ti [...] -- bash`` in one of the Cilium pods and check "
"the contents of the directory ``/var/lib/cilium/clustermesh/``. It must contain "
"a configuration file for each remote cluster along with all the required SSL "
"certificates and keys. The filenames must match the cluster names as provided "
"by the ``--cluster-name`` argument or ``cluster-name`` ConfigMap option. If the "
"directory is empty or incomplete, regenerate the secret again and ensure that "
"the secret is correctly mounted into the DaemonSet."
msgstr ""

#: ../../operations/troubleshooting.rst:706
msgid ""
"Validate that the connection to the remote cluster could be established. You "
"will see a log message like this in the ``cilium-agent`` logs for each remote "
"cluster::"
msgstr ""

#: ../../operations/troubleshooting.rst:712
msgid "If the connection failed, you will see a warning like this::"
msgstr ""

#: ../../operations/troubleshooting.rst:716
msgid "If the connection fails, check the following:"
msgstr ""

#: ../../operations/troubleshooting.rst:718
msgid ""
"Validate that the ``hostAliases`` section in the Cilium DaemonSet maps each "
"remote cluster to the IP of the LoadBalancer that makes the remote control "
"plane available."
msgstr ""

#: ../../operations/troubleshooting.rst:722
msgid ""
"Validate that a local node in the source cluster can reach the IP specified in "
"the ``hostAliases`` section. The ``clustermesh-secrets`` secret contains a "
"configuration file for each remote cluster, it will point to a logical name "
"representing the remote cluster:"
msgstr ""

#: ../../operations/troubleshooting.rst:732
msgid ""
"The name will *NOT* be resolvable via DNS outside of the cilium pod. The name "
"is mapped to an IP using ``hostAliases``. Run ``kubectl -n kube-system get ds "
"cilium -o yaml`` and grep for the FQDN to retrieve the IP that is configured. "
"Then use ``curl`` to validate that the port is reachable."
msgstr ""

#: ../../operations/troubleshooting.rst:738
msgid ""
"A firewall between the local cluster and the remote cluster may drop the "
"control plane connection. Ensure that port 2379/TCP is allowed."
msgstr ""

#: ../../operations/troubleshooting.rst:742
msgid "State Propagation"
msgstr ""

#: ../../operations/troubleshooting.rst:744
msgid ""
"Run ``cilium node list`` in one of the Cilium pods and validate that it lists "
"both local nodes and nodes from remote clusters. If this discovery does not "
"work, validate the following:"
msgstr ""

#: ../../operations/troubleshooting.rst:748
msgid ""
"In each cluster, check that the kvstore contains information about *local* "
"nodes by running:"
msgstr ""

#: ../../operations/troubleshooting.rst:757
msgid ""
"The kvstore will only contain nodes of the **local cluster**. It will **not** "
"contain nodes of remote clusters. The state in the kvstore is used for other "
"clusters to discover all nodes so it is important that local nodes are listed."
msgstr ""

#: ../../operations/troubleshooting.rst:762
msgid ""
"Validate the connectivity health matrix across clusters by running ``cilium-"
"health status`` inside any Cilium pod. It will list the status of the "
"connectivity health check to each remote node."
msgstr ""

#: ../../operations/troubleshooting.rst:766
#: ../../operations/troubleshooting.rst:776
#: ../../operations/troubleshooting.rst:793
#: ../../operations/troubleshooting.rst:813
msgid "If this fails:"
msgstr ""

#: ../../operations/troubleshooting.rst:768
msgid ""
"Make sure that the network allows the health checking traffic as specified in "
"the section :ref:`firewall_requirements`."
msgstr ""

#: ../../operations/troubleshooting.rst:771
msgid ""
"Validate that identities are synchronized correctly by running ``cilium "
"identity list`` in one of the Cilium pods. It must list identities from all "
"clusters. You can determine what cluster an identity belongs to by looking at "
"the label ``io.cilium.k8s.policy.cluster``."
msgstr ""

#: ../../operations/troubleshooting.rst:778
msgid ""
"Is the identity information available in the kvstore of each cluster? You can "
"confirm this by running ``cilium kvstore get --recursive cilium/state/"
"identities/v1/``."
msgstr ""

#: ../../operations/troubleshooting.rst:784
msgid ""
"The kvstore will only contain identities of the **local cluster**. It will "
"**not** contain identities of remote clusters. The state in the kvstore is used "
"for other clusters to discover all identities so it is important that local "
"identities are listed."
msgstr ""

#: ../../operations/troubleshooting.rst:789
msgid ""
"Validate that the IP cache is synchronized correctly by running ``cilium bpf "
"ipcache list`` or ``cilium map get cilium_ipcache``. The output must contain "
"pod IPs from local and remote clusters."
msgstr ""

#: ../../operations/troubleshooting.rst:795
msgid ""
"Is the IP cache information available in the kvstore of each cluster? You can "
"confirm this by running ``cilium kvstore get --recursive cilium/state/ip/v1/``."
msgstr ""

#: ../../operations/troubleshooting.rst:801
msgid ""
"The kvstore will only contain IPs of the **local cluster**. It will **not** "
"contain IPs of remote clusters. The state in the kvstore is used for other "
"clusters to discover all pod IPs so it is important that local identities are "
"listed."
msgstr ""

#: ../../operations/troubleshooting.rst:806
msgid ""
"When using global services, ensure that global services are configured with "
"endpoints from all clusters. Run ``cilium service list`` in any Cilium pod and "
"validate that the backend IPs consist of pod IPs from all clusters running "
"relevant backends. You can further validate the correct datapath plumbing by "
"running ``cilium bpf lb list`` to inspect the state of the eBPF maps."
msgstr ""

#: ../../operations/troubleshooting.rst:815
msgid ""
"Are services available in the kvstore of each cluster? You can confirm this by "
"running ``cilium kvstore get --recursive cilium/state/services/v1/``."
msgstr ""

#: ../../operations/troubleshooting.rst:819
msgid ""
"Run ``cilium debuginfo`` and look for the section ``k8s-service-cache``. In "
"that section, you will find the contents of the service correlation cache. It "
"will list the Kubernetes services and endpoints of the local cluster.  It will "
"also have a section ``externalEndpoints`` which must list all endpoints of "
"remote clusters."
msgstr ""

#: ../../operations/troubleshooting.rst:843
msgid ""
"The sections ``services`` and ``endpoints`` represent the services of the local "
"cluster, the section ``externalEndpoints`` lists all remote services and will "
"be correlated with services matching the same ``ServiceID``."
msgstr ""

#: ../../operations/troubleshooting.rst:849
msgid "Symptom Library"
msgstr ""

#: ../../operations/troubleshooting.rst:852
msgid "Node to node traffic is being dropped"
msgstr ""

#: ../../operations/troubleshooting.rst:855
msgid "Symptom"
msgstr ""

#: ../../operations/troubleshooting.rst:857
msgid ""
"Endpoint to endpoint communication on a single node succeeds but communication "
"fails between endpoints across multiple nodes."
msgstr ""

#: ../../operations/troubleshooting.rst:861
msgid "Troubleshooting steps:"
msgstr ""

#: ../../operations/troubleshooting.rst:863
msgid ""
"Run ``cilium-health status`` on the node of the source and destination "
"endpoint. It should describe the connectivity from that node to other nodes in "
"the cluster, and to a simulated endpoint on each other node. Identify points in "
"the cluster that cannot talk to each other. If the command does not describe "
"the status of the other node, there may be an issue with the KV-Store."
msgstr ""

#: ../../operations/troubleshooting.rst:870
msgid ""
"Run ``cilium monitor`` on the node of the source and destination endpoint. Look "
"for packet drops."
msgstr ""

#: ../../operations/troubleshooting.rst:873
msgid "When running in :ref:`arch_overlay` mode:"
msgstr ""

#: ../../operations/troubleshooting.rst:875
msgid ""
"Run ``cilium bpf tunnel list`` and verify that each Cilium node is aware of the "
"other nodes in the cluster.  If not, check the logfile for errors."
msgstr ""

#: ../../operations/troubleshooting.rst:878
msgid ""
"If nodes are being populated correctly, run ``tcpdump -n -i cilium_vxlan`` on "
"each node to verify whether cross node traffic is being forwarded correctly "
"between nodes."
msgstr ""

#: ../../operations/troubleshooting.rst:882
msgid "If packets are being dropped,"
msgstr ""

#: ../../operations/troubleshooting.rst:884
msgid ""
"verify that the node IP listed in ``cilium bpf tunnel list`` can reach each "
"other."
msgstr ""

#: ../../operations/troubleshooting.rst:886
msgid "verify that the firewall on each node allows UDP port 8472."
msgstr ""

#: ../../operations/troubleshooting.rst:888
msgid "When running in :ref:`arch_direct_routing` mode:"
msgstr ""

#: ../../operations/troubleshooting.rst:890
msgid ""
"Run ``ip route`` or check your cloud provider router and verify that you have "
"routes installed to route the endpoint prefix between all nodes."
msgstr ""

#: ../../operations/troubleshooting.rst:893
msgid "Verify that the firewall on each node permits to route the endpoint IPs."
msgstr ""

#: ../../operations/troubleshooting.rst:897
msgid "Useful Scripts"
msgstr ""

#: ../../operations/troubleshooting.rst:902
msgid "Retrieve Cilium pod managing a particular pod"
msgstr ""

#: ../../operations/troubleshooting.rst:904
msgid ""
"Identifies the Cilium pod that is managing a particular pod in a namespace:"
msgstr ""

#: ../../operations/troubleshooting.rst:910
#: ../../operations/troubleshooting.rst:930
#: ../../operations/troubleshooting.rst:953
msgid "**Example:**"
msgstr ""

#: ../../operations/troubleshooting.rst:922
msgid "Execute a command in all Kubernetes Cilium pods"
msgstr ""

#: ../../operations/troubleshooting.rst:924
msgid "Run a command within all Cilium pods of a cluster"
msgstr ""

#: ../../operations/troubleshooting.rst:943
msgid "List unmanaged Kubernetes pods"
msgstr ""

#: ../../operations/troubleshooting.rst:945
msgid ""
"Lists all Kubernetes pods in the cluster for which Cilium does *not* provide "
"networking. This includes pods running in host-networking mode and pods that "
"were started before Cilium was deployed."
msgstr ""

#: ../../operations/troubleshooting.rst:967
msgid "Reporting a problem"
msgstr ""

#: ../../operations/troubleshooting.rst:969
msgid ""
"Before you report a problem, make sure to retrieve the necessary information "
"from your cluster before the failure state is lost."
msgstr ""

#: ../../operations/troubleshooting.rst:973
msgid "Automatic log & state collection"
msgstr ""

#: ../../operations/troubleshooting.rst:977
msgid ""
"Then, execute ``cilium sysdump`` command to collect troubleshooting information "
"from your Kubernetes cluster:"
msgstr ""

#: ../../operations/troubleshooting.rst:984
msgid ""
"Note that by default ``cilium sysdump`` will attempt to collect as much logs as "
"possible and for all the nodes in the cluster. If your cluster size is above 20 "
"nodes, consider setting the following options to limit the size of the sysdump. "
"This is not required, but useful for those who have a constraint on bandwidth "
"or upload size."
msgstr ""

#: ../../operations/troubleshooting.rst:990
msgid ""
"set the ``--node-list`` option to pick only a few nodes in case the cluster has "
"many of them."
msgstr ""

#: ../../operations/troubleshooting.rst:992
msgid ""
"set the ``--logs-since-time`` option to go back in time to when the issues "
"started."
msgstr ""

#: ../../operations/troubleshooting.rst:993
msgid ""
"set the ``--logs-limit-bytes`` option to limit the size of the log files (note: "
"passed onto ``kubectl logs``; does not apply to entire collection archive)."
msgstr ""

#: ../../operations/troubleshooting.rst:996
msgid ""
"Ideally, a sysdump that has a full history of select nodes, rather than a brief "
"history of all the nodes, would be preferred (by using ``--node-list``). The "
"second recommended way would be to use ``--logs-since-time`` if you are able to "
"narrow down when the issues started. Lastly, if the Cilium agent and Operator "
"logs are too large, consider ``--logs-limit-bytes``."
msgstr ""

#: ../../operations/troubleshooting.rst:1002
msgid "Use ``--help`` to see more options:"
msgstr ""

#: ../../operations/troubleshooting.rst:1009
msgid "Single Node Bugtool"
msgstr ""

#: ../../operations/troubleshooting.rst:1011
msgid ""
"If you are not running Kubernetes, it is also possible to run the bug "
"collection tool manually with the scope of a single node:"
msgstr ""

#: ../../operations/troubleshooting.rst:1014
msgid ""
"The ``cilium-bugtool`` captures potentially useful information about your "
"environment for debugging. The tool is meant to be used for debugging a single "
"Cilium agent node. In the Kubernetes case, if you have multiple Cilium pods, "
"the tool can retrieve debugging information from all of them. The tool works by "
"archiving a collection of command output and files from several places. By "
"default, it writes to the ``tmp`` directory."
msgstr ""

#: ../../operations/troubleshooting.rst:1021
msgid ""
"Note that the command needs to be run from inside the Cilium pod/container."
msgstr ""

#: ../../operations/troubleshooting.rst:1027
msgid ""
"When running it with no option as shown above, it will try to copy various "
"files and execute some commands. If ``kubectl`` is detected, it will search for "
"Cilium pods. The default label being ``k8s-app=cilium``, but this and the "
"namespace can be changed via ``k8s-namespace`` and ``k8s-label`` respectively."
msgstr ""

#: ../../operations/troubleshooting.rst:1032
msgid ""
"If you want to capture the archive from a Kubernetes pod, then the process is a "
"bit different"
msgstr ""

#: ../../operations/troubleshooting.rst:1055
msgid ""
"Please check the archive for sensitive information and strip it away before "
"sharing it with us."
msgstr ""

#: ../../operations/troubleshooting.rst:1058
msgid "Below is an approximate list of the kind of information in the archive."
msgstr ""

#: ../../operations/troubleshooting.rst:1060
msgid "Cilium status"
msgstr ""

#: ../../operations/troubleshooting.rst:1061
msgid "Cilium version"
msgstr ""

#: ../../operations/troubleshooting.rst:1062
msgid "Kernel configuration"
msgstr ""

#: ../../operations/troubleshooting.rst:1063
msgid "Resolve configuration"
msgstr ""

#: ../../operations/troubleshooting.rst:1064
msgid "Cilium endpoint state"
msgstr ""

#: ../../operations/troubleshooting.rst:1065
msgid "Cilium logs"
msgstr ""

#: ../../operations/troubleshooting.rst:1066
msgid "Docker logs"
msgstr ""

#: ../../operations/troubleshooting.rst:1067
msgid "``dmesg``"
msgstr ""

#: ../../operations/troubleshooting.rst:1068
msgid "``ethtool``"
msgstr ""

#: ../../operations/troubleshooting.rst:1069
msgid "``ip a``"
msgstr ""

#: ../../operations/troubleshooting.rst:1070
msgid "``ip link``"
msgstr ""

#: ../../operations/troubleshooting.rst:1071
msgid "``ip r``"
msgstr ""

#: ../../operations/troubleshooting.rst:1072
msgid "``iptables-save``"
msgstr ""

#: ../../operations/troubleshooting.rst:1073
msgid "``kubectl -n kube-system get pods``"
msgstr ""

#: ../../operations/troubleshooting.rst:1074
msgid "``kubectl get pods,svc for all namespaces``"
msgstr ""

#: ../../operations/troubleshooting.rst:1075
msgid "``uname``"
msgstr ""

#: ../../operations/troubleshooting.rst:1076
msgid "``uptime``"
msgstr ""

#: ../../operations/troubleshooting.rst:1077
msgid "``cilium bpf * list``"
msgstr ""

#: ../../operations/troubleshooting.rst:1078
msgid "``cilium endpoint get for each endpoint``"
msgstr ""

#: ../../operations/troubleshooting.rst:1079
msgid "``cilium endpoint list``"
msgstr ""

#: ../../operations/troubleshooting.rst:1080
msgid "``hostname``"
msgstr ""

#: ../../operations/troubleshooting.rst:1081
msgid "``cilium policy get``"
msgstr ""

#: ../../operations/troubleshooting.rst:1082
msgid "``cilium service list``"
msgstr ""

#: ../../operations/troubleshooting.rst:1083
msgid "..."
msgstr ""

#: ../../operations/troubleshooting.rst:1087
msgid "Debugging information"
msgstr ""

#: ../../operations/troubleshooting.rst:1089
msgid ""
"If you are not running Kubernetes, you can use the ``cilium debuginfo`` command "
"to retrieve useful debugging information. If you are running Kubernetes, this "
"command is automatically run as part of the system dump."
msgstr ""

#: ../../operations/troubleshooting.rst:1093
msgid ""
"``cilium debuginfo`` can print useful output from the Cilium API. The output "
"format is in Markdown format so this can be used when reporting a bug on the "
"`issue tracker`_.  Running without arguments will print to standard output, but "
"you can also redirect to a file like"
msgstr ""

#: ../../operations/troubleshooting.rst:1104
msgid ""
"Please check the debuginfo file for sensitive information and strip it away "
"before sharing it with us."
msgstr ""

#: ../../operations/troubleshooting.rst:1109
msgid "Slack Assistance"
msgstr ""

#: ../../operations/troubleshooting.rst:1111
msgid ""
"The Cilium slack community is helpful first point of assistance to get help "
"troubleshooting a problem or to discuss options on how to address a problem."
msgstr ""

#: ../../operations/troubleshooting.rst:1114
msgid ""
"The slack community is open to everyone. You can request an invite email by "
"visiting `Slack <https://cilium.herokuapp.com/>`_."
msgstr ""

#: ../../operations/troubleshooting.rst:1118
msgid "Report an issue via GitHub"
msgstr ""

#: ../../operations/troubleshooting.rst:1120
msgid ""
"If you believe to have found an issue in Cilium, please report a `GitHub "
"issue`_ and make sure to attach a system dump as described above to ensure that "
"developers have the best chance to reproduce the issue."
msgstr ""

#: ../../operations/upgrade.rst:11
msgid "Upgrade Guide"
msgstr ""

#: ../../operations/upgrade.rst:15
msgid ""
"This upgrade guide is intended for Cilium running on Kubernetes. If you have "
"questions, feel free to ping us on the :term:`Slack channel`."
msgstr ""

#: ../../operations/upgrade-warning.rst:9
msgid ""
"Read the full upgrade guide to understand all the necessary steps before "
"performing them."
msgstr ""

#: ../../operations/upgrade-warning.rst:12
msgid ""
"Do not upgrade to \\ |NEXT_RELEASE|.0 before reading the section :ref:"
"`current_release_required_changes` and completing the required steps. Skipping "
"this step may lead to an non-functional upgrade."
msgstr ""

#: ../../operations/upgrade.rst:23
msgid "Running pre-flight check (Required)"
msgstr ""

#: ../../operations/upgrade.rst:25
msgid ""
"When rolling out an upgrade with Kubernetes, Kubernetes will first terminate "
"the pod followed by pulling the new image version and then finally spin up the "
"new image. In order to reduce the downtime of the agent and to prevent "
"``ErrImagePull`` errors during upgrade, the pre-flight check pre-pulls the new "
"image version. If you are running in :ref:`kubeproxy-free` mode you must also "
"pass on the Kubernetes API Server IP and / or the Kubernetes API Server Port "
"when generating the ``cilium-preflight.yaml`` file."
msgstr ""

#: ../../operations/upgrade.rst:35 ../../operations/upgrade.rst:114
#: ../../operations/upgrade.rst:174 ../../operations/upgrade.rst:248
msgid "kubectl"
msgstr ""

#: ../../operations/upgrade.rst:57
msgid "kubectl (kubeproxy-free)"
msgstr ""

#: ../../operations/upgrade.rst:71
msgid "Helm (kubeproxy-free)"
msgstr ""

#: ../../operations/upgrade.rst:83
msgid ""
"After applying the ``cilium-preflight.yaml``, ensure that the number of READY "
"pods is the same number of Cilium pods running."
msgstr ""

#: ../../operations/upgrade.rst:93
msgid ""
"Once the number of READY pods are equal, make sure the Cilium pre-flight "
"deployment is also marked as READY 1/1. If it shows READY 0/1, consult the :ref:"
"`cnp_validation` section and resolve issues with the deployment before "
"continuing with the upgrade."
msgstr ""

#: ../../operations/upgrade.rst:107
msgid "Clean up pre-flight check"
msgstr ""

#: ../../operations/upgrade.rst:109
msgid ""
"Once the number of READY for the preflight :term:`DaemonSet` is the same as the "
"number of cilium pods running and the preflight ``Deployment`` is marked as "
"READY ``1/1`` you can delete the cilium-preflight and proceed with the upgrade."
msgstr ""

#: ../../operations/upgrade.rst:129
msgid "Upgrading Cilium"
msgstr ""

#: ../../operations/upgrade.rst:131
msgid ""
"During normal cluster operations, all Cilium components should run the same "
"version. Upgrading just one of them (e.g., upgrading the agent without "
"upgrading the operator) could result in unexpected cluster behavior. The "
"following steps will describe how to upgrade all of the components from one "
"stable release to a later stable release."
msgstr ""

#: ../../operations/upgrade.rst:140
msgid "Step 1: Upgrade to latest patch version"
msgstr ""

#: ../../operations/upgrade.rst:142
msgid ""
"When upgrading from one minor release to another minor release, for example 1.x "
"to 1.y, it is recommended to upgrade to the latest patch release for a Cilium "
"release series first. The latest patch releases for each supported version of "
"Cilium are `here <https://github.com/cilium/cilium#stable-releases>`_. "
"Upgrading to the latest patch release ensures the most seamless experience if a "
"rollback is required following the minor release upgrade. The upgrade guides "
"for previous versions can be found for each minor version at the bottom left "
"corner."
msgstr ""

#: ../../operations/upgrade.rst:152
msgid "Step 2: Use Helm to Upgrade your Cilium deployment"
msgstr ""

#: ../../operations/upgrade.rst:154
msgid ""
":term:`Helm` can be used to either upgrade Cilium directly or to generate a new "
"set of YAML files that can be used to upgrade an existing deployment via "
"``kubectl``. By default, Helm will generate the new templates using the default "
"values files packaged with each new release. You still need to ensure that you "
"are specifying the equivalent options as used for the initial deployment, "
"either by specifying a them at the command line or by committing the values to "
"a YAML file."
msgstr ""

#: ../../gettingstarted/k8s-install-download-release.rst:9
msgid ""
"Make sure you have Helm 3 `installed <https://helm.sh/docs/intro/install/>`_. "
"Helm 2 is `no longer supported <https://helm.sh/blog/helm-v2-deprecation-"
"timeline/>`_."
msgstr ""

#: ../../gettingstarted/k8s-install-download-release.rst:14
msgid "Setup Helm repository:"
msgstr ""

#: ../../gettingstarted/k8s-install-download-release.rst:22
msgid ""
"Download the Cilium release tarball and change to the kubernetes install "
"directory:"
msgstr ""

#: ../../operations/upgrade.rst:164
msgid ""
"To minimize datapath disruption during the upgrade, the "
"``upgradeCompatibility`` option should be set to the initial Cilium version "
"which was installed in this cluster. Valid options are:"
msgstr ""

#: ../../operations/upgrade.rst:168
msgid "``1.7`` if the initial install was Cilium 1.7.x or earlier."
msgstr ""

#: ../../operations/upgrade.rst:169
msgid "``1.8`` if the initial install was Cilium 1.8.x."
msgstr ""

#: ../../operations/upgrade.rst:170
msgid "``1.9`` if the initial install was Cilium 1.9.x."
msgstr ""

#: ../../operations/upgrade.rst:171
msgid "``1.10`` if the initial install was Cilium 1.10.x."
msgstr ""

#: ../../operations/upgrade.rst:176
msgid "Generate the required YAML file and deploy it:"
msgstr ""

#: ../../operations/upgrade.rst:188
msgid "Deploy Cilium release via Helm:"
msgstr ""

#: ../../operations/upgrade.rst:198
msgid ""
"Instead of using ``--set``, you can also save the values relative to your "
"deployment in a YAML file and use it to regenerate the YAML for the latest "
"Cilium version. Running any of the previous commands will overwrite the "
"existing cluster's :term:`ConfigMap` so it is critical to preserve any existing "
"options, either by setting them at the command line or storing them in a YAML "
"file, similar to:"
msgstr ""

#: ../../operations/upgrade.rst:215
msgid "You can then upgrade using this values file by running:"
msgstr ""

#: ../../operations/upgrade.rst:223
msgid ""
"When upgrading from one minor release to another minor release using ``helm "
"upgrade``, do *not* use Helm's ``--reuse-values`` flag. The ``--reuse-values`` "
"flag ignores any newly introduced values present in the new release and thus "
"may cause the Helm template to render incorrectly. Instead, if you want to "
"reuse the values from your existing installation, save the old values in a "
"values file, check the file for any renamed or deprecated values, and then pass "
"it to the ``helm upgrade`` command as described above. You can retrieve and "
"save the values from an existing installation with the following command:"
msgstr ""

#: ../../operations/upgrade.rst:237
msgid ""
"The ``--reuse-values`` flag may only be safely used if the Cilium chart version "
"remains unchanged, for example when ``helm upgrade`` is used to apply "
"configuration changes without upgrading Cilium."
msgstr ""

#: ../../operations/upgrade.rst:242
msgid "Step 3: Rolling Back"
msgstr ""

#: ../../operations/upgrade.rst:244
msgid ""
"Occasionally, it may be necessary to undo the rollout because a step was missed "
"or something went wrong during upgrade. To undo the rollout run:"
msgstr ""

#: ../../operations/upgrade.rst:261
msgid ""
"This will revert the latest changes to the Cilium ``DaemonSet`` and return "
"Cilium to the state it was in prior to the upgrade."
msgstr ""

#: ../../operations/upgrade.rst:266
msgid ""
"When rolling back after new features of the new minor version have already been "
"consumed, consult the :ref:`version_notes` to check and prepare for "
"incompatible feature use before downgrading/rolling back. This step is only "
"required after new functionality introduced in the new minor version has "
"already been explicitly used by creating new resources or by opting into new "
"features via the :term:`ConfigMap`."
msgstr ""

#: ../../operations/upgrade.rst:277
msgid "Version Specific Notes"
msgstr ""

#: ../../operations/upgrade.rst:279
msgid ""
"This section documents the specific steps required for upgrading from one "
"version of Cilium to another version of Cilium. There are particular version "
"transitions which are suggested by the Cilium developers to avoid known issues "
"during upgrade, then subsequently there are sections for specific upgrade "
"transitions, ordered by version."
msgstr ""

#: ../../operations/upgrade.rst:285
msgid ""
"The table below lists suggested upgrade transitions, from a specified current "
"version running in a cluster to a specified target version. If a specific "
"combination is not listed in the table below, then it may not be safe. In that "
"case, consider performing incremental upgrades between versions (e.g. upgrade "
"from ``1.9.x`` to ``1.10.y`` first, and to ``1.11.z`` only afterwards)."
msgstr ""

#: ../../operations/upgrade.rst:292
msgid "Current version"
msgstr ""

#: ../../operations/upgrade.rst:292
msgid "Target version"
msgstr ""

#: ../../operations/upgrade.rst:292
msgid "L3/L4 impact"
msgstr ""

#: ../../operations/upgrade.rst:292
msgid "L7 impact"
msgstr ""

#: ../../operations/upgrade.rst:294
msgid "``1.10.x``"
msgstr ""

#: ../../operations/upgrade.rst:294
msgid "``1.11.y``"
msgstr ""

#: ../../operations/upgrade.rst:294 ../../operations/upgrade.rst:296
#: ../../operations/upgrade.rst:298
msgid "Minimal to None"
msgstr ""

#: ../../operations/upgrade.rst:294 ../../operations/upgrade.rst:296
#: ../../operations/upgrade.rst:298
msgid "Clients must reconnect[1]"
msgstr ""

#: ../../operations/upgrade.rst:296
msgid "``1.9.x``"
msgstr ""

#: ../../operations/upgrade.rst:296
msgid "``1.10.y``"
msgstr ""

#: ../../operations/upgrade.rst:298
msgid "``1.8.x``"
msgstr ""

#: ../../operations/upgrade.rst:298
msgid "``1.9.y``"
msgstr ""

#: ../../operations/upgrade.rst:301
msgid "Annotations:"
msgstr ""

#: ../../operations/upgrade.rst:303
msgid ""
"**Clients must reconnect**: Any traffic flowing via a proxy (for example, "
"because an L7 policy is in place) will be disrupted during upgrade. Endpoints "
"communicating via the proxy must reconnect to re-establish connections."
msgstr ""

#: ../../operations/upgrade.rst:313
msgid "1.12 Upgrade Notes"
msgstr ""

#: ../../operations/upgrade.rst:315
msgid ""
"The Cilium agent does not support the legacy ``nat46-range`` option as well as "
"the per-endpoint ``NAT46`` configuration anymore. Both are replaced in favor of "
"NAT46/64 handling for services."
msgstr ""

#: ../../operations/upgrade.rst:320 ../../operations/upgrade.rst:457
msgid "New Options"
msgstr ""

#: ../../operations/upgrade.rst:322
msgid ""
"``ipv6-native-routing-cidr``: This option specifies the IPv6 CIDR for native "
"routing. It must be set whenever running in direct routing mode with IPv6 "
"masquerading enabled."
msgstr ""

#: ../../operations/upgrade.rst:325
msgid ""
"``instance-tags-filter``: This option specifies a list of tags used to search "
"for EC2 instances (and existing ENIs). EC2 Instances tags is in the form of "
"k1=v1,k2=v2 (multiple k/v pairs can also be passed by repeating the CLI flag\") "
"Use ``instanceTagsFilter`` in Helm chart."
msgstr ""

#: ../../operations/upgrade.rst:329
msgid ""
"``nodes-gc-interval``: This option was marked as deprecated and has no effect "
"in 1.11. Cilium Node Garbage collector is added back in 1.12 (but for k8s GC "
"instead of kvstore), so this flag is moved out of deprecated list."
msgstr ""

#: ../../operations/upgrade.rst:334 ../../operations/upgrade.rst:411
msgid "Removed Options"
msgstr ""

#: ../../operations/upgrade.rst:336
msgid ""
"The endpoint config option ``Conntrack`` was removed. The option was used to "
"disable the stateful connection tracking for the endpoint. However, many Cilium "
"features depend on the tracking. Therefore the option to disable the connection "
"tracking was removed. In addition, we deprecated the ``disable-conntrack`` "
"option and made it non-operational. It will be removed in version 1.13."
msgstr ""

#: ../../operations/upgrade.rst:342
msgid ""
"The ``host-reachable-services-protos`` option (``.hostServices.protocols`` in "
"Helm) was deprecated, and it will be removed in version 1.13."
msgstr ""

#: ../../operations/upgrade.rst:344
msgid ""
"The ``native-routing-cidr`` option deprecated in 1.11 in favor of ``ipv4-native-"
"routing-cidr`` has been removed."
msgstr ""

#: ../../operations/upgrade.rst:348 ../../operations/upgrade.rst:433
msgid "Deprecated Options"
msgstr ""

#: ../../operations/upgrade.rst:350
msgid ""
"The ``CiliumEgressNATPolicy`` CRD has been deprecated, and will be removed in "
"version 1.13. It is superseded by the ``CiliumEgressGatewayPolicy`` CRD, which "
"allows for better selection of the Egress Node, Egress Interface and Masquerade "
"IP."
msgstr ""

#: ../../operations/upgrade.rst:355 ../../operations/upgrade.rst:476
msgid "Helm Options"
msgstr ""

#: ../../operations/upgrade.rst:357
msgid ""
"``bandwidthManager`` has been deprecated in favor of ``bandwidthManager."
"enabled``, and will be removed in 1.13."
msgstr ""

#: ../../operations/upgrade.rst:359
msgid ""
"``bpf.hostRouting`` has been deprecated in favor of ``bpf.hostLegacyRouting``, "
"and will be removed in 1.13."
msgstr ""

#: ../../operations/upgrade.rst:361
msgid ""
"``hubble.tls.ca.cert`` has been deprecated in favor of ``tls.ca.cert``, and "
"will be removed in 1.13."
msgstr ""

#: ../../operations/upgrade.rst:363
msgid ""
"``hubble.tls.ca.key`` has been deprecated in favor of ``tls.ca.key``, and will "
"be removed in 1.13."
msgstr ""

#: ../../operations/upgrade.rst:365
msgid ""
"``hubble.ui.securityContext.enabled`` has been deprecated in favor of ``hubble."
"ui.securityContext``, and will be removed in 1.13."
msgstr ""

#: ../../operations/upgrade.rst:367
msgid ""
"``clustermesh.apiserver.tls.ca.cert`` has been deprecated in favor of ``tls.ca."
"key``, and will be removed in 1.13."
msgstr ""

#: ../../operations/upgrade.rst:369
msgid ""
"``clustermesh.apiserver.tls.ca.key`` has been deprecated in favor of ``tls.ca."
"key``, and will be removed in 1.13."
msgstr ""

#: ../../operations/upgrade.rst:371
msgid ""
"``operator.unmanagedPodWatcher.restart`` has been introduced to govern whether "
"the cilium-operator will attempt to restart pods that are not managed by "
"Cilium. To retain consistency with earlier releases, this setting is enabled by "
"default."
msgstr ""

#: ../../operations/upgrade.rst:375
msgid "``tls.enabled`` has been removed as this attribute is not used at all."
msgstr ""

#: ../../operations/upgrade.rst:376
msgid ""
"Only one CA will be generated with either the helm or CronJob auto method, "
"there will be a short disruption while the new CA is propagated to all nodes."
msgstr ""

#: ../../operations/upgrade.rst:382
msgid "1.11 Upgrade Notes"
msgstr ""

#: ../../operations/upgrade.rst:384
msgid ""
"The Cilium agent will now fail instead of falling back to auto-detection when "
"device wildcard expansion (``--devices=eth+``) yields no devices."
msgstr ""

#: ../../operations/upgrade.rst:386
msgid ""
"Device auto-detection now discovers devices through the routing table and only "
"considers devices that have a global unicast route in some routing table."
msgstr ""

#: ../../operations/upgrade.rst:388
msgid ""
"The XDP-based prefilter is enabled for all devices specified by ``--devices`` "
"if ``--prefilter-device`` is set."
msgstr ""

#: ../../operations/upgrade.rst:390
msgid ""
"New flags were added to enable installation where alternative VXLAN/Geneve and "
"health ports need to be used (``--tunnel-port`` and ``--cluster-health-port``). "
"Default values of these flags haven't changed, however if ``--tunnel-port`` "
"gets set to non-default values on upgrade, there will datapath downtime between "
"nodes. If ``--tunnel-port`` needs to change, it's recommended to perform the "
"upgrade first, and change the port afterwards, in order to separate agent "
"upgrade from configuration update. Changing ``--cluster-health-port`` will not "
"affect datapath, however it's recommended to still handle configuration change "
"separately from agent upgrade. Changing both ports simultaneously shouldn't "
"cause any issues."
msgstr ""

#: ../../operations/upgrade.rst:399
msgid ""
"When Egress Gateway is enabled, upgrading to 1.11 will cause a brief "
"interruption of the connectivity between the client pods and the egress gateway "
"nodes. Once the connectivity is restored, clients will need to reconnect."
msgstr ""

#: ../../operations/upgrade.rst:405
msgid "Removed Metrics/Labels"
msgstr ""

#: ../../operations/upgrade.rst:407
msgid ""
"``cilium_operator_identity_gc_entries_total`` is removed. Please use "
"``cilium_operator_identity_gc_entries`` instead."
msgstr ""

#: ../../operations/upgrade.rst:408
msgid ""
"``cilium_operator_identity_gc_runs_total`` is removed. Please use "
"``cilium_operator_identity_gc_runs`` instead."
msgstr ""

#: ../../operations/upgrade.rst:413
msgid ""
"``bpf-compile-debug``: This option does not have any effect since 1.10 and is "
"now removed."
msgstr ""

#: ../../operations/upgrade.rst:415
msgid ""
"``k8s-force-json-patch``: This option does not have any effect for environments "
"running Kubernetes >= 1.13, is deprecated since 1.10, and now removed."
msgstr ""

#: ../../operations/upgrade.rst:418
msgid ""
"``masquerade``: This option has been deprecated in 1.10 and replaced by "
"``enable-ipv4-masquerade``."
msgstr ""

#: ../../operations/upgrade.rst:420
msgid ""
"``skip-crd-creation``: This option does not have any effect since 1.10 and is "
"now removed."
msgstr ""

#: ../../operations/upgrade.rst:422
msgid ""
"``hubble-flow-buffer-size``: This option was deprecated in 1.10 in favor of "
"``hubble-event-buffer-capacity``. It is now removed."
msgstr ""

#: ../../operations/upgrade.rst:424
msgid ""
"The ``Capabilities`` Helm value has been removed. When using ``helm template`` "
"to generate the Kubernetes manifest for a specific Kubernetes version, please "
"use the ``--kube-version`` flag (introduced in Helm 3.6.0) instead."
msgstr ""

#: ../../operations/upgrade.rst:427
msgid ""
"The deprecated ``hubble-ca-cert`` ConfigMap has been removed. Use ``hubble-ca-"
"secret`` secret instead."
msgstr ""

#: ../../operations/upgrade.rst:429
msgid ""
"The ``azure-cloud-name`` option for ``cilium-operator-azure`` was deprecated in "
"1.10 and is now removed."
msgstr ""

#: ../../operations/upgrade.rst:435
msgid ""
"``native-routing-cidr``: This option has been deprecated in favor of ``ipv4-"
"native-routing-cidr`` and will be removed in 1.12."
msgstr ""

#: ../../operations/upgrade.rst:437
msgid ""
"``prefilter-device`` and ``prefilter-mode``: These options have been deprecated "
"in favor of ``enable-xdp-prefilter`` and ``bpf-lb-acceleration``, and will be "
"removed in 1.12. To select the prefilter devices use ``devices``."
msgstr ""

#: ../../operations/upgrade.rst:440
msgid ""
"The NodePort related ``bpf-lb-bypass-fib-lookup`` option to enable a FIB lookup "
"bypass optimization for NodePort's reverse NAT handling has been deprecated as "
"the Linux kernel's FIB table is now always consulted. Thus, explicitly setting "
"the option has no effect. It is scheduled for removal in 1.12."
msgstr ""

#: ../../operations/upgrade.rst:445
msgid ""
"IPVLAN support has been deprecated due to lack of feature support and lack of "
"community interest. `Recent improvements Virtual Ethernet device performance "
"<https://cilium.io/blog/2020/11/10/cilium-19#veth>`_ have granted many of the "
"benefits of IPVLAN to the standard veth mode."
msgstr ""

#: ../../operations/upgrade.rst:449
msgid ""
"Support for Consul as a kvstore backend has been deprecated due to a lack of "
"community interest. It is planned for removal in 1.12."
msgstr ""

#: ../../operations/upgrade.rst:451
msgid ""
"The in-pod Cilium CLI command ``cilium policy trace`` has been deprecated in "
"favor of approaches using the `Network Policy Editor <https://app.networkpolicy."
"io>`_ or guide for `policy_verdicts`."
msgstr ""

#: ../../operations/upgrade.rst:454
msgid "Cilium no longer recognizes label sources from Mesos."
msgstr ""

#: ../../operations/upgrade.rst:459
msgid ""
"``kvstore-max-consecutive-quorum-errors``: This option configures the max "
"acceptable kvstore consecutive quorum errors before the agent assumes permanent "
"failure."
msgstr ""

#: ../../operations/upgrade.rst:464
msgid "Renamed Options"
msgstr ""

#: ../../operations/upgrade.rst:466
msgid "The following option has been renamed:"
msgstr ""

#: ../../operations/upgrade.rst:468
msgid "``enable-egress-gateway`` to ``enable-ipv4-egress-gateway``."
msgstr ""

#: ../../operations/upgrade.rst:471
msgid "Deprecated Options in Cilium Operator"
msgstr ""

#: ../../operations/upgrade.rst:472
msgid ""
"``nodes-gc-interval``: This option will be removed in 1.12. Its value does not "
"have any effect in 1.11."
msgstr ""

#: ../../operations/upgrade.rst:478
msgid "``hostFirewall`` was renamed to ``hostFirewall.enabled``."
msgstr ""

#: ../../operations/upgrade.rst:479
msgid ""
"``ipam.operator.clusterPoolIPv4PodCIDR`` was deprecated in favor of ``ipam."
"operator.clusterPoolIPv4PodCIDRList``"
msgstr ""

#: ../../operations/upgrade.rst:480
msgid ""
"``ipam.operator.clusterPoolIPv6PodCIDR`` was deprecated in favor of ``ipam."
"operator.clusterPoolIPv6PodCIDRList``"
msgstr ""

#: ../../operations/upgrade.rst:481
msgid ""
"``nativeRoutingCIDR``: was deprecated in favor of ``ipv4NativeRoutingCIDR``"
msgstr ""

#: ../../operations/upgrade.rst:484
msgid "Advanced"
msgstr ""

#: ../../operations/upgrade.rst:487
msgid "Upgrade Impact"
msgstr ""

#: ../../operations/upgrade.rst:489
msgid ""
"Upgrades are designed to have minimal impact on your running deployment. "
"Networking connectivity, policy enforcement and load balancing will remain "
"functional in general. The following is a list of operations that will not be "
"available during the upgrade:"
msgstr ""

#: ../../operations/upgrade.rst:494
msgid ""
"API aware policy rules are enforced in user space proxies and are currently "
"running as part of the Cilium pod unless Cilium is configured to run in Istio "
"mode. Upgrading Cilium will cause the proxy to restart which will result in a "
"connectivity outage and connection to be reset."
msgstr ""

#: ../../operations/upgrade.rst:499
msgid ""
"Existing policy will remain effective but implementation of new policy rules "
"will be postponed to after the upgrade has been completed on a particular node."
msgstr ""

#: ../../operations/upgrade.rst:503
msgid ""
"Monitoring components such as ``cilium monitor`` will experience a brief outage "
"while the Cilium pod is restarting. Events are queued up and read after the "
"upgrade. If the number of events exceeds the event buffer size, events will be "
"lost."
msgstr ""

#: ../../operations/upgrade.rst:512
msgid "Rebasing a ConfigMap"
msgstr ""

#: ../../operations/upgrade.rst:514
msgid ""
"This section describes the procedure to rebase an existing :term:`ConfigMap` to "
"the template of another version."
msgstr ""

#: ../../operations/upgrade.rst:518
msgid "Export the current ConfigMap"
msgstr ""

#: ../../operations/upgrade.rst:550
msgid ""
"In the :term:`ConfigMap` above, we can verify that Cilium is using ``debug`` "
"with ``true``, it has a etcd endpoint running with `TLS <https://etcd.io/docs/"
"latest/op-guide/security/>`_, and the etcd is set up to have `client to server "
"authentication <https://etcd.io/docs/latest/op-guide/security/#example-2-client-"
"to-server-authentication-with-https-client-certificates>`_."
msgstr ""

#: ../../operations/upgrade.rst:555
msgid "Generate the latest ConfigMap"
msgstr ""

#: ../../operations/upgrade.rst:567
msgid "Add new options"
msgstr ""

#: ../../operations/upgrade.rst:569
msgid ""
"Add the new options manually to your old :term:`ConfigMap`, and make the "
"necessary changes."
msgstr ""

#: ../../operations/upgrade.rst:572
msgid ""
"In this example, the ``debug`` option is meant to be kept with ``true``, the "
"``etcd-config`` is kept unchanged, and ``monitor-aggregation`` is a new option, "
"but after reading the :ref:`version_notes` the value was kept unchanged from "
"the default value."
msgstr ""

#: ../../operations/upgrade.rst:577
msgid ""
"After making the necessary changes, the old :term:`ConfigMap` was migrated with "
"the new options while keeping the configuration that we wanted:"
msgstr ""

#: ../../operations/upgrade.rst:611
msgid "Apply new ConfigMap"
msgstr ""

#: ../../operations/upgrade.rst:613
msgid ""
"After adding the options, manually save the file with your changes and install "
"the :term:`ConfigMap` in the ``kube-system`` namespace of your cluster."
msgstr ""

#: ../../operations/upgrade.rst:620
msgid ""
"As the :term:`ConfigMap` is successfully upgraded we can start upgrading Cilium "
"``DaemonSet`` and ``RBAC`` which will pick up the latest configuration from "
"the :term:`ConfigMap`."
msgstr ""

#: ../../operations/upgrade.rst:628
msgid "Restrictions on unique prefix lengths for CIDR policy rules"
msgstr ""

#: ../../operations/upgrade.rst:630
msgid ""
"The Linux kernel applies limitations on the complexity of eBPF code that is "
"loaded into the kernel so that the code may be verified as safe to execute on "
"packets. Over time, Linux releases become more intelligent about the "
"verification of programs which allows more complex programs to be loaded. "
"However, the complexity limitations affect some features in Cilium depending on "
"the kernel version that is used with Cilium."
msgstr ""

#: ../../operations/upgrade.rst:637
msgid ""
"One such limitation affects Cilium's configuration of CIDR policies. On Linux "
"kernels 4.10 and earlier, this manifests as a restriction on the number of "
"unique prefix lengths supported in CIDR policy rules."
msgstr ""

#: ../../operations/upgrade.rst:641
msgid ""
"Unique prefix lengths are counted by looking at the prefix portion of CIDR "
"rules and considering which prefix lengths are unique. For example, in the "
"following policy example, the ``toCIDR`` section specifies a ``/32``, and the "
"``toCIDRSet`` section specifies a ``/8`` with a ``/12`` removed from it. In "
"addition, three prefix lengths are always counted: the host prefix length for "
"the protocol (IPv4: ``/32``, IPv6: ``/128``), the default prefix length "
"(``/0``), and the cluster prefix length (default IPv4: ``/8``, IPv6: ``/64``). "
"All in all, the following example counts as seven unique prefix lengths in IPv4:"
msgstr ""

#: ../../operations/upgrade.rst:650
msgid "``/32`` (from ``toCIDR``, also from host prefix)"
msgstr ""

#: ../../operations/upgrade.rst:651
msgid "``/12`` (from ``toCIDRSet``)"
msgstr ""

#: ../../operations/upgrade.rst:652
msgid "``/11`` (from ``toCIDRSet``)"
msgstr ""

#: ../../operations/upgrade.rst:653
msgid "``/10`` (from ``toCIDRSet``)"
msgstr ""

#: ../../operations/upgrade.rst:654
msgid "``/9`` (from ``toCIDRSet``)"
msgstr ""

#: ../../operations/upgrade.rst:655
msgid "``/8`` (from cluster prefix)"
msgstr ""

#: ../../operations/upgrade.rst:656
msgid "``/0`` (from default prefix)"
msgstr ""

#: ../../operations/upgrade.rst:661
msgid "k8s YAML"
msgstr ""

#: ../../operations/upgrade.rst:664
msgid "JSON"
msgstr ""

#: ../../operations/upgrade.rst:673 ../../operations/upgrade.rst:714
msgid "Affected versions"
msgstr ""

#: ../../operations/upgrade.rst:675
msgid "Any version of Cilium running on Linux 4.10 or earlier"
msgstr ""

#: ../../operations/upgrade.rst:677
msgid ""
"When a CIDR policy with too many unique prefix lengths is imported, Cilium will "
"reject the policy with a message like the following:"
msgstr ""

#: ../../operations/upgrade.rst:687
msgid ""
"The supported count of unique prefix lengths may differ between Cilium minor "
"releases, for example Cilium 1.1 supported 20 unique prefix lengths on Linux "
"4.10 or older, while Cilium 1.2 only supported 18 (for IPv4) or 4 (for IPv6)."
msgstr ""

#: ../../operations/upgrade.rst:692 ../../operations/upgrade.rst:719
msgid "Mitigation"
msgstr ""

#: ../../operations/upgrade.rst:694
msgid ""
"Users may construct CIDR policies that use fewer unique prefix lengths. This "
"can be achieved by composing or decomposing adjacent prefixes."
msgstr ""

#: ../../operations/upgrade.rst:698
msgid "Solution"
msgstr ""

#: ../../operations/upgrade.rst:700
msgid ""
"Upgrade the host Linux version to 4.11 or later. This step is beyond the scope "
"of the Cilium guide."
msgstr ""

#: ../../operations/upgrade.rst:705
msgid ""
"Migrating from kvstore-backed identities to Kubernetes CRD-backed identities"
msgstr ""

#: ../../operations/upgrade.rst:707
msgid ""
"Beginning with cilium 1.6, Kubernetes CRD-backed security identities can be "
"used for smaller clusters. Along with other changes in 1.6 this allows kvstore-"
"free operation if desired. It is possible to migrate identities from an "
"existing kvstore deployment to CRD-backed identities. This minimizes "
"disruptions to traffic as the update rolls out through the cluster."
msgstr ""

#: ../../operations/upgrade.rst:716
msgid "Cilium 1.6 deployments using kvstore-backend identities"
msgstr ""

#: ../../operations/upgrade.rst:721
msgid ""
"When identities change, existing connections can be disrupted while cilium "
"initializes and synchronizes with the shared identity store. The disruption "
"occurs when new numeric identities are used for existing pods on some instances "
"and others are used on others. When converting to CRD-backed identities, it is "
"possible to pre-allocate CRD identities so that the numeric identities match "
"those in the kvstore. This allows new and old cilium instances in the rollout "
"to agree."
msgstr ""

#: ../../operations/upgrade.rst:729
msgid ""
"The steps below show an example of such a migration. It is safe to re-run the "
"command if desired. It will identify already allocated identities or ones that "
"cannot be migrated. Note that identity ``34815`` is migrated, ``17003`` is "
"already migrated, and ``11730`` has a conflict and a new ID allocated for those "
"labels."
msgstr ""

#: ../../operations/upgrade.rst:735
msgid ""
"The steps below assume a stable cluster with no new identities created during "
"the rollout. Once a cilium using CRD-backed identities is running, it may begin "
"allocating identities in a way that conflicts with older ones in the kvstore."
msgstr ""

#: ../../operations/upgrade.rst:739
msgid "The cilium preflight manifest requires etcd support and can be built with:"
msgstr ""

#: ../../operations/upgrade.rst:756
msgid "Example migration"
msgstr ""

#: ../../operations/upgrade.rst:789
msgid ""
"It is also possible to use the ``--k8s-kubeconfig-path``  and ``--kvstore-opt`` "
"``cilium`` CLI options with the preflight command. The default is to derive the "
"configuration as cilium-agent does."
msgstr ""

#: ../../operations/upgrade.rst:798
msgid "Clearing CRD identities"
msgstr ""

#: ../../operations/upgrade.rst:800
msgid ""
"If a migration has gone wrong, it possible to start with a clean slate. Ensure "
"that no cilium instances are running with identity-allocation-mode crd and "
"execute:"
msgstr ""

#: ../../operations/upgrade.rst:809
msgid "CNP Validation"
msgstr ""

#: ../../operations/upgrade.rst:811
msgid ""
"Running the CNP Validator will make sure the policies deployed in the cluster "
"are valid. It is important to run this validation before an upgrade so it will "
"make sure Cilium has a correct behavior after upgrade. Avoiding doing this "
"validation might cause Cilium from updating its ``NodeStatus`` in those invalid "
"Network Policies as well as in the worst case scenario it might give a false "
"sense of security to the user if a policy is badly formatted and Cilium is not "
"enforcing that policy due a bad validation schema. This CNP Validator is "
"automatically executed as part of the pre-flight check :ref:`pre_flight`."
msgstr ""

#: ../../operations/upgrade.rst:820
msgid ""
"Start by deployment the ``cilium-pre-flight-check`` and check if the "
"``Deployment`` shows READY 1/1, if it does not check the pod logs."
msgstr ""

#: ../../operations/upgrade.rst:837
msgid ""
"In this example, we can see the ``CiliumNetworkPolicy`` in the ``default`` "
"namespace with the name ``cnp-update`` is not valid for the Cilium version we "
"are trying to upgrade. In order to fix this policy we need to edit it, we can "
"do this by saving the policy locally and modify it. For this example it seems "
"the ``.spec.labels`` has set an array of strings which is not correct as per "
"the official schema."
msgstr ""

#: ../../operations/upgrade.rst:867
msgid ""
"To fix this policy we need to set the ``.spec.labels`` with the right format "
"and commit these changes into Kubernetes."
msgstr ""

#: ../../operations/upgrade.rst:895
msgid ""
"After applying the fixed policy we can delete the pod that was validating the "
"policies so that Kubernetes creates a new pod immediately to verify if the "
"fixed policies are now valid."
msgstr ""

#: ../../operations/upgrade.rst:914
msgid ""
"Once they are valid you can continue with the upgrade process. :ref:"
"`cleanup_preflight_check`"
msgstr ""
